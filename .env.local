# Configuração para execução local com Ollama
MAIN_PROVIDER=ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3:latest
OLLAMA_EMBEDDINGS_MODEL=nomic-embed-text:latest

# Configuração para vector store local
VECTOR_STORE_TYPE=chroma_local
VECTOR_STORE_CONFIG={"persist_directory": "./local_chroma_db", "collection_name": "agentcore_local"}

# Habilitar testes locais
ENABLE_VECTOR_TESTS=true
ENABLE_MODEL_COMPARISON=true
TESTING=true