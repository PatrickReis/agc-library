# Guia de Uso - AgentCore

Este guia mostra como usar a biblioteca AgentCore depois de instalada.

## üéØ Conceitos Principais

A AgentCore √© baseada em alguns conceitos fundamentais:

1. **api2tool**: Fun√ß√£o principal que converte APIs OpenAPI em ferramentas utiliz√°veis
2. **Provedores LLM**: Sistema unificado para diferentes provedores (Bedrock, OpenAI, etc.)
3. **Agentes**: Orquestra√ß√£o inteligente usando LangGraph
4. **Logging**: Sistema profissional de logs

## üöÄ In√≠cio R√°pido

### Uso B√°sico
```python
from agentCore.utils import api2tool

# Converter OpenAPI para ferramentas
tools = api2tool("./api.json")
print(f"Geradas {len(tools)} ferramentas")
```

### Uso Completo
```python
from agentCore.utils import api2tool
from agentCore.providers import get_llm
from agentCore.graphs import create_agent_graph
from langchain_core.messages import HumanMessage

# 1. Configurar LLM
llm = get_llm()  # Usa AWS Bedrock por padr√£o

# 2. Converter API para ferramentas
tools = api2tool("./api.json")
tool_functions = [tool['function'] for tool in tools]

# 3. Criar agente
agent = create_agent_graph(llm, tools=tool_functions)

# 4. Usar agente
response = agent.invoke({
    "messages": [HumanMessage(content="Sua pergunta aqui")]
})

print(response["messages"][-1].content)
```

## üì° Trabalhando com APIs

### 1. Diferentes Fontes de OpenAPI

```python
# Arquivo local
tools = api2tool("./openapi.json")

# URL remota
tools = api2tool("https://api.exemplo.com/swagger.json")

# Dicion√°rio Python
openapi_dict = {"openapi": "3.0.0", ...}
tools = api2tool(openapi_dict)
```

### 2. Formatos de Sa√≠da

```python
# Lista de ferramentas (padr√£o)
tools = api2tool("api.json", output_format="tools")

# Dicion√°rio indexado
tools_dict = api2tool("api.json", output_format="dict")

# Gerar arquivo Python
python_code = api2tool("api.json", output_format="file")

# Apenas nomes
names = api2tool("api.json", output_format="names")

# Informa√ß√µes da API
info = api2tool("api.json", output_format="info")
```

### 3. Especificar URL Base

```python
tools = api2tool("api.json", base_url="https://production-api.com")
```

## ü§ñ Provedores LLM

### AWS Bedrock (Recomendado)

```python
from agentCore.providers import get_llm, get_embeddings, get_provider_info

# Usar Bedrock (padr√£o)
llm = get_llm("bedrock")
embeddings = get_embeddings("bedrock")

# Ver informa√ß√µes
info = get_provider_info("bedrock")
print(info)
```

### Outros Provedores

```python
# OpenAI
llm = get_llm("openai")

# Ollama (local)
llm = get_llm("ollama")

# Google Gemini
llm = get_llm("gemini")

# Provedor autom√°tico (baseado em MAIN_PROVIDER)
llm = get_llm()
```

## üîß Configura√ß√£o Avan√ßada

### 1. Vari√°veis de Ambiente

```bash
# Provedor principal
MAIN_PROVIDER=bedrock

# AWS Bedrock
AWS_REGION=us-east-1
BEDROCK_MODEL=anthropic.claude-3-sonnet-20240229-v1:0

# OpenAI
OPENAI_API_KEY=sua_chave
OPENAI_MODEL=gpt-4

# Ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3:latest
```

### 2. Arquivo .env
Crie um arquivo `.env` no seu projeto:

```env
MAIN_PROVIDER=bedrock
AWS_REGION=us-east-1
BEDROCK_MODEL=anthropic.claude-3-sonnet-20240229-v1:0
```

### 3. Logging Personalizado

```python
from agentCore.logger import get_logger

logger = get_logger("minha_app")

logger.info("Aplica√ß√£o iniciada")
logger.success("Opera√ß√£o bem-sucedida")
logger.error("Erro na aplica√ß√£o")
logger.tool_execution("minha_ferramenta")
```

## üìä Interface CLI

A AgentCore inclui uma interface de linha de comando:

### Comandos Principais

```bash
# Ver vers√£o
agentcore version

# Ver informa√ß√µes do provedor
agentcore info

# Converter OpenAPI
agentcore convert api.json -o tools.py

# Validar OpenAPI
agentcore validate api.json
```

### Exemplos CLI

```bash
# Gerar ferramentas de uma API
agentcore convert https://petstore.swagger.io/v2/swagger.json

# Especificar URL base
agentcore convert api.json -b https://api.producao.com

# Diferentes formatos
agentcore convert api.json -f info  # mostrar informa√ß√µes
agentcore convert api.json -f names # listar nomes
```

## üé® Casos de Uso Avan√ßados

### 1. Multi-API Agent

```python
from agentCore.utils import api2tool
from agentCore.providers import get_llm
from agentCore.graphs import create_agent_graph

# Combinar m√∫ltiplas APIs
weather_tools = api2tool("weather_api.json")
news_tools = api2tool("news_api.json")
calendar_tools = api2tool("calendar_api.json")

# Juntar todas as ferramentas
all_tools = []
all_tools.extend([t['function'] for t in weather_tools])
all_tools.extend([t['function'] for t in news_tools])
all_tools.extend([t['function'] for t in calendar_tools])

# Criar agente super poderoso
llm = get_llm()
agent = create_agent_graph(llm, tools=all_tools)

# Usar para tarefas complexas
result = agent.invoke({
    "messages": [HumanMessage(content="Qual o clima hoje e agende uma reuni√£o √†s 15h")]
})
```

### 2. Customizar Comportamento

```python
# Usar provedor espec√≠fico mesmo com padr√£o configurado
llm = get_llm("openai")  # for√ßar OpenAI

# Gerar arquivo customizado
code = api2tool("api.json", output_format="file")
with open("minhas_tools.py", "w") as f:
    f.write(code)

# Adicionar logging personalizado
from agentCore.logger import get_logger
logger = get_logger("meu_agente")

def meu_agente_personalizado():
    logger.info("Iniciando meu agente")
    # ... sua l√≥gica aqui
    logger.success("Agente conclu√≠do")
```

### 3. Integra√ß√£o com Frameworks

#### FastAPI
```python
from fastapi import FastAPI
from agentCore.utils import api2tool
from agentCore.providers import get_llm

app = FastAPI()
llm = get_llm()

@app.post("/chat")
async def chat(message: str):
    # Usar agente aqui
    return {"response": "..."}
```

#### Flask
```python
from flask import Flask, request
from agentCore.utils import api2tool

app = Flask(__name__)
tools = api2tool("api.json")

@app.route("/api", methods=["POST"])
def api_endpoint():
    # Usar ferramentas aqui
    return {"result": "..."}
```

## üìö Exemplos Pr√°ticos

### Exemplo 1: Agente de E-commerce

```python
from agentCore.utils import api2tool
from agentCore.providers import get_llm
from agentCore.graphs import create_agent_graph

# Carregar API de e-commerce
tools = api2tool("ecommerce_api.json", base_url="https://loja.com/api")
tool_functions = [tool['function'] for tool in tools]

# Criar agente
llm = get_llm()
agent = create_agent_graph(llm, tools=tool_functions)

# Usar agente
questions = [
    "Mostre produtos em promo√ß√£o",
    "Qual o status do pedido #12345?",
    "Adicione produto X ao carrinho"
]

for q in questions:
    result = agent.invoke({"messages": [HumanMessage(content=q)]})
    print(f"P: {q}")
    print(f"R: {result['messages'][-1].content}\n")
```

### Exemplo 2: Sistema de Monitoramento

```python
from agentCore.utils import api2tool
from agentCore.logger import get_logger

logger = get_logger("monitor")

# APIs de monitoramento
monitoring_tools = api2tool("monitoring_api.json")

def check_system_health():
    logger.info("Iniciando verifica√ß√£o de sa√∫de do sistema")

    # Usar ferramentas para verificar m√©tricas
    for tool in monitoring_tools:
        try:
            result = tool['function']()
            logger.success(f"Check OK: {tool['schema']['name']}")
        except Exception as e:
            logger.error(f"Check FAILED: {tool['schema']['name']}: {e}")
```

## üîç Debug e Troubleshooting

### Verificar Configura√ß√£o

```python
from agentCore.providers import get_provider_info

# Ver configura√ß√£o atual
info = get_provider_info()
print(f"Provedor: {info['provider']}")
print(f"Configurado: {info}")

# Testar conex√£o
try:
    llm = get_llm()
    response = llm.invoke("test")
    print("‚úÖ LLM funcionando")
except Exception as e:
    print(f"‚ùå Erro no LLM: {e}")
```

### Logs Detalhados

```python
from agentCore.logger import get_logger

# Logger mais verboso
logger = get_logger("debug", level="DEBUG")
logger.debug("Informa√ß√£o detalhada")
```

### Testar APIs

```python
# Verificar se API est√° acess√≠vel
info = api2tool("api.json", output_format="info")
print(f"API: {info['title']}")
print(f"Ferramentas: {info['tool_count']}")

# Listar ferramentas dispon√≠veis
names = api2tool("api.json", output_format="names")
print("Ferramentas:", names)
```

## üöÄ Deploy em Produ√ß√£o

### 1. Configura√ß√£o para Produ√ß√£o

```python
import os
os.environ['MAIN_PROVIDER'] = 'bedrock'
os.environ['AWS_REGION'] = 'us-east-1'

# Usar IAM roles em vez de chaves hardcoded
from agentCore.providers import get_llm
llm = get_llm()  # Ser√° configurado via IAM role
```

### 2. Docker

```dockerfile
FROM python:3.11-slim
RUN pip install agentcore[aws]
COPY . .
CMD ["python", "app.py"]
```

### 3. Vari√°veis de Ambiente Seguras

```bash
# Use secrets managers em produ√ß√£o
export BEDROCK_MODEL=anthropic.claude-3-sonnet-20240229-v1:0
export AWS_REGION=us-east-1
# N√£o inclua chaves de API em vari√°veis - use IAM roles
```

## üìà Performance e Otimiza√ß√£o

### Cache de Ferramentas
```python
# Gerar uma vez, reutilizar
tools = api2tool("api.json")
# Salvar tools se necess√°rio para reuso
```

### Logging Eficiente
```python
from agentCore.logger import get_logger

# Logger espec√≠fico para cada componente
api_logger = get_logger("api")
agent_logger = get_logger("agent")
```

### Modelos Otimizados
```python
# Usar modelos menores para tarefas simples
os.environ['BEDROCK_MODEL'] = 'anthropic.claude-3-haiku-20240307-v1:0'
```

---

Para mais exemplos e documenta√ß√£o completa, consulte o README.md e os arquivos de exemplo inclusos na biblioteca.