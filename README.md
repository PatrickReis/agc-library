# AgentCore v2.0 ğŸš€

Biblioteca Python enterprise-ready para construÃ§Ã£o de agentes de IA com orquestraÃ§Ã£o multi-framework, armazenamento vetorial multi-provedor e capacidades avanÃ§adas de avaliaÃ§Ã£o.

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![AgentCore](https://img.shields.io/badge/AgentCore-v2.0-green.svg)]()

## ğŸŒŸ CaracterÃ­sticas Principais v2.0

### ğŸ”§ **Multi-Framework Orchestration**
- **CrewAI**: CenÃ¡rios empresariais complexos com multi-agentes
- **LangGraph**: Fluxos simples e sequenciais
- **AutoGen**: Conversas e execuÃ§Ã£o de cÃ³digo
- **Auto-seleÃ§Ã£o**: Escolha automÃ¡tica baseada no caso de uso

### ğŸ—„ï¸ **Multi-Provider Vector Storage**
- **AWS OpenSearch**: ProduÃ§Ã£o empresarial escalÃ¡vel
- **AWS Kendra**: Busca empresarial com ML
- **AWS S3 + FAISS**: SoluÃ§Ã£o econÃ´mica
- **Qdrant**: Vector search nativo e rÃ¡pido
- **ChromaDB**: Desenvolvimento local
- **Pinecone**: Vector search gerenciado
- **FAISS**: Research e alta performance

### âœ‚ï¸ **Advanced Chunking Strategies**
- **Semantic**: Agrupamento por similaridade
- **Recursive**: Preserva boundaries hierÃ¡rquicos
- **Sliding Window**: Overlap controlado
- **Markdown-Aware**: Preserva estrutura de documentaÃ§Ã£o
- **Code-Aware**: EspecÃ­fico para cÃ³digo fonte
- **Auto-detection**: DetecÃ§Ã£o automÃ¡tica do tipo de conteÃºdo

### ğŸ­ **Production-Ready Features**
- **Multi-provedor LLM**: AWS Bedrock, OpenAI, Ollama, Google Gemini
- **Sistema de avaliaÃ§Ã£o**: Framework OpenAI/evals para validaÃ§Ã£o
- **ComparaÃ§Ã£o de modelos**: Performance entre diferentes LLMs
- **Reasoning avanÃ§ado**: Chain-of-thought e step-by-step
- **Observabilidade completa**: Tracing, mÃ©tricas e visualizaÃ§Ã£o
- **Auto-configuraÃ§Ã£o**: Baseada em caso de uso e ambiente
- **Health monitoring**: Checks automÃ¡ticos e mÃ©tricas de uso
- **AWS Integration**: Bedrock, OpenSearch, S3, Kendra
- **ConversÃ£o API2Tool**: OpenAPI para ferramentas LangGraph

## ğŸ“¦ InstalaÃ§Ã£o

### ğŸ”§ InstalaÃ§Ã£o Local (Desenvolvimento)
```bash
# Clone do repositÃ³rio
git clone <url-do-repositorio>
cd agentcore

# Ambiente virtual (recomendado)
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou venv\Scripts\activate  # Windows

# InstalaÃ§Ã£o em modo desenvolvimento
pip install -e .

# Ou com todas as dependÃªncias
pip install -e .[aws,crewai,semantic,qdrant,dev]
```

### ğŸ“‹ Requisitos do Sistema
- Python 3.8 ou superior
- pip (gerenciador de pacotes Python)
- Acesso Ã  internet para download das dependÃªncias

### ğŸ“¦ InstalaÃ§Ã£o via PyPI (Quando DisponÃ­vel)
```bash
# InstalaÃ§Ã£o bÃ¡sica
pip install agentcore

# Para produÃ§Ã£o AWS
pip install agentcore[aws]

# Para desenvolvimento avanÃ§ado
pip install agentcore[crewai,semantic,qdrant]

# InstalaÃ§Ã£o completa
pip install agentcore[full]
```

### ğŸ³ Docker
```dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
RUN pip install agentcore[aws]
COPY . .
CMD ["python", "app.py"]
```

## ğŸš€ InÃ­cio RÃ¡pido

### ğŸ¯ Conceitos Fundamentais

1. **api2tool**: Converte APIs OpenAPI em ferramentas utilizÃ¡veis
2. **Multi-Framework**: OrquestraÃ§Ã£o inteligente (CrewAI, LangGraph, AutoGen)
3. **Multi-Provider**: Storage vetorial flexÃ­vel (AWS, Qdrant, ChromaDB, etc.)
4. **Auto-Configuration**: ConfiguraÃ§Ã£o baseada em caso de uso
5. **Advanced Features**: AvaliaÃ§Ã£o, observabilidade, chunking inteligente

### ğŸ”§ Uso BÃ¡sico

```python
from agentCore.utils import api2tool
from agentCore.providers import get_llm
from agentCore import get_orchestrator, get_vector_store

# 1. ConfiguraÃ§Ã£o automÃ¡tica baseada no ambiente
llm = get_llm()  # Auto-seleciona provedor
orchestrator = get_orchestrator("auto")  # Auto-seleciona framework
vector_store = get_vector_store()  # Auto-configura storage

# 2. ConversÃ£o de API para ferramentas
tools = api2tool("./api.json")
print(f"Geradas {len(tools)} ferramentas")
```

### ğŸ¢ ConfiguraÃ§Ã£o Empresarial

```python
from agentCore import auto_configure_vector_store, get_orchestrator

# Auto-configuraÃ§Ã£o para ambiente empresarial
store = auto_configure_vector_store(
    use_case="enterprise",
    environment="production",
    budget="high"
)
# Auto-seleciona: AWS OpenSearch

orchestrator = get_orchestrator("auto",
    use_case="enterprise",
    complexity="high",
    team_size="large"
)
# Auto-seleciona: CrewAI
```

### ğŸ§ª Desenvolvimento Local

```python
# Setup rÃ¡pido para desenvolvimento
store = auto_configure_vector_store("development", "development", "low")
# Auto-seleciona: ChromaDB local

orchestrator = get_orchestrator("auto", complexity="low")
# Auto-seleciona: LangGraph
```

### ğŸ“Š AvaliaÃ§Ã£o e Monitoramento

```python
from agentCore.evaluation import PromptEvaluator, ModelComparator
from agentCore.observability import get_tracer

# Sistema de avaliaÃ§Ã£o
evaluator = PromptEvaluator()
results = evaluator.evaluate_dataset(test_cases)

# ComparaÃ§Ã£o de modelos
comparator = ModelComparator(["bedrock", "openai"])
comparison = comparator.compare_on_dataset(test_cases)

# Observabilidade
tracer = get_tracer()
with tracer.trace_execution("complex_task"):
    # Sua operaÃ§Ã£o aqui
    pass
```

## âš™ï¸ ConfiguraÃ§Ã£o

### ğŸ“ Arquivo .env

```bash
# ========================================
# CONFIGURAÃ‡ÃƒO PRINCIPAL
# ========================================
MAIN_PROVIDER=bedrock
ORCHESTRATOR_TYPE=auto
VECTOR_STORE_TYPE=auto

# ========================================
# AWS BEDROCK (PRODUÃ‡ÃƒO RECOMENDADA)
# ========================================
AWS_REGION=us-east-1
AWS_ACCESS_KEY_ID=your_access_key
AWS_SECRET_ACCESS_KEY=your_secret_key
BEDROCK_MODEL=anthropic.claude-3-sonnet-20240229-v1:0
BEDROCK_EMBEDDINGS_MODEL=amazon.titan-embed-text-v1

# ========================================
# VECTOR STORES
# ========================================
# AWS OpenSearch
OPENSEARCH_ENDPOINT=https://your-domain.us-east-1.es.amazonaws.com
OPENSEARCH_REGION=us-east-1

# Qdrant
QDRANT_URL=http://localhost:6333
QDRANT_API_KEY=your_qdrant_key

# Pinecone
PINECONE_API_KEY=your_pinecone_key
PINECONE_ENVIRONMENT=us-east1-gcp

# ========================================
# OUTROS PROVEDORES LLM
# ========================================
# OpenAI
OPENAI_API_KEY=your_openai_key
OPENAI_MODEL=gpt-4

# Ollama (Local)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3:latest

# Google Gemini
GEMINI_API_KEY=your_gemini_key
GEMINI_MODEL=gemini-1.5-flash

# ========================================
# CONFIGURAÃ‡Ã•ES AVANÃ‡ADAS
# ========================================
LOG_LEVEL=INFO
TRACING_ENABLED=true
CACHE_ENABLED=true
CHUNKING_STRATEGY=auto
```

### ğŸ”§ ConfiguraÃ§Ã£o por CÃ³digo

```python
import os
from agentCore import configure_environment

# ConfiguraÃ§Ã£o programÃ¡tica
configure_environment(
    main_provider="bedrock",
    vector_store="aws_opensearch",
    orchestrator="crewai",
    environment="production"
)

# Ou via variÃ¡veis
os.environ.update({
    'MAIN_PROVIDER': 'bedrock',
    'AWS_REGION': 'us-east-1',
    'VECTOR_STORE_TYPE': 'aws_opensearch'
})
```

### ğŸ“Š Logging e Monitoramento

```python
from agentCore.logger import get_logger
from agentCore.observability import enable_tracing, get_metrics

# Logger personalizado
logger = get_logger("minha_app", level="DEBUG")
logger.info("AplicaÃ§Ã£o iniciada")
logger.success("OperaÃ§Ã£o bem-sucedida")
logger.tool_execution("api_call", duration=1.2)

# Habilitar tracing
enable_tracing(enabled=True, output_file="trace.json")

# MÃ©tricas de uso
metrics = get_metrics()
print(f"ExecuÃ§Ãµes: {metrics['total_executions']}")
```

## ğŸ“š Exemplos PrÃ¡ticos

### ğŸ¯ Exemplo 1: Setup Empresarial Completo

```python
from agentCore import (
    auto_configure_vector_store,
    get_orchestrator,
    get_llm
)
from agentCore.utils import api2tool
from agentCore.evaluation import PromptEvaluator
from agentCore.observability import get_tracer

# 1. Auto-configuraÃ§Ã£o para produÃ§Ã£o
store = auto_configure_vector_store(
    use_case="enterprise",
    environment="production",
    budget="high"
)
# Resultado: AWS OpenSearch configurado automaticamente

orchestrator = get_orchestrator("auto",
    use_case="enterprise",
    complexity="high"
)
# Resultado: CrewAI selecionado automaticamente

llm = get_llm()  # Bedrock por padrÃ£o

# 2. Carregar APIs empresariais
weather_tools = api2tool("./weather_api.json")
crm_tools = api2tool("./crm_api.json")
analytics_tools = api2tool("./analytics_api.json")

# 3. Criar agente empresarial
all_tools = []
for tools in [weather_tools, crm_tools, analytics_tools]:
    all_tools.extend([tool['function'] for tool in tools])

agent = orchestrator.create_agent(llm, tools=all_tools)

# 4. Sistema de avaliaÃ§Ã£o
evaluator = PromptEvaluator()
test_cases = [
    {"input": "Clima em SP?", "expected": "temperatura"},
    {"input": "Vendas Q4?", "expected": "relatÃ³rio"}
]
results = evaluator.evaluate_dataset(test_cases, agent)
print(f"AcurÃ¡cia: {results['accuracy']:.2%}")

# 5. Observabilidade
tracer = get_tracer()
with tracer.trace_execution("enterprise_query"):
    response = agent.invoke({
        "messages": [{"role": "user", "content": "AnÃ¡lise completa do Q4"}]
    })
```

### ğŸ§ª Exemplo 2: Desenvolvimento Local com Ollama

```python
from agentCore import get_vector_store, get_orchestrator
from agentCore.providers import get_llm
from agentCore.chunking import get_chunking_strategy, ChunkingMethod

# Setup local otimizado
store = get_vector_store("chromadb", {"persist_directory": "./local_db"})
orchestrator = get_orchestrator("langgraph")  # Simples para desenvolvimento
llm = get_llm("ollama")

# Chunking inteligente
chunker = get_chunking_strategy(ChunkingMethod.SEMANTIC)
documents = [
    "DocumentaÃ§Ã£o tÃ©cnica longa...",
    "CÃ³digo Python com funÃ§Ãµes...",
    "Artigo cientÃ­fico..."
]

for doc in documents:
    chunks = chunker.chunk(doc, metadata={"source": "local"})
    store.add_documents(chunks)

# Query com contexto
results = store.similarity_search("Como implementar autenticaÃ§Ã£o?")
context = "\n".join([r.page_content for r in results])

response = llm.invoke(f"Contexto: {context}\n\nPergunta: Como implementar autenticaÃ§Ã£o?")
print(response.content)
```

### ğŸ”¬ Exemplo 3: ComparaÃ§Ã£o de Modelos

```python
from agentCore.evaluation import ModelComparator
from agentCore.providers import get_llm

# Configurar mÃºltiplos modelos
models = {
    "bedrock_sonnet": get_llm("bedrock"),
    "openai_gpt4": get_llm("openai"),
    "ollama_llama3": get_llm("ollama")
}

# Dataset de teste
test_cases = [
    {
        "input": "Explique machine learning",
        "expected_keywords": ["algoritmo", "dados", "treinamento"]
    },
    {
        "input": "Como fazer uma API REST?",
        "expected_keywords": ["HTTP", "endpoint", "JSON"]
    }
]

# ComparaÃ§Ã£o automÃ¡tica
comparator = ModelComparator(models)
results = comparator.compare_on_dataset(test_cases)

# Resultados detalhados
for model_name, metrics in results.items():
    print(f"{model_name}:")
    print(f"  AcurÃ¡cia: {metrics['accuracy']:.2%}")
    print(f"  LatÃªncia: {metrics['avg_latency']:.2f}s")
    print(f"  Score: {metrics['weighted_score']:.2f}")
```

### ğŸ¨ Exemplo 4: Chunking AvanÃ§ado

```python
from agentCore.chunking import (
    get_chunking_strategy,
    ChunkingMethod,
    detect_content_type
)

# Auto-detecÃ§Ã£o de tipo de conteÃºdo
def smart_chunk(text, content_type="auto"):
    if content_type == "auto":
        content_type = detect_content_type(text)

    strategy_map = {
        "markdown": ChunkingMethod.MARKDOWN_AWARE,
        "code": ChunkingMethod.CODE_AWARE,
        "academic": ChunkingMethod.SEMANTIC,
        "general": ChunkingMethod.RECURSIVE
    }

    method = strategy_map.get(content_type, ChunkingMethod.RECURSIVE)
    chunker = get_chunking_strategy(method)
    return chunker.chunk(text)

# Exemplos com diferentes tipos
markdown_doc = """
# TÃ­tulo Principal
## SeÃ§Ã£o 1
ConteÃºdo da seÃ§Ã£o...
### SubseÃ§Ã£o
Mais conteÃºdo...
"""

code_doc = """
def calculate_total(items):
    \"\"\"Calcula o total dos itens\"\"\"
    return sum(item.price for item in items)

class ShoppingCart:
    def __init__(self):
        self.items = []
"""

# Chunking automÃ¡tico
md_chunks = smart_chunk(markdown_doc)  # Detecta: markdown
code_chunks = smart_chunk(code_doc)    # Detecta: code

print(f"Markdown: {len(md_chunks)} chunks")
print(f"Code: {len(code_chunks)} chunks")
```

## ğŸ—ï¸ Arquitetura v2.0

```
agentCore/
â”œâ”€â”€ utils/              # ğŸ”§ Utilidades principais
â”‚   â”œâ”€â”€ api2tool.py        # ConversÃ£o OpenAPI â†’ Ferramentas
â”‚   â””â”€â”€ openapi_to_tools.py
â”œâ”€â”€ providers/          # ğŸ¤– Provedores LLM multi-cloud
â”‚   â”œâ”€â”€ llm_providers.py   # AWS Bedrock, OpenAI, Ollama, Gemini
â”‚   â””â”€â”€ vector_store_factory.py
â”œâ”€â”€ orchestration/      # ğŸ­ Multi-framework orchestration
â”‚   â”œâ”€â”€ crewai_orchestrator.py    # CrewAI para empresas
â”‚   â”œâ”€â”€ langgraph_orchestrator.py # LangGraph para fluxos simples
â”‚   â”œâ”€â”€ autogen_orchestrator.py   # AutoGen para conversas
â”‚   â””â”€â”€ auto_selector.py          # Auto-seleÃ§Ã£o inteligente
â”œâ”€â”€ vector_stores/      # ğŸ—„ï¸ Storage vetorial multi-provedor
â”‚   â”œâ”€â”€ aws_opensearch.py         # AWS OpenSearch
â”‚   â”œâ”€â”€ aws_kendra.py             # AWS Kendra
â”‚   â”œâ”€â”€ aws_s3_faiss.py           # AWS S3 + FAISS
â”‚   â”œâ”€â”€ qdrant_provider.py        # Qdrant local/cloud
â”‚   â”œâ”€â”€ chromadb_provider.py      # ChromaDB
â”‚   â”œâ”€â”€ pinecone_provider.py      # Pinecone
â”‚   â””â”€â”€ faiss_provider.py         # FAISS local
â”œâ”€â”€ evaluation/         # ğŸ“Š Sistema de avaliaÃ§Ã£o
â”‚   â”œâ”€â”€ prompt_evaluator.py       # OpenAI/evals style
â”‚   â”œâ”€â”€ model_comparison.py       # ComparaÃ§Ã£o de modelos
â”‚   â””â”€â”€ metrics.py                # MÃ©tricas de performance
â”œâ”€â”€ reasoning/          # ğŸ§  Reasoning avanÃ§ado
â”‚   â”œâ”€â”€ chain_of_thought.py       # Chain of Thought
â”‚   â””â”€â”€ step_by_step.py           # Step-by-step reasoning
â”œâ”€â”€ chunking/           # âœ‚ï¸ EstratÃ©gias de chunking
â”‚   â”œâ”€â”€ text_chunker.py           # MÃºltiplas estratÃ©gias
â”‚   â”œâ”€â”€ semantic_chunker.py       # Chunking semÃ¢ntico
â”‚   â”œâ”€â”€ code_chunker.py           # EspecÃ­fico para cÃ³digo
â”‚   â””â”€â”€ content_detector.py       # Auto-detecÃ§Ã£o de tipo
â”œâ”€â”€ observability/      # ğŸ” Observabilidade completa
â”‚   â”œâ”€â”€ agent_tracer.py           # Tracing de execuÃ§Ãµes
â”‚   â”œâ”€â”€ metrics_collector.py      # Coleta de mÃ©tricas
â”‚   â””â”€â”€ visualization.py          # VisualizaÃ§Ã£o de traces
â”œâ”€â”€ graphs/             # ğŸ•¸ï¸ OrquestraÃ§Ã£o LangGraph (legado)
â”‚   â””â”€â”€ graph.py
â””â”€â”€ logger/             # ğŸ“ Sistema de logging
    â””â”€â”€ logger.py
```

### ğŸ”„ Fluxo de Auto-ConfiguraÃ§Ã£o

```mermaid
graph TD
    A[InÃ­cio] --> B[Detectar Caso de Uso]
    B --> C[Analisar Ambiente]
    C --> D[Avaliar OrÃ§amento]
    D --> E{Tipo de Setup?}
    E -->|Desenvolvimento| F[ChromaDB + LangGraph + Ollama]
    E -->|Empresa| G[AWS OpenSearch + CrewAI + Bedrock]
    E -->|Pesquisa| H[Qdrant + Semantic Chunking + Bedrock]
    F --> I[ConfiguraÃ§Ã£o Aplicada]
    G --> I
    H --> I
```

## ğŸ” Casos de Uso e RecomendaÃ§Ãµes

### ğŸ¢ **Empresa/ProduÃ§Ã£o**
```python
# Setup automÃ¡tico para produÃ§Ã£o
store = auto_configure_vector_store("enterprise", "production", "high")
# âœ… Resultado: AWS OpenSearch

orchestrator = get_orchestrator("auto", use_case="enterprise")
# âœ… Resultado: CrewAI com multi-agentes

llm = get_llm("bedrock")  # Claude 3 Sonnet
```

**CaracterÃ­sticas:**
- â˜ï¸ AWS OpenSearch para vectors (escalÃ¡vel)
- ğŸ¤– CrewAI para orquestraÃ§Ã£o complexa
- ğŸ”’ AWS Bedrock para LLM (seguro)
- ğŸ“Š Monitoring e health checks integrados

### ğŸ§ª **Desenvolvimento Local**
```python
# Setup rÃ¡pido para desenvolvimento
store = auto_configure_vector_store("development", "development", "low")
# âœ… Resultado: ChromaDB local

orchestrator = get_orchestrator("auto", complexity="low")
# âœ… Resultado: LangGraph

llm = get_llm("ollama")  # Llama3 local
```

**CaracterÃ­sticas:**
- ğŸ’» ChromaDB local (sem custos)
- ğŸ”— LangGraph simples
- ğŸ  Ollama para LLM local
- âš¡ Setup em segundos

### ğŸ”¬ **Pesquisa/Academia**
```python
# Setup para pesquisa
store = auto_configure_vector_store("research", "development", "medium")
# âœ… Resultado: Qdrant com embeddings semÃ¢nticos

chunker = get_chunking_strategy(ChunkingMethod.SEMANTIC)
# âœ… Chunking baseado em similaridade semÃ¢ntica
```

**CaracterÃ­sticas:**
- ğŸ§  Qdrant para vector search avanÃ§ado
- ğŸ“š Chunking semÃ¢ntico para papers
- ğŸ“Š MÃ©tricas detalhadas de performance
- ğŸ”¬ Ferramentas de anÃ¡lise

## ğŸ“Š ComparaÃ§Ã£o de VersÃµes

| **Aspecto** | **v1.0** | **v2.0** |
|-------------|----------|----------|
| **Orchestration** | âŒ Apenas LangGraph | âœ… Multi-framework (CrewAI, LangGraph, AutoGen) |
| **Vector Storage** | âŒ Apenas ChromaDB | âœ… 7+ providers (AWS, Qdrant, Pinecone, etc.) |
| **Chunking** | âŒ Split bÃ¡sico | âœ… 7+ estratÃ©gias inteligentes |
| **Production** | âŒ Apenas desenvolvimento | âœ… AWS-native, enterprise-ready |
| **Configuration** | âŒ Manual | âœ… Auto-configuraÃ§Ã£o baseada em use case |
| **Scalability** | âŒ Limitada | âœ… Enterprise-grade |
| **Evaluation** | âŒ NÃ£o disponÃ­vel | âœ… OpenAI/evals + comparaÃ§Ã£o de modelos |
| **Observability** | âŒ Logs bÃ¡sicos | âœ… Tracing completo + mÃ©tricas |

## ğŸš€ Migration Guide (v1.0 â†’ v2.0)

### âœ… **MudanÃ§as NÃ£o-Disruptivas**
```python
# âœ… CÃ³digo v1.0 continua funcionando
from agentCore.utils import api2tool
from agentCore.providers import get_llm
from agentCore.graphs import create_agent_graph

# Funciona exatamente igual
tools = api2tool("api.json")
llm = get_llm()
agent = create_agent_graph(llm, tools)
```

### ğŸ†• **Novas Funcionalidades Opcionais**
```python
# ğŸ†• Adicione gradualmente as melhorias
from agentCore import get_vector_store, get_orchestrator

# Upgrade incremental
vector_store = get_vector_store()  # Auto-configure
orchestrator = get_orchestrator("auto")  # Auto-select
```

## ğŸ“‹ Changelog

### [2.0.0] - 2024-12-29

#### âœ¨ Adicionado
- **Multi-Framework Orchestration**: CrewAI, LangGraph, AutoGen
- **Multi-Provider Vector Storage**: AWS OpenSearch, Kendra, S3+FAISS, Qdrant, Pinecone
- **Advanced Chunking**: 7+ estratÃ©gias inteligentes
- **Auto-Configuration**: Baseada em caso de uso
- **Evaluation Framework**: OpenAI/evals style
- **Model Comparison**: Performance entre LLMs
- **Advanced Observability**: Tracing e mÃ©tricas
- **Production Features**: AWS integration, health monitoring

#### ğŸ”„ Melhorado
- Sistema de providers LLM mais robusto
- Logging com mais nÃ­veis e contexto
- DocumentaÃ§Ã£o centralizada
- Performance geral

#### ğŸ”’ Mantido (Backward Compatible)
- Interface `api2tool` original
- ConfiguraÃ§Ã£o via variÃ¡veis de ambiente
- Estrutura bÃ¡sica de providers

### [1.0.0] - 2024-12-29

#### âœ¨ Primeira Release
- Funcionalidade principal `api2tool`
- Suporte multi-provedor LLM bÃ¡sico
- Sistema de logging
- OrquestraÃ§Ã£o LangGraph
- IntegraÃ§Ã£o ChromaDB

## âœ… VerificaÃ§Ã£o da InstalaÃ§Ã£o

### ğŸ§ª Teste BÃ¡sico
```python
# Testar importaÃ§Ã£o
from agentCore.utils import api2tool
from agentCore.providers import get_llm, get_provider_info

# Verificar provedor
info = get_provider_info()
print(f"Provedor ativo: {info['provider']}")

# Testar conversÃ£o
tools = api2tool('https://petstore.swagger.io/v2/swagger.json', output_format='info')
print(f"API: {tools['title']} - {tools['tool_count']} ferramentas")
```

### ğŸ“Š Teste Completo
```python
# Execute o demo completo
python e2e_demo_local.py
```

### ğŸ”§ Debug
```python
from agentCore.providers import get_provider_info

# Verificar configuraÃ§Ã£o
info = get_provider_info()
print(f"ConfiguraÃ§Ã£o: {info}")

# Testar conexÃ£o
try:
    llm = get_llm()
    response = llm.invoke("test")
    print("âœ… LLM funcionando")
except Exception as e:
    print(f"âŒ Erro: {e}")
```

## ğŸ”§ Desenvolvimento

### ğŸ› ï¸ Setup do Ambiente
```bash
# Clone e setup
git clone <repositorio>
cd agentcore
python -m venv venv
source venv/bin/activate
pip install -e .[dev]
```

### ğŸ§ª Testes
```bash
# Executar testes
pytest tests/ -v

# Com cobertura
pytest --cov=agentCore --cov-report=html

# Testes especÃ­ficos
pytest tests/test_evaluation.py -v
```

### ğŸ“ FormataÃ§Ã£o
```bash
# FormataÃ§Ã£o automÃ¡tica
black agentCore/
isort agentCore/
flake8 agentCore/

# Pre-commit hooks
pre-commit install
pre-commit run --all-files
```

### ğŸ—ï¸ Build
```bash
# Build da distribuiÃ§Ã£o
python -m build

# Verificar build
twine check dist/*
```

## ğŸ“„ LicenÃ§a

MIT License - veja o arquivo [LICENSE](LICENSE) para detalhes.

## ğŸ¤ Contribuindo

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add: AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

### ğŸ“‹ Guidelines
- Siga o padrÃ£o de cÃ³digo existente
- Adicione testes para novas funcionalidades
- Atualize a documentaÃ§Ã£o quando necessÃ¡rio
- Use conventional commits

## ğŸ“ Suporte

- **ğŸ› Issues**: [GitHub Issues](https://github.com/your-org/agent-core/issues)
- **ğŸ’¬ DiscussÃµes**: [GitHub Discussions](https://github.com/your-org/agent-core/discussions)
- **ğŸ“– DocumentaÃ§Ã£o**: Veja exemplos acima
- **ğŸ“§ Contato**: [email@exemplo.com](mailto:email@exemplo.com)

---

## ğŸ‰ ConclusÃ£o

A **AgentCore v2.0** Ã© uma biblioteca enterprise-ready que resolve todos os desafios de produÃ§Ã£o:

- âœ… **Flexibilidade**: Multi-framework, multi-provider, multi-estratÃ©gia
- âœ… **Escalabilidade**: AWS-native para produÃ§Ã£o enterprise
- âœ… **Simplicidade**: Auto-configuraÃ§Ã£o baseada em caso de uso
- âœ… **Observabilidade**: Tracing, mÃ©tricas e avaliaÃ§Ã£o completa
- âœ… **Compatibilidade**: MantÃ©m interface v1.0 funcionando

**RecomendaÃ§Ã£o**: Use para projetos reais de produÃ§Ã£o com confianÃ§a! ğŸš€