# AgentCore v2.0 üöÄ

Biblioteca Python enterprise-ready para constru√ß√£o de agentes de IA com orquestra√ß√£o multi-framework, armazenamento vetorial multi-provedor e capacidades avan√ßadas de avalia√ß√£o.

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![AgentCore](https://img.shields.io/badge/AgentCore-v2.0-green.svg)]()

## üåü Caracter√≠sticas Principais v2.0

### üîß **Multi-Framework Orchestration**
- **CrewAI**: Cen√°rios empresariais complexos com multi-agentes
- **LangGraph**: Fluxos simples e sequenciais
- **AutoGen**: Conversas e execu√ß√£o de c√≥digo
- **Auto-sele√ß√£o**: Escolha autom√°tica baseada no caso de uso

### üóÑÔ∏è **Multi-Provider Vector Storage**
- **AWS OpenSearch**: Produ√ß√£o empresarial escal√°vel
- **AWS Kendra**: Busca empresarial com ML
- **AWS S3 + FAISS**: Solu√ß√£o econ√¥mica
- **Qdrant**: Vector search nativo e r√°pido
- **ChromaDB**: Desenvolvimento local
- **Pinecone**: Vector search gerenciado
- **FAISS**: Research e alta performance

### ‚úÇÔ∏è **Advanced Chunking Strategies**
- **Semantic**: Agrupamento por similaridade
- **Recursive**: Preserva boundaries hier√°rquicos
- **Sliding Window**: Overlap controlado
- **Markdown-Aware**: Preserva estrutura de documenta√ß√£o
- **Code-Aware**: Espec√≠fico para c√≥digo fonte
- **Auto-detection**: Detec√ß√£o autom√°tica do tipo de conte√∫do

### üè≠ **Production-Ready Features**
- **Multi-provedor LLM**: AWS Bedrock, OpenAI, Ollama, Google Gemini
- **Sistema de avalia√ß√£o**: Framework OpenAI/evals para valida√ß√£o
- **Compara√ß√£o de modelos**: Performance entre diferentes LLMs
- **Reasoning avan√ßado**: Chain-of-thought e step-by-step
- **Observabilidade completa**: Tracing, m√©tricas e visualiza√ß√£o
- **Auto-configura√ß√£o**: Baseada em caso de uso e ambiente
- **Health monitoring**: Checks autom√°ticos e m√©tricas de uso
- **AWS Integration**: Bedrock, OpenSearch, S3, Kendra
- **Convers√£o API2Tool**: OpenAPI para ferramentas LangGraph

## üì¶ Instala√ß√£o

### üîß Instala√ß√£o Local (Desenvolvimento)
```bash
# Clone do reposit√≥rio
git clone <url-do-repositorio>
cd agentcore

# Ambiente virtual (recomendado)
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou venv\Scripts\activate  # Windows

# Instala√ß√£o em modo desenvolvimento
pip install -e .

# Ou com todas as depend√™ncias
pip install -e .[aws,crewai,semantic,qdrant,dev]
```

### üìã Requisitos do Sistema
- Python 3.8 ou superior
- pip (gerenciador de pacotes Python)
- Acesso √† internet para download das depend√™ncias

### üì¶ Instala√ß√£o via PyPI (Quando Dispon√≠vel)
```bash
# Instala√ß√£o b√°sica
pip install agentcore

# Para produ√ß√£o AWS
pip install agentcore[aws]

# Para desenvolvimento avan√ßado
pip install agentcore[crewai,semantic,qdrant]

# Instala√ß√£o completa
pip install agentcore[full]
```

### üê≥ Docker
```dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
RUN pip install agentcore[aws]
COPY . .
CMD ["python", "app.py"]
```

## üöÄ In√≠cio R√°pido

### üéØ Conceitos Fundamentais

1. **api2tool**: Converte APIs OpenAPI em ferramentas utiliz√°veis
2. **Multi-Framework**: Orquestra√ß√£o inteligente (CrewAI, LangGraph, AutoGen)
3. **Multi-Provider**: Storage vetorial flex√≠vel (AWS, Qdrant, ChromaDB, etc.)
4. **Auto-Configuration**: Configura√ß√£o baseada em caso de uso
5. **Advanced Features**: Avalia√ß√£o, observabilidade, chunking inteligente

### üîß Uso B√°sico

```python
from agentCore.utils import api2tool
from agentCore.providers import get_llm
from agentCore import get_orchestrator, get_vector_store

# 1. Configura√ß√£o autom√°tica baseada no ambiente
llm = get_llm()  # Auto-seleciona provedor
orchestrator = get_orchestrator("auto")  # Auto-seleciona framework
vector_store = get_vector_store()  # Auto-configura storage

# 2. Convers√£o de API para ferramentas
tools = api2tool("./api.json")
print(f"Geradas {len(tools)} ferramentas")
```

### üè¢ Configura√ß√£o Empresarial

```python
from agentCore import auto_configure_vector_store, get_orchestrator

# Auto-configura√ß√£o para ambiente empresarial
store = auto_configure_vector_store(
    use_case="enterprise",
    environment="production",
    budget="high"
)
# Auto-seleciona: AWS OpenSearch

orchestrator = get_orchestrator("auto",
    use_case="enterprise",
    complexity="high",
    team_size="large"
)
# Auto-seleciona: CrewAI
```

### üß™ Desenvolvimento Local

```python
# Setup r√°pido para desenvolvimento
store = auto_configure_vector_store("development", "development", "low")
# Auto-seleciona: ChromaDB local

orchestrator = get_orchestrator("auto", complexity="low")
# Auto-seleciona: LangGraph
```

### üìä Avalia√ß√£o e Monitoramento

```python
from agentCore.evaluation import PromptEvaluator, ModelComparator
from agentCore.observability import get_tracer

# Sistema de avalia√ß√£o
evaluator = PromptEvaluator()
results = evaluator.evaluate_dataset(test_cases)

# Compara√ß√£o de modelos
comparator = ModelComparator(["bedrock", "openai"])
comparison = comparator.compare_on_dataset(test_cases)

# Observabilidade
tracer = get_tracer()
with tracer.trace_execution("complex_task"):
    # Sua opera√ß√£o aqui
    pass
```

## ‚öôÔ∏è Configura√ß√£o

### üìÅ Arquivo .env

```bash
# ========================================
# CONFIGURA√á√ÉO PRINCIPAL
# ========================================
MAIN_PROVIDER=bedrock
ORCHESTRATOR_TYPE=auto
VECTOR_STORE_TYPE=auto

# ========================================
# AWS BEDROCK (PRODU√á√ÉO RECOMENDADA)
# ========================================
AWS_REGION=us-east-1
AWS_ACCESS_KEY_ID=your_access_key
AWS_SECRET_ACCESS_KEY=your_secret_key
BEDROCK_MODEL=anthropic.claude-3-sonnet-20240229-v1:0
BEDROCK_EMBEDDINGS_MODEL=amazon.titan-embed-text-v1

# ========================================
# VECTOR STORES
# ========================================
# AWS OpenSearch
OPENSEARCH_ENDPOINT=https://your-domain.us-east-1.es.amazonaws.com
OPENSEARCH_REGION=us-east-1

# Qdrant
QDRANT_URL=http://localhost:6333
QDRANT_API_KEY=your_qdrant_key

# Pinecone
PINECONE_API_KEY=your_pinecone_key
PINECONE_ENVIRONMENT=us-east1-gcp

# ========================================
# OUTROS PROVEDORES LLM
# ========================================
# OpenAI
OPENAI_API_KEY=your_openai_key
OPENAI_MODEL=gpt-4

# Ollama (Local)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3:latest

# Google Gemini
GEMINI_API_KEY=your_gemini_key
GEMINI_MODEL=gemini-1.5-flash

# ========================================
# CONFIGURA√á√ïES AVAN√áADAS
# ========================================
LOG_LEVEL=INFO
TRACING_ENABLED=true
CACHE_ENABLED=true
CHUNKING_STRATEGY=auto
```

### üîß Configura√ß√£o por C√≥digo

```python
import os
from agentCore import configure_environment

# Configura√ß√£o program√°tica
configure_environment(
    main_provider="bedrock",
    vector_store="aws_opensearch",
    orchestrator="crewai",
    environment="production"
)

# Ou via vari√°veis
os.environ.update({
    'MAIN_PROVIDER': 'bedrock',
    'AWS_REGION': 'us-east-1',
    'VECTOR_STORE_TYPE': 'aws_opensearch'
})
```

### üìä Logging e Monitoramento

```python
from agentCore.logger import get_logger
from agentCore.observability import enable_tracing, get_metrics

# Logger personalizado
logger = get_logger("minha_app", level="DEBUG")
logger.info("Aplica√ß√£o iniciada")
logger.success("Opera√ß√£o bem-sucedida")
logger.tool_execution("api_call", duration=1.2)

# Habilitar tracing
enable_tracing(enabled=True, output_file="trace.json")

# M√©tricas de uso
metrics = get_metrics()
print(f"Execu√ß√µes: {metrics['total_executions']}")
```

## üìö Exemplos Pr√°ticos

### üéØ Exemplo 1: Setup Empresarial Completo

```python
from agentCore import (
    auto_configure_vector_store,
    get_orchestrator,
    get_llm
)
from agentCore.utils import api2tool
from agentCore.evaluation import PromptEvaluator
from agentCore.observability import get_tracer

# 1. Auto-configura√ß√£o para produ√ß√£o
store = auto_configure_vector_store(
    use_case="enterprise",
    environment="production",
    budget="high"
)
# Resultado: AWS OpenSearch configurado automaticamente

orchestrator = get_orchestrator("auto",
    use_case="enterprise",
    complexity="high"
)
# Resultado: CrewAI selecionado automaticamente

llm = get_llm()  # Bedrock por padr√£o

# 2. Carregar APIs empresariais
weather_tools = api2tool("./weather_api.json")
crm_tools = api2tool("./crm_api.json")
analytics_tools = api2tool("./analytics_api.json")

# 3. Criar agente empresarial
all_tools = []
for tools in [weather_tools, crm_tools, analytics_tools]:
    all_tools.extend([tool['function'] for tool in tools])

agent = orchestrator.create_agent(llm, tools=all_tools)

# 4. Sistema de avalia√ß√£o
evaluator = PromptEvaluator()
test_cases = [
    {"input": "Clima em SP?", "expected": "temperatura"},
    {"input": "Vendas Q4?", "expected": "relat√≥rio"}
]
results = evaluator.evaluate_dataset(test_cases, agent)
print(f"Acur√°cia: {results['accuracy']:.2%}")

# 5. Observabilidade
tracer = get_tracer()
with tracer.trace_execution("enterprise_query"):
    response = agent.invoke({
        "messages": [{"role": "user", "content": "An√°lise completa do Q4"}]
    })
```

### üß™ Exemplo 2: Desenvolvimento Local com Ollama

```python
from agentCore import get_vector_store, get_orchestrator
from agentCore.providers import get_llm
from agentCore.chunking import get_chunking_strategy, ChunkingMethod

# Setup local otimizado
store = get_vector_store("chromadb", {"persist_directory": "./local_db"})
orchestrator = get_orchestrator("langgraph")  # Simples para desenvolvimento
llm = get_llm("ollama")

# Chunking inteligente
chunker = get_chunking_strategy(ChunkingMethod.SEMANTIC)
documents = [
    "Documenta√ß√£o t√©cnica longa...",
    "C√≥digo Python com fun√ß√µes...",
    "Artigo cient√≠fico..."
]

for doc in documents:
    chunks = chunker.chunk(doc, metadata={"source": "local"})
    store.add_documents(chunks)

# Query com contexto
results = store.similarity_search("Como implementar autentica√ß√£o?")
context = "\n".join([r.page_content for r in results])

response = llm.invoke(f"Contexto: {context}\n\nPergunta: Como implementar autentica√ß√£o?")
print(response.content)
```

### üî¨ Exemplo 3: Compara√ß√£o de Modelos

```python
from agentCore.evaluation import ModelComparator
from agentCore.providers import get_llm

# Configurar m√∫ltiplos modelos
models = {
    "bedrock_sonnet": get_llm("bedrock"),
    "openai_gpt4": get_llm("openai"),
    "ollama_llama3": get_llm("ollama")
}

# Dataset de teste
test_cases = [
    {
        "input": "Explique machine learning",
        "expected_keywords": ["algoritmo", "dados", "treinamento"]
    },
    {
        "input": "Como fazer uma API REST?",
        "expected_keywords": ["HTTP", "endpoint", "JSON"]
    }
]

# Compara√ß√£o autom√°tica
comparator = ModelComparator(models)
results = comparator.compare_on_dataset(test_cases)

# Resultados detalhados
for model_name, metrics in results.items():
    print(f"{model_name}:")
    print(f"  Acur√°cia: {metrics['accuracy']:.2%}")
    print(f"  Lat√™ncia: {metrics['avg_latency']:.2f}s")
    print(f"  Score: {metrics['weighted_score']:.2f}")
```

### üé® Exemplo 4: Chunking Avan√ßado

```python
from agentCore.chunking import (
    get_chunking_strategy,
    ChunkingMethod,
    detect_content_type
)

# Auto-detec√ß√£o de tipo de conte√∫do
def smart_chunk(text, content_type="auto"):
    if content_type == "auto":
        content_type = detect_content_type(text)

    strategy_map = {
        "markdown": ChunkingMethod.MARKDOWN_AWARE,
        "code": ChunkingMethod.CODE_AWARE,
        "academic": ChunkingMethod.SEMANTIC,
        "general": ChunkingMethod.RECURSIVE
    }

    method = strategy_map.get(content_type, ChunkingMethod.RECURSIVE)
    chunker = get_chunking_strategy(method)
    return chunker.chunk(text)

# Exemplos com diferentes tipos
markdown_doc = """
# T√≠tulo Principal
## Se√ß√£o 1
Conte√∫do da se√ß√£o...
### Subse√ß√£o
Mais conte√∫do...
"""

code_doc = """
def calculate_total(items):
    \"\"\"Calcula o total dos itens\"\"\"
    return sum(item.price for item in items)

class ShoppingCart:
    def __init__(self):
        self.items = []
"""

# Chunking autom√°tico
md_chunks = smart_chunk(markdown_doc)  # Detecta: markdown
code_chunks = smart_chunk(code_doc)    # Detecta: code

print(f"Markdown: {len(md_chunks)} chunks")
print(f"Code: {len(code_chunks)} chunks")
```

## üèóÔ∏è Arquitetura v2.0

```
agentCore/
‚îú‚îÄ‚îÄ utils/              # üîß Utilidades principais
‚îÇ   ‚îú‚îÄ‚îÄ api2tool.py        # Convers√£o OpenAPI ‚Üí Ferramentas
‚îÇ   ‚îî‚îÄ‚îÄ openapi_to_tools.py
‚îú‚îÄ‚îÄ providers/          # ü§ñ Provedores LLM multi-cloud
‚îÇ   ‚îú‚îÄ‚îÄ llm_providers.py   # AWS Bedrock, OpenAI, Ollama, Gemini
‚îÇ   ‚îî‚îÄ‚îÄ vector_store_factory.py
‚îú‚îÄ‚îÄ orchestration/      # üé≠ Multi-framework orchestration
‚îÇ   ‚îú‚îÄ‚îÄ crewai_orchestrator.py    # CrewAI para empresas
‚îÇ   ‚îú‚îÄ‚îÄ langgraph_orchestrator.py # LangGraph para fluxos simples
‚îÇ   ‚îú‚îÄ‚îÄ autogen_orchestrator.py   # AutoGen para conversas
‚îÇ   ‚îî‚îÄ‚îÄ auto_selector.py          # Auto-sele√ß√£o inteligente
‚îú‚îÄ‚îÄ vector_stores/      # üóÑÔ∏è Storage vetorial multi-provedor
‚îÇ   ‚îú‚îÄ‚îÄ aws_opensearch.py         # AWS OpenSearch
‚îÇ   ‚îú‚îÄ‚îÄ aws_kendra.py             # AWS Kendra
‚îÇ   ‚îú‚îÄ‚îÄ aws_s3_faiss.py           # AWS S3 + FAISS
‚îÇ   ‚îú‚îÄ‚îÄ qdrant_provider.py        # Qdrant local/cloud
‚îÇ   ‚îú‚îÄ‚îÄ chromadb_provider.py      # ChromaDB
‚îÇ   ‚îú‚îÄ‚îÄ pinecone_provider.py      # Pinecone
‚îÇ   ‚îî‚îÄ‚îÄ faiss_provider.py         # FAISS local
‚îú‚îÄ‚îÄ evaluation/         # üìä Sistema de avalia√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ prompt_evaluator.py       # OpenAI/evals style
‚îÇ   ‚îú‚îÄ‚îÄ model_comparison.py       # Compara√ß√£o de modelos
‚îÇ   ‚îî‚îÄ‚îÄ metrics.py                # M√©tricas de performance
‚îú‚îÄ‚îÄ reasoning/          # üß† Reasoning avan√ßado
‚îÇ   ‚îú‚îÄ‚îÄ chain_of_thought.py       # Chain of Thought
‚îÇ   ‚îî‚îÄ‚îÄ step_by_step.py           # Step-by-step reasoning
‚îú‚îÄ‚îÄ chunking/           # ‚úÇÔ∏è Estrat√©gias de chunking
‚îÇ   ‚îú‚îÄ‚îÄ text_chunker.py           # M√∫ltiplas estrat√©gias
‚îÇ   ‚îú‚îÄ‚îÄ semantic_chunker.py       # Chunking sem√¢ntico
‚îÇ   ‚îú‚îÄ‚îÄ code_chunker.py           # Espec√≠fico para c√≥digo
‚îÇ   ‚îî‚îÄ‚îÄ content_detector.py       # Auto-detec√ß√£o de tipo
‚îú‚îÄ‚îÄ observability/      # üîç Observabilidade completa
‚îÇ   ‚îú‚îÄ‚îÄ agent_tracer.py           # Tracing de execu√ß√µes
‚îÇ   ‚îú‚îÄ‚îÄ metrics_collector.py      # Coleta de m√©tricas
‚îÇ   ‚îî‚îÄ‚îÄ visualization.py          # Visualiza√ß√£o de traces
‚îú‚îÄ‚îÄ graphs/             # üï∏Ô∏è Orquestra√ß√£o LangGraph (legado)
‚îÇ   ‚îî‚îÄ‚îÄ graph.py
‚îî‚îÄ‚îÄ logger/             # üìù Sistema de logging
    ‚îî‚îÄ‚îÄ logger.py
```

### üîÑ Fluxo de Auto-Configura√ß√£o

```mermaid
graph TD
    A[In√≠cio] --> B[Detectar Caso de Uso]
    B --> C[Analisar Ambiente]
    C --> D[Avaliar Or√ßamento]
    D --> E{Tipo de Setup?}
    E -->|Desenvolvimento| F[ChromaDB + LangGraph + Ollama]
    E -->|Empresa| G[AWS OpenSearch + CrewAI + Bedrock]
    E -->|Pesquisa| H[Qdrant + Semantic Chunking + Bedrock]
    F --> I[Configura√ß√£o Aplicada]
    G --> I
    H --> I
```

## üîç Casos de Uso e Recomenda√ß√µes

### üè¢ **Empresa/Produ√ß√£o**
```python
# Setup autom√°tico para produ√ß√£o
store = auto_configure_vector_store("enterprise", "production", "high")
# ‚úÖ Resultado: AWS OpenSearch

orchestrator = get_orchestrator("auto", use_case="enterprise")
# ‚úÖ Resultado: CrewAI com multi-agentes

llm = get_llm("bedrock")  # Claude 3 Sonnet
```

**Caracter√≠sticas:**
- ‚òÅÔ∏è AWS OpenSearch para vectors (escal√°vel)
- ü§ñ CrewAI para orquestra√ß√£o complexa
- üîí AWS Bedrock para LLM (seguro)
- üìä Monitoring e health checks integrados

### üß™ **Desenvolvimento Local**
```python
# Setup r√°pido para desenvolvimento
store = auto_configure_vector_store("development", "development", "low")
# ‚úÖ Resultado: ChromaDB local

orchestrator = get_orchestrator("auto", complexity="low")
# ‚úÖ Resultado: LangGraph

llm = get_llm("ollama")  # Llama3 local
```

**Caracter√≠sticas:**
- üíª ChromaDB local (sem custos)
- üîó LangGraph simples
- üè† Ollama para LLM local
- ‚ö° Setup em segundos

### üî¨ **Pesquisa/Academia**
```python
# Setup para pesquisa
store = auto_configure_vector_store("research", "development", "medium")
# ‚úÖ Resultado: Qdrant com embeddings sem√¢nticos

chunker = get_chunking_strategy(ChunkingMethod.SEMANTIC)
# ‚úÖ Chunking baseado em similaridade sem√¢ntica
```

**Caracter√≠sticas:**
- üß† Qdrant para vector search avan√ßado
- üìö Chunking sem√¢ntico para papers
- üìä M√©tricas detalhadas de performance
- üî¨ Ferramentas de an√°lise

## üìä Compara√ß√£o de Vers√µes

| **Aspecto** | **v1.0** | **v2.0** |
|-------------|----------|----------|
| **Orchestration** | ‚ùå Apenas LangGraph | ‚úÖ Multi-framework (CrewAI, LangGraph, AutoGen) |
| **Vector Storage** | ‚ùå Apenas ChromaDB | ‚úÖ 7+ providers (AWS, Qdrant, Pinecone, etc.) |
| **Chunking** | ‚ùå Split b√°sico | ‚úÖ 7+ estrat√©gias inteligentes |
| **Production** | ‚ùå Apenas desenvolvimento | ‚úÖ AWS-native, enterprise-ready |
| **Configuration** | ‚ùå Manual | ‚úÖ Auto-configura√ß√£o baseada em use case |
| **Scalability** | ‚ùå Limitada | ‚úÖ Enterprise-grade |
| **Evaluation** | ‚ùå N√£o dispon√≠vel | ‚úÖ OpenAI/evals + compara√ß√£o de modelos |
| **Observability** | ‚ùå Logs b√°sicos | ‚úÖ Tracing completo + m√©tricas |

## üöÄ Migration Guide (v1.0 ‚Üí v2.0)

### ‚úÖ **Mudan√ßas N√£o-Disruptivas**
```python
# ‚úÖ C√≥digo v1.0 continua funcionando
from agentCore.utils import api2tool
from agentCore.providers import get_llm
from agentCore.graphs import create_agent_graph

# Funciona exatamente igual
tools = api2tool("api.json")
llm = get_llm()
agent = create_agent_graph(llm, tools)
```

### üÜï **Novas Funcionalidades Opcionais**
```python
# üÜï Adicione gradualmente as melhorias
from agentCore import get_vector_store, get_orchestrator

# Upgrade incremental
vector_store = get_vector_store()  # Auto-configure
orchestrator = get_orchestrator("auto")  # Auto-select
```

## üìã Changelog

### [2.0.0] - 2024-12-29

#### ‚ú® Adicionado
- **Multi-Framework Orchestration**: CrewAI, LangGraph, AutoGen
- **Multi-Provider Vector Storage**: AWS OpenSearch, Kendra, S3+FAISS, Qdrant, Pinecone
- **Advanced Chunking**: 7+ estrat√©gias inteligentes
- **Auto-Configuration**: Baseada em caso de uso
- **Evaluation Framework**: OpenAI/evals style
- **Model Comparison**: Performance entre LLMs
- **Advanced Observability**: Tracing e m√©tricas
- **Production Features**: AWS integration, health monitoring

#### üîÑ Melhorado
- Sistema de providers LLM mais robusto
- Logging com mais n√≠veis e contexto
- Documenta√ß√£o centralizada
- Performance geral

#### üîí Mantido (Backward Compatible)
- Interface `api2tool` original
- Configura√ß√£o via vari√°veis de ambiente
- Estrutura b√°sica de providers

### [1.0.0] - 2024-12-29

#### ‚ú® Primeira Release
- Funcionalidade principal `api2tool`
- Suporte multi-provedor LLM b√°sico
- Sistema de logging
- Orquestra√ß√£o LangGraph
- Integra√ß√£o ChromaDB

## ‚úÖ Verifica√ß√£o da Instala√ß√£o

### üß™ Teste B√°sico
```python
# Testar importa√ß√£o
from agentCore.utils import api2tool
from agentCore.providers import get_llm, get_provider_info

# Verificar provedor
info = get_provider_info()
print(f"Provedor ativo: {info['provider']}")

# Testar convers√£o
tools = api2tool('https://petstore.swagger.io/v2/swagger.json', output_format='info')
print(f"API: {tools['title']} - {tools['tool_count']} ferramentas")
```

### üìä Teste Completo
```python
# Execute o demo completo
python e2e_demo_local.py
```

### üîß Debug
```python
from agentCore.providers import get_provider_info

# Verificar configura√ß√£o
info = get_provider_info()
print(f"Configura√ß√£o: {info}")

# Testar conex√£o
try:
    llm = get_llm()
    response = llm.invoke("test")
    print("‚úÖ LLM funcionando")
except Exception as e:
    print(f"‚ùå Erro: {e}")
```

## üîß Desenvolvimento

### üõ†Ô∏è Setup do Ambiente
```bash
# Clone e setup
git clone <repositorio>
cd agentcore
python -m venv venv
source venv/bin/activate
pip install -e .[dev]
```

### üß™ Testes
```bash
# Executar testes
pytest tests/ -v

# Com cobertura
pytest --cov=agentCore --cov-report=html

# Testes espec√≠ficos
pytest tests/test_evaluation.py -v
```

### üìù Formata√ß√£o
```bash
# Formata√ß√£o autom√°tica
black agentCore/
isort agentCore/
flake8 agentCore/

# Pre-commit hooks
pre-commit install
pre-commit run --all-files
```

### üèóÔ∏è Build
```bash
# Build da distribui√ß√£o
python -m build

# Verificar build
twine check dist/*
```

## üìÑ Licen√ßa

MIT License - veja o arquivo [LICENSE](LICENSE) para detalhes.

## ü§ù Contribuindo

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudan√ßas (`git commit -m 'Add: AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

### üìã Guidelines
- Siga o padr√£o de c√≥digo existente
- Adicione testes para novas funcionalidades
- Atualize a documenta√ß√£o quando necess√°rio
- Use conventional commits

## üìû Suporte

- **üêõ Issues**: [GitHub Issues](https://github.com/your-org/agent-core/issues)
- **üí¨ Discuss√µes**: [GitHub Discussions](https://github.com/your-org/agent-core/discussions)
- **üìñ Documenta√ß√£o**: Veja exemplos acima
- **üìß Contato**: [email@exemplo.com](mailto:email@exemplo.com)

---

## üéâ Conclus√£o

A **AgentCore v2.0** √© uma biblioteca enterprise-ready que resolve todos os desafios de produ√ß√£o:

- ‚úÖ **Flexibilidade**: Multi-framework, multi-provider, multi-estrat√©gia
- ‚úÖ **Escalabilidade**: AWS-native para produ√ß√£o enterprise
- ‚úÖ **Simplicidade**: Auto-configura√ß√£o baseada em caso de uso
- ‚úÖ **Observabilidade**: Tracing, m√©tricas e avalia√ß√£o completa
- ‚úÖ **Compatibilidade**: Mant√©m interface v1.0 funcionando

**Recomenda√ß√£o**: Use para projetos reais de produ√ß√£o com confian√ßa! üöÄ