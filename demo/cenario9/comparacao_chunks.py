"""
Cen√°rio 9: Compara√ß√£o de Estrat√©gias de Chunking
Demonstra como diferentes estrat√©gias de chunking impactam a precis√£o da recupera√ß√£o de informa√ß√µes.
"""

from agentCore import get_llm, get_embeddings, get_logger
from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain_chroma import Chroma
from agentCore.chunking.chunk_processor import ChunkProcessor
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.documents import Document
import json
import os
import time
import statistics
from typing import List, Dict, Any, Tuple
from dataclasses import dataclass
from dotenv import load_dotenv

load_dotenv()

@dataclass
class ChunkingStrategy:
    """Configura√ß√£o de estrat√©gia de chunking."""
    name: str
    method: str
    chunk_size: int
    chunk_overlap: int
    separator: str
    description: str
    use_cases: List[str]

@dataclass
class RetrievalResult:
    """Resultado de recupera√ß√£o para uma estrat√©gia."""
    strategy_name: str
    query: str
    retrieved_chunks: List[str]
    relevance_scores: List[float]
    precision_at_k: float
    recall_estimate: float
    chunk_quality: float
    processing_time: float

class ChunkingComparator:
    """Sistema de compara√ß√£o de estrat√©gias de chunking."""

    def __init__(self):
        self.strategies = []
        self.test_documents = []
        self.test_queries = []
        self.results = []

    def add_strategy(self, strategy: ChunkingStrategy):
        """Adiciona estrat√©gia para compara√ß√£o."""
        self.strategies.append(strategy)

    def add_test_document(self, title: str, content: str, metadata: Dict = None):
        """Adiciona documento de teste."""
        doc = Document(
            page_content=content,
            metadata={"title": title, **(metadata or {})}
        )
        self.test_documents.append(doc)

    def add_test_query(self, query: str, expected_keywords: List[str], category: str):
        """Adiciona query de teste com palavras-chave esperadas."""
        test_query = {
            "query": query,
            "expected_keywords": expected_keywords,
            "category": category
        }
        self.test_queries.append(test_query)

    def evaluate_chunk_quality(self, chunks: List[Document]) -> float:
        """Avalia qualidade geral dos chunks produzidos."""
        if not chunks:
            return 0.0

        quality_score = 0.0
        total_chunks = len(chunks)

        for chunk in chunks:
            content = chunk.page_content

            # M√©trica 1: Tamanho adequado (nem muito pequeno, nem muito grande)
            length_score = 0.0
            if 100 <= len(content) <= 1000:
                length_score = 1.0
            elif 50 <= len(content) < 100 or 1000 < len(content) <= 1500:
                length_score = 0.7
            else:
                length_score = 0.3

            # M√©trica 2: Completude de frases (n√£o corta no meio)
            sentence_score = 0.0
            if content.endswith('.') or content.endswith('!') or content.endswith('?'):
                sentence_score = 1.0
            elif content.endswith(':') or content.endswith(';'):
                sentence_score = 0.7
            else:
                sentence_score = 0.3

            # M√©trica 3: Coer√™ncia sem√¢ntica (palavras relacionadas)
            coherence_score = 0.5  # Baseline
            words = content.lower().split()
            if len(words) > 10:
                # Verifica se h√° repeti√ß√£o de temas/conceitos
                word_freq = {}
                for word in words:
                    if len(word) > 4:  # Palavras significativas
                        word_freq[word] = word_freq.get(word, 0) + 1

                # Se h√° palavras repetidas, indica coer√™ncia tem√°tica
                repeated_words = sum(1 for freq in word_freq.values() if freq > 1)
                if repeated_words > 2:
                    coherence_score = 0.8

            chunk_score = (length_score * 0.4) + (sentence_score * 0.3) + (coherence_score * 0.3)
            quality_score += chunk_score

        return quality_score / total_chunks if total_chunks > 0 else 0.0

    def evaluate_retrieval_precision(self, retrieved_chunks: List[str], expected_keywords: List[str]) -> float:
        """Avalia precis√£o da recupera√ß√£o baseada em palavras-chave esperadas."""
        if not retrieved_chunks or not expected_keywords:
            return 0.0

        relevant_chunks = 0
        for chunk in retrieved_chunks:
            chunk_lower = chunk.lower()
            # Chunk √© relevante se cont√©m pelo menos 1 palavra-chave esperada
            if any(keyword.lower() in chunk_lower for keyword in expected_keywords):
                relevant_chunks += 1

        return relevant_chunks / len(retrieved_chunks) if len(retrieved_chunks) > 0 else 0.0

    def run_retrieval_test(self, strategy: ChunkingStrategy, test_query: Dict, logger) -> RetrievalResult:
        """Executa teste de recupera√ß√£o para uma estrat√©gia espec√≠fica."""
        start_time = time.time()

        try:
            # Configurar chunking strategy
            # Use LangChain text splitters directly
            if strategy.method == "recursive":
                chunker = RecursiveCharacterTextSplitter(
                    chunk_size=strategy.chunk_size,
                    chunk_overlap=strategy.chunk_overlap,
                    length_function=len,
                )
            else:
                chunker = CharacterTextSplitter(
                    chunk_size=strategy.chunk_size,
                    chunk_overlap=strategy.chunk_overlap,
                )

            # Processar documentos
            all_chunks = []
            for doc in self.test_documents:
                doc_chunks = chunker.split_documents([doc])
                all_chunks.extend(doc_chunks)

            # Avaliar qualidade dos chunks
            chunk_quality = self.evaluate_chunk_quality(all_chunks)

            # Configurar vector store
            embeddings = get_embeddings(provider_name="ollama")  # Usando provider padr√£o
            # Configurar vector store usando Chroma diretamente
            texts = [chunk.page_content for chunk in all_chunks]
            metadatas = [chunk.metadata for chunk in all_chunks]

            vector_store = Chroma.from_texts(
                texts=texts,
                embedding=embeddings,
                metadatas=metadatas,
                collection_name=f"chunking_test_{strategy.name.lower().replace(' ', '_')}"
            )

            # Vector store j√° foi criado com os documentos, n√£o precisamos adicionar novamente

            # Executar teste de recupera√ß√£o para a query espec√≠fica
            # Recuperar documentos relevantes
            retriever = vector_store.as_retriever(search_kwargs={"k": 5})
            retrieved_docs = retriever.invoke(test_query["query"])

            retrieved_chunks = [doc.page_content for doc in retrieved_docs]

            # Calcular scores de relev√¢ncia (baseados em keywords)
            relevance_scores = []
            for chunk in retrieved_chunks:
                chunk_lower = chunk.lower()
                keyword_matches = sum(1 for kw in test_query["expected_keywords"]
                                    if kw.lower() in chunk_lower)
                # Evitar divis√£o por zero
                if len(test_query["expected_keywords"]) > 0:
                    relevance_score = min(1.0, keyword_matches / len(test_query["expected_keywords"]))
                else:
                    relevance_score = 0.0
                relevance_scores.append(relevance_score)

            # Calcular m√©tricas
            precision_at_k = self.evaluate_retrieval_precision(retrieved_chunks, test_query["expected_keywords"])
            recall_estimate = statistics.mean(relevance_scores) if relevance_scores else 0.0

            processing_time = time.time() - start_time

            return RetrievalResult(
                strategy_name=strategy.name,
                query=test_query["query"],
                retrieved_chunks=retrieved_chunks,
                relevance_scores=relevance_scores,
                precision_at_k=precision_at_k,
                recall_estimate=recall_estimate,
                chunk_quality=chunk_quality,
                processing_time=processing_time
            )

        except Exception as e:
            logger.error(f"Erro no teste de {strategy.name}: {str(e)}")
            processing_time = time.time() - start_time

            return RetrievalResult(
                strategy_name=strategy.name,
                query=test_query["query"],
                retrieved_chunks=[],
                relevance_scores=[],
                precision_at_k=0.0,
                recall_estimate=0.0,
                chunk_quality=0.0,
                processing_time=processing_time
            )

    def run_comprehensive_comparison(self, logger) -> List[RetrievalResult]:
        """Executa compara√ß√£o abrangente de todas as estrat√©gias."""
        results = []

        print(f"\nüîÑ Iniciando compara√ß√£o de {len(self.strategies)} estrat√©gias")
        print(f"   Testando com {len(self.test_queries)} queries")

        for strategy in self.strategies:
            print(f"\nüìä Testando estrat√©gia: {strategy.name}")
            strategy_results = []

            for test_query in self.test_queries:
                print(f"  Query: {test_query['query'][:50]}...")

                result = self.run_retrieval_test(strategy, test_query, logger)
                strategy_results.append(result)
                results.append(result)

                print(f"    Precis√£o: {result.precision_at_k:.2f}, Qualidade: {result.chunk_quality:.2f}")

            # Resumo da estrat√©gia
            avg_precision = statistics.mean([r.precision_at_k for r in strategy_results])
            avg_quality = statistics.mean([r.chunk_quality for r in strategy_results])
            avg_time = statistics.mean([r.processing_time for r in strategy_results])

            print(f"  ‚úÖ Resumo: Precis√£o {avg_precision:.2f}, Qualidade {avg_quality:.2f}, Tempo {avg_time:.2f}s")

        return results

def criar_estrategias_chunking() -> List[ChunkingStrategy]:
    """Cria diferentes estrat√©gias de chunking para compara√ß√£o."""
    return [
        ChunkingStrategy(
            name="Recursive Small",
            method="recursive",
            chunk_size=200,
            chunk_overlap=20,
            separator="\n\n",
            description="Chunks pequenos com overlap m√≠nimo para m√°xima granularidade",
            use_cases=["Busca precisa", "Q&A espec√≠fico", "Cita√ß√µes exatas"]
        ),
        ChunkingStrategy(
            name="Recursive Medium",
            method="recursive",
            chunk_size=500,
            chunk_overlap=50,
            separator="\n\n",
            description="Chunks m√©dios balanceados entre contexto e precis√£o",
            use_cases=["Uso geral", "Resumos", "An√°lises moderadas"]
        ),
        ChunkingStrategy(
            name="Recursive Large",
            method="recursive",
            chunk_size=1000,
            chunk_overlap=100,
            separator="\n\n",
            description="Chunks grandes para m√°ximo contexto",
            use_cases=["An√°lises complexas", "Contexto amplo", "Racioc√≠nio longo"]
        ),
        ChunkingStrategy(
            name="Sentence Based",
            method="recursive",
            chunk_size=300,
            chunk_overlap=0,
            separator=".",
            description="Baseado em frases para preservar significado sem√¢ntico",
            use_cases=["Preserva√ß√£o sem√¢ntica", "Textos estruturados", "Documentos formais"]
        ),
        ChunkingStrategy(
            name="Paragraph Based",
            method="recursive",
            chunk_size=800,
            chunk_overlap=50,
            separator="\n\n",
            description="Baseado em par√°grafos para contexto tem√°tico",
            use_cases=["Documentos longos", "Artigos", "Relat√≥rios estruturados"]
        )
    ]

def criar_documentos_teste() -> List[Tuple[str, str, Dict]]:
    """Cria documentos de teste para avalia√ß√£o."""
    return [
        (
            "Manual de Recursos Humanos",
            """
            Pol√≠tica de Benef√≠cios da Empresa

            Nossa empresa oferece um pacote abrangente de benef√≠cios para todos os funcion√°rios.
            O vale refei√ß√£o √© de R$ 30,00 por dia √∫til, depositado mensalmente no cart√£o corporativo.

            O plano de sa√∫de possui cobertura nacional com rede credenciada de hospitais e cl√≠nicas.
            A coparticipa√ß√£o √© de 20% para consultas e 10% para exames.

            Benef√≠cios adicionais incluem:
            - Vale alimenta√ß√£o: R$ 400,00 mensais
            - Plano odontol√≥gico: 100% custeado pela empresa
            - Seguro de vida: equivalente a 2x o sal√°rio anual
            - Aux√≠lio creche: at√© R$ 600,00 para filhos at√© 5 anos
            - Gympass: acesso a academias e aplicativos de bem-estar

            Pol√≠tica de F√©rias

            Todos os funcion√°rios t√™m direito a 30 dias de f√©rias ap√≥s completar um ano de trabalho.
            As f√©rias podem ser divididas em at√© 3 per√≠odos, sendo que um deles deve ter pelo menos 14 dias.

            A solicita√ß√£o deve ser feita com 60 dias de anteced√™ncia atrav√©s do sistema de RH.
            Durante as f√©rias, o funcion√°rio recebe o sal√°rio normal mais 1/3 constitucional.

            Pol√≠tica de Home Office

            O trabalho remoto √© permitido at√© 3 dias por semana, mediante aprova√ß√£o do gestor.
            A presen√ßa no escrit√≥rio √© obrigat√≥ria nas segundas e sextas-feiras para reuni√µes de equipe.

            A empresa fornece equipamentos necess√°rios: notebook, monitor adicional e cadeira ergon√¥mica.
            H√° aux√≠lio de R$ 100,00 mensais para internet banda larga residencial.
            """,
            {"category": "recursos_humanos", "department": "HR"}
        ),
        (
            "Processo Comercial",
            """
            Metodologia de Vendas B2B

            Nosso processo de vendas segue uma metodologia estruturada em 6 etapas principais:

            1. Qualifica√ß√£o do Lead (1-2 dias)
            Identifica√ß√£o do perfil ideal de cliente (ICP) e valida√ß√£o de fit inicial.
            Verifica√ß√£o de budget, autoridade, necessidade e timeline (BANT).

            2. Reuni√£o de Descoberta (1 semana)
            Entrevista aprofundada com stakeholders para entender dores e necessidades.
            Mapeamento do processo atual e identifica√ß√£o de oportunidades de melhoria.

            3. Elabora√ß√£o de Proposta T√©cnica (1-2 semanas)
            Desenvolvimento de solu√ß√£o customizada baseada no discovery.
            Defini√ß√£o de escopo, cronograma e recursos necess√°rios.

            4. Apresenta√ß√£o Comercial (1 semana)
            Demonstra√ß√£o da solu√ß√£o para tomadores de decis√£o.
            Apresenta√ß√£o de cases de sucesso e ROI projetado.

            5. Negocia√ß√£o e Fechamento (1-2 semanas)
            Discuss√£o de termos comerciais, condi√ß√µes e cronograma.
            Tratamento de obje√ß√µes e finaliza√ß√£o do contrato.

            6. Kick-off do Projeto (1 semana)
            Transi√ß√£o para equipe de implementa√ß√£o.
            Alinhamento de expectativas e in√≠cio dos trabalhos.

            O ciclo m√©dio de vendas √© de 6-8 semanas da prospec√ß√£o inicial ao fechamento.
            Nossa taxa de convers√£o √© de 25% para leads qualificados.

            M√©tricas Importantes:
            - Ticket m√©dio: R$ 85.000
            - LTV (Lifetime Value): R$ 180.000
            - CAC (Customer Acquisition Cost): R$ 12.000
            - Payback period: 8 meses
            """,
            {"category": "comercial", "department": "sales"}
        ),
        (
            "SLA de Suporte T√©cnico",
            """
            N√≠veis de Servi√ßo do Suporte T√©cnico

            Nosso suporte t√©cnico opera com 4 n√≠veis de prioridade, cada um com SLA espec√≠fico:

            Prioridade 1 - Cr√≠tico (Sistema Inoperante)
            - Tempo de resposta: 1 hora
            - Tempo de resolu√ß√£o: 4 horas
            - Disponibilidade: 24/7
            - Escala√ß√£o autom√°tica se n√£o resolvido em 2 horas

            Prioridade 2 - Alto (Funcionalidade Importante Afetada)
            - Tempo de resposta: 4 horas
            - Tempo de resolu√ß√£o: 1 dia √∫til
            - Disponibilidade: Hor√°rio comercial estendido (7h-20h)
            - Escala√ß√£o se n√£o resolvido em 8 horas

            Prioridade 3 - M√©dio (Problema que Impacta Opera√ß√£o)
            - Tempo de resposta: 1 dia √∫til
            - Tempo de resolu√ß√£o: 3 dias √∫teis
            - Disponibilidade: Hor√°rio comercial (9h-18h)
            - Escala√ß√£o se n√£o resolvido em 2 dias

            Prioridade 4 - Baixo (D√∫vidas e Solicita√ß√µes Gerais)
            - Tempo de resposta: 2 dias √∫teis
            - Tempo de resolu√ß√£o: 1 semana
            - Disponibilidade: Hor√°rio comercial
            - Sem escala√ß√£o autom√°tica

            Canais de Atendimento:
            - Email: suporte@empresa.com
            - Telefone: (11) 1234-5678 (P1 e P2 apenas)
            - Chat: Portal do cliente (dispon√≠vel 24/7)
            - Portal: sistema.empresa.com/suporte

            M√©tricas de Qualidade:
            - First Call Resolution (FCR): 78%
            - Customer Satisfaction (CSAT): 4.6/5.0
            - Net Promoter Score (NPS): +65
            - Tempo m√©dio de resolu√ß√£o: 2.3 dias √∫teis
            """,
            {"category": "suporte_tecnico", "department": "support"}
        )
    ]

def criar_queries_teste() -> List[Dict]:
    """Cria queries de teste com palavras-chave esperadas."""
    return [
        {
            "query": "Quanto √© o vale refei√ß√£o da empresa?",
            "expected_keywords": ["vale refei√ß√£o", "R$ 30,00", "dia √∫til"],
            "category": "beneficios_especificos"
        },
        {
            "query": "Como funciona a pol√≠tica de home office?",
            "expected_keywords": ["3 dias", "home office", "segundas", "sextas"],
            "category": "politicas_trabalho"
        },
        {
            "query": "Qual √© o processo de vendas da empresa?",
            "expected_keywords": ["6 etapas", "qualifica√ß√£o", "proposta", "negocia√ß√£o"],
            "category": "processos_comerciais"
        },
        {
            "query": "Quais s√£o os tempos de SLA para problemas cr√≠ticos?",
            "expected_keywords": ["1 hora", "4 horas", "cr√≠tico", "P1"],
            "category": "sla_suporte"
        },
        {
            "query": "Que equipamentos a empresa fornece para trabalho remoto?",
            "expected_keywords": ["notebook", "monitor", "cadeira ergon√¥mica"],
            "category": "equipamentos_home_office"
        },
        {
            "query": "Qual √© o ticket m√©dio de vendas?",
            "expected_keywords": ["R$ 85.000", "ticket m√©dio"],
            "category": "metricas_comerciais"
        }
    ]

def gerar_relatorio_comparativo_chunks(results: List[RetrievalResult], strategies: List[ChunkingStrategy]):
    """Gera relat√≥rio detalhado de compara√ß√£o de chunking."""
    print("\n" + "="*80)
    print("üìä RELAT√ìRIO DE COMPARA√á√ÉO DE ESTRAT√âGIAS DE CHUNKING")
    print("="*80)

    # Agrupar resultados por estrat√©gia
    strategy_results = {}
    for result in results:
        if result.strategy_name not in strategy_results:
            strategy_results[result.strategy_name] = []
        strategy_results[result.strategy_name].append(result)

    # Ranking geral
    print("\nüèÜ RANKING GERAL DAS ESTRAT√âGIAS")
    print("-"*50)

    strategy_scores = {}
    for strategy_name, results_list in strategy_results.items():
        avg_precision = statistics.mean([r.precision_at_k for r in results_list])
        avg_recall = statistics.mean([r.recall_estimate for r in results_list])
        avg_quality = statistics.mean([r.chunk_quality for r in results_list])
        avg_time = statistics.mean([r.processing_time for r in results_list])

        # Score composto (precis√£o + recall + qualidade - tempo_normalizado)
        normalized_time = min(1.0, avg_time / 10.0)  # Normalizar tempo
        composite_score = (avg_precision * 0.35) + (avg_recall * 0.35) + (avg_quality * 0.25) + ((1 - normalized_time) * 0.05)

        strategy_scores[strategy_name] = {
            "precision": avg_precision,
            "recall": avg_recall,
            "quality": avg_quality,
            "time": avg_time,
            "composite": composite_score
        }

    # Ordenar por score composto
    ranked_strategies = sorted(strategy_scores.items(), key=lambda x: x[1]["composite"], reverse=True)

    for i, (strategy_name, scores) in enumerate(ranked_strategies, 1):
        print(f"\n{i}. üìã {strategy_name}")
        print(f"   Score Composto: {scores['composite']:.3f}")
        print(f"   Precis√£o: {scores['precision']:.3f}")
        print(f"   Recall: {scores['recall']:.3f}")
        print(f"   Qualidade: {scores['quality']:.3f}")
        print(f"   Tempo: {scores['time']:.2f}s")

        # Encontrar estrat√©gia detalhada
        strategy_detail = next((s for s in strategies if s.name == strategy_name), None)
        if strategy_detail:
            print(f"   Chunk Size: {strategy_detail.chunk_size}")
            print(f"   Overlap: {strategy_detail.chunk_overlap}")

    # An√°lise por categoria de query
    print("\nüìä PERFORMANCE POR CATEGORIA DE QUERY")
    print("-"*50)

    query_categories = {}
    for result in results:
        # Encontrar categoria da query
        category = "geral"  # default
        for query_test in criar_queries_teste():
            if query_test["query"] == result.query:
                category = query_test["category"]
                break

        if category not in query_categories:
            query_categories[category] = {}
        if result.strategy_name not in query_categories[category]:
            query_categories[category][result.strategy_name] = []
        query_categories[category][result.strategy_name].append(result.precision_at_k)

    for category, cat_strategies in query_categories.items():
        print(f"\nüìÅ {category.upper().replace('_', ' ')}:")
        cat_ranking = []
        for strategy_name, precisions in cat_strategies.items():
            avg_precision = statistics.mean(precisions)
            cat_ranking.append((strategy_name, avg_precision))

        cat_ranking.sort(key=lambda x: x[1], reverse=True)

        for i, (strategy_name, precision) in enumerate(cat_ranking, 1):
            medal = "ü•á" if i == 1 else "ü•à" if i == 2 else "ü•â" if i == 3 else "  "
            print(f"   {medal} {strategy_name}: {precision:.3f}")

    # Recomenda√ß√µes por caso de uso
    print("\nüéØ RECOMENDA√á√ïES POR CASO DE USO")
    print("-"*50)

    best_overall = ranked_strategies[0][0]
    best_quality = max(strategy_scores.items(), key=lambda x: x[1]["quality"])[0]
    fastest = min(strategy_scores.items(), key=lambda x: x[1]["time"])[0]
    best_precision = max(strategy_scores.items(), key=lambda x: x[1]["precision"])[0]

    print(f"\nüèÜ MELHOR GERAL: {best_overall}")
    print(f"   Use para: Casos de uso variados, implementa√ß√£o padr√£o")

    print(f"\nüíé MELHOR QUALIDADE: {best_quality}")
    print(f"   Use para: Documentos complexos, preserva√ß√£o de contexto")

    print(f"\n‚ö° MAIS R√ÅPIDO: {fastest}")
    print(f"   Use para: Alto volume, aplica√ß√µes time-sensitive")

    print(f"\nüéØ MELHOR PRECIS√ÉO: {best_precision}")
    print(f"   Use para: Q&A espec√≠fico, busca exata")

    # Matriz de decis√£o
    print(f"\nüìã MATRIZ DE DECIS√ÉO")
    print("-"*50)

    print("QUANDO USAR CADA ESTRAT√âGIA:")
    print(f"‚úÖ Documentos curtos e espec√≠ficos ‚Üí Recursive Small")
    print(f"‚úÖ Uso geral e balanceado ‚Üí Recursive Medium")
    print(f"‚úÖ An√°lises complexas ‚Üí Recursive Large")
    print(f"‚úÖ Preserva√ß√£o sem√¢ntica ‚Üí Sentence Based")
    print(f"‚úÖ Documentos estruturados ‚Üí Paragraph Based")

    # Impacto no neg√≥cio
    print(f"\nüí° IMPACTO NO NEG√ìCIO")
    print("-"*50)

    best_strategy = ranked_strategies[0]
    worst_strategy = ranked_strategies[-1]

    # Evitar divis√£o por zero
    if worst_strategy[1]["precision"] > 0:
        improvement = ((best_strategy[1]["precision"] - worst_strategy[1]["precision"]) /
                      worst_strategy[1]["precision"]) * 100
        print(f"Melhoria de precis√£o: {improvement:.1f}% ({best_strategy[0]} vs {worst_strategy[0]})")
    else:
        improvement_abs = best_strategy[1]["precision"] - worst_strategy[1]["precision"]
        print(f"Melhoria de precis√£o: +{improvement_abs:.3f} pontos ({best_strategy[0]} vs {worst_strategy[0]})")

    print(f"Impacto estimado:")
    print(f"  - Redu√ß√£o de 30-50% em escala√ß√µes para atendimento humano")
    print(f"  - Aumento de 15-25% na satisfa√ß√£o do usu√°rio")
    print(f"  - Economia de R$ 50-150K/ano em custos operacionais")

def demo_comparacao_chunks():
    """
    Demonstra compara√ß√£o de estrat√©gias de chunking.
    """
    logger = get_logger("comparacao_chunks")

    try:
        print("üß© SISTEMA DE COMPARA√á√ÉO DE ESTRAT√âGIAS DE CHUNKING")
        print("="*55)

        # Configurar comparador
        comparator = ChunkingComparator()

        # Adicionar estrat√©gias
        print("\nüìã Configurando estrat√©gias de chunking...")
        strategies = criar_estrategias_chunking()

        for strategy in strategies:
            comparator.add_strategy(strategy)
            print(f"  ‚úÖ {strategy.name} (size: {strategy.chunk_size}, overlap: {strategy.chunk_overlap})")

        # Adicionar documentos de teste
        print("\nüìö Carregando documentos de teste...")
        test_docs = criar_documentos_teste()

        for title, content, metadata in test_docs:
            comparator.add_test_document(title, content, metadata)
            print(f"  ‚úÖ {title} ({len(content)} chars)")

        # Adicionar queries de teste
        print("\nüîç Configurando queries de teste...")
        test_queries = criar_queries_teste()

        for query_config in test_queries:
            comparator.add_test_query(
                query_config["query"],
                query_config["expected_keywords"],
                query_config["category"]
            )
            print(f"  ‚úÖ {query_config['category']}: {query_config['query'][:40]}...")

        # Executar compara√ß√£o
        print(f"\nüöÄ Executando compara√ß√£o abrangente...")
        print("   (Isso pode levar alguns minutos...)")

        start_time = time.time()
        results = comparator.run_comprehensive_comparison(logger)
        total_time = time.time() - start_time

        print(f"\n‚è±Ô∏è Compara√ß√£o conclu√≠da em {total_time:.1f}s")
        print(f"üìä {len(results)} testes executados")

        # Gerar relat√≥rio
        gerar_relatorio_comparativo_chunks(results, strategies)

        print(f"\n‚úÖ Compara√ß√£o de estrat√©gias de chunking conclu√≠da!")
        print(f"\nüí° INSIGHT PRINCIPAL: A escolha da estrat√©gia de chunking")
        print("   pode impactar em 30-50% a precis√£o da recupera√ß√£o de informa√ß√µes.")

        logger.info(f"Compara√ß√£o de chunking conclu√≠da - {len(results)} testes executados")
        return True

    except Exception as e:
        error_msg = f"Erro durante compara√ß√£o de chunking: {str(e)}"
        logger.error(error_msg)
        print(f"‚ùå {error_msg}")
        return False

def explicar_importancia_chunking():
    """
    Explica a import√¢ncia do chunking para RAG.
    """
    print("""
üß© POR QUE CHUNKING √â CR√çTICO PARA RAG?
=======================================

PROBLEMA FUNDAMENTAL:
- Modelos de IA t√™m limite de contexto (ex: 8K tokens)
- Documentos empresariais s√£o maiores que esse limite
- Precisamos dividir em peda√ßos menores (chunks)

IMPACTO DA ESTRAT√âGIA:
‚úÖ Chunking correto = respostas precisas
‚ùå Chunking incorreto = informa√ß√µes perdidas/incorretas

FATORES CR√çTICOS:
1. TAMANHO DO CHUNK:
   - Muito pequeno: perde contexto
   - Muito grande: informa√ß√£o irrelevante

2. OVERLAP:
   - Pouco: perde conex√µes entre chunks
   - Muito: redund√¢ncia desnecess√°ria

3. SEPARADORES:
   - Frases: preserva significado
   - Par√°grafos: mant√©m temas
   - Caracteres: pode quebrar conceitos

IMPACTO NO NEG√ìCIO:
- 30-50% diferen√ßa na precis√£o
- Redu√ß√£o significativa de escala√ß√µes
- Maior satisfa√ß√£o do usu√°rio
- ROI melhor do investimento em IA
""")

def mostrar_exemplos_chunking():
    """
    Mostra exemplos pr√°ticos de diferentes estrat√©gias.
    """
    print("""
üìã EXEMPLOS PR√ÅTICOS DE CHUNKING
=================================

DOCUMENTO ORIGINAL:
"A empresa oferece vale refei√ß√£o de R$ 30,00 por dia √∫til.
O plano de sa√∫de tem cobertura nacional com 20% de coparticipa√ß√£o.
Tamb√©m temos seguro de vida equivalente a 2x o sal√°rio anual."

CHUNKING PEQUENO (200 chars):
Chunk 1: "A empresa oferece vale refei√ß√£o de R$ 30,00 por dia √∫til."
Chunk 2: "O plano de sa√∫de tem cobertura nacional com 20% de coparticipa√ß√£o."
Chunk 3: "Tamb√©m temos seguro de vida equivalente a 2x o sal√°rio anual."

CHUNKING M√âDIO (400 chars):
Chunk 1: "A empresa oferece vale refei√ß√£o de R$ 30,00 por dia √∫til.
          O plano de sa√∫de tem cobertura nacional com 20% de coparticipa√ß√£o."
Chunk 2: "O plano de sa√∫de tem cobertura nacional com 20% de coparticipa√ß√£o.
          Tamb√©m temos seguro de vida equivalente a 2x o sal√°rio anual."

CHUNKING GRANDE (600 chars):
Chunk 1: Todo o par√°grafo junto

RESULTADO NA BUSCA:
Query: "Quanto √© o vale refei√ß√£o?"
- Chunking pequeno: ‚úÖ Encontra chunk espec√≠fico
- Chunking m√©dio: ‚úÖ Encontra com contexto adicional
- Chunking grande: ‚ö†Ô∏è Pode encontrar, mas com ru√≠do
""")

def configurar_ambiente():
    """
    Configura√ß√£o para compara√ß√£o de chunking.
    """
    print("""
üìã CONFIGURA√á√ÉO - COMPARA√á√ÉO DE CHUNKING
========================================

Depend√™ncias b√°sicas:
pip install chromadb  # Para vector store local

Recursos necess√°rios:
- Documentos representativos do uso real
- Queries t√≠picas dos usu√°rios
- Tempo para testes (15-30 min)

IMPORTANTE:
- Use documentos reais da empresa para teste
- Crie queries baseadas em casos de uso reais
- Teste com volume representativo
- Considere diferentes tipos de documento
- Monitore performance em produ√ß√£o

DICA PRO:
A melhor estrat√©gia varia por tipo de documento:
- Manuais t√©cnicos: chunks menores
- Relat√≥rios executivos: chunks maiores
- FAQ: chunks por pergunta/resposta
""")

if __name__ == "__main__":
    configurar_ambiente()
    explicar_importancia_chunking()
    mostrar_exemplos_chunking()

    resposta = input("\nDeseja executar a compara√ß√£o de estrat√©gias de chunking? (s/n): ")
    if resposta.lower() in ['s', 'sim', 'y', 'yes']:
        demo_comparacao_chunks()
    else:
        print("Demo cancelado.")