"""
Cen√°rio 2: Chat com Base Vetorial (RAG - Retrieval Augmented Generation)
Demonstra chat inteligente que consulta documentos pr√≥prios da empresa.
"""

from agentCore import get_llm, get_embeddings, get_logger
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.documents import Document
from langchain.chains import RetrievalQA
from langchain.prompts import ChatPromptTemplate
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
import os
from dotenv import load_dotenv

load_dotenv()

def criar_base_conhecimento():
    """
    Cria uma base de conhecimento exemplo com documentos da empresa.
    """
    documentos = [
        Document(
            page_content="""
            Pol√≠tica de Recursos Humanos - Benef√≠cios

            Nossa empresa oferece os seguintes benef√≠cios:
            - Vale refei√ß√£o: R$ 30,00 por dia √∫til
            - Vale alimenta√ß√£o: R$ 400,00 mensais
            - Plano de sa√∫de: cobertura nacional com coparticipa√ß√£o
            - Plano odontol√≥gico: 100% custeado pela empresa
            - Seguro de vida: 2x o sal√°rio anual
            - Aux√≠lio creche: at√© R$ 600,00 para filhos at√© 5 anos
            - Gympass: acesso a academias e apps de bem-estar
            """,
            metadata={"source": "rh_beneficios.pdf", "category": "recursos_humanos"}
        ),
        Document(
            page_content="""
            Pol√≠tica de Home Office

            O trabalho remoto √© permitido seguindo estas diretrizes:
            - M√°ximo 3 dias por semana em home office
            - Presen√ßa obrigat√≥ria nas segundas e sextas-feiras
            - Reuni√µes importantes presenciais devem ter prioridade
            - Equipamentos fornecidos: notebook, monitor adicional, cadeira ergon√¥mica
            - Internet: aux√≠lio de R$ 100,00 mensais para banda larga
            - Hor√°rio flex√≠vel: entrada entre 8h e 10h
            """,
            metadata={"source": "politica_home_office.pdf", "category": "recursos_humanos"}
        ),
        Document(
            page_content="""
            Produtos e Servi√ßos - Cat√°logo 2024

            Oferecemos as seguintes solu√ß√µes tecnol√≥gicas:
            - Desenvolvimento de software customizado
            - Consultoria em transforma√ß√£o digital
            - Implementa√ß√£o de sistemas ERP
            - Solu√ß√µes de intelig√™ncia artificial
            - An√°lise de dados e Business Intelligence
            - Seguran√ßa cibern√©tica e compliance
            - Cloud computing e migra√ß√£o AWS/Azure
            """,
            metadata={"source": "catalogo_produtos.pdf", "category": "comercial"}
        ),
        Document(
            page_content="""
            Processo de Vendas

            Nosso processo comercial segue estas etapas:
            1. Qualifica√ß√£o do lead (1-2 dias)
            2. Reuni√£o de descoberta com cliente (1 semana)
            3. Elabora√ß√£o de proposta t√©cnica (1-2 semanas)
            4. Apresenta√ß√£o comercial (1 semana)
            5. Negocia√ß√£o e fechamento (1-2 semanas)
            6. Kick-off do projeto (1 semana)

            Prazo m√©dio total: 6-8 semanas da prospec√ß√£o ao in√≠cio do projeto.
            """,
            metadata={"source": "processo_vendas.pdf", "category": "comercial"}
        ),
        Document(
            page_content="""
            Suporte T√©cnico - SLA

            N√≠veis de suporte oferecidos:
            - Cr√≠tico (P1): Resposta em 1 hora, resolu√ß√£o em 4 horas
            - Alto (P2): Resposta em 4 horas, resolu√ß√£o em 1 dia √∫til
            - M√©dio (P3): Resposta em 1 dia √∫til, resolu√ß√£o em 3 dias √∫teis
            - Baixo (P4): Resposta em 2 dias √∫teis, resolu√ß√£o em 1 semana

            Canais de atendimento:
            - Email: suporte@empresa.com
            - Telefone: (11) 1234-5678
            - Chat: dispon√≠vel 24/7 no portal do cliente
            """,
            metadata={"source": "sla_suporte.pdf", "category": "tecnico"}
        )
    ]

    return documentos

def demo_chat_com_rag():
    """
    Demonstra chat inteligente que consulta base de conhecimento interna.
    """
    logger = get_logger("chat_rag")

    # Configura√ß√µes
    provider = os.getenv("LLM_PROVIDER", "ollama")
    model_name = os.getenv("MODEL_NAME", "llama3:latest")
    vector_provider = os.getenv("VECTOR_PROVIDER", "chroma")

    logger.info(f"Iniciando RAG com LLM: {provider}/{model_name}, Vector Store: {vector_provider}")

    try:
        # 1. Criar base de conhecimento
        print("üìö Criando base de conhecimento...")
        documentos = criar_base_conhecimento()

        # 2. Configurar chunking strategy
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50,
            length_function=len,
        )

        # 3. Processar documentos
        chunks = text_splitter.split_documents(documentos)

        print(f"‚úÖ {len(chunks)} chunks criados dos documentos")

        # 4. Configurar embeddings e vector store
        embeddings = get_embeddings(provider_name=provider)

        # 5. Criar vector store usando Chroma diretamente
        print("üîç Indexando documentos...")
        texts = [chunk.page_content for chunk in chunks]
        metadatas = [chunk.metadata for chunk in chunks]

        vector_store = Chroma.from_texts(
            texts=texts,
            embedding=embeddings,
            metadatas=metadatas,
            collection_name="knowledge_base_demo"
        )
        print("‚úÖ Documentos indexados com sucesso")

        # 6. Configurar LLM
        llm = get_llm(provider_name=provider)

        # 7. Criar retriever
        retriever = vector_store.as_retriever(
            search_kwargs={"k": 3}  # Buscar top 3 documentos mais relevantes
        )

        # 8. Template de prompt otimizado para RAG
        prompt_template = ChatPromptTemplate.from_messages([
            ("system", """Voc√™ √© um assistente especializado em responder d√∫vidas sobre nossa empresa.
            Use APENAS as informa√ß√µes fornecidas no contexto abaixo para responder.
            Se a informa√ß√£o n√£o estiver dispon√≠vel no contexto, diga claramente que n√£o possui essa informa√ß√£o.

            Contexto:
            {context}"""),
            ("human", "{question}")
        ])

        # 9. Perguntas de exemplo
        perguntas = [
            "Quais s√£o os benef√≠cios oferecidos pela empresa?",
            "Como funciona a pol√≠tica de home office?",
            "Qual √© o prazo m√©dio do processo de vendas?",
            "Quais s√£o os n√≠veis de SLA do suporte t√©cnico?",
            "A empresa oferece aux√≠lio para internet em home office?"
        ]

        print("\n" + "="*80)
        print("ü§ñ ASSISTENTE EMPRESARIAL COM RAG")
        print("="*80)

        for i, pergunta in enumerate(perguntas, 1):
            print(f"\n{'='*60}")
            print(f"PERGUNTA {i}: {pergunta}")
            print('='*60)

            # Recuperar documentos relevantes
            docs_relevantes = retriever.get_relevant_documents(pergunta)

            # Preparar contexto
            contexto = "\n\n".join([doc.page_content for doc in docs_relevantes])

            # Gerar resposta
            messages = prompt_template.format_messages(
                context=contexto,
                question=pergunta
            )

            response = llm.invoke(messages)

            print(f"RESPOSTA: {response.content}")

            # Mostrar fontes consultadas
            fontes = [doc.metadata.get('source', 'Documento sem fonte') for doc in docs_relevantes]
            print(f"\nüìã FONTES CONSULTADAS: {', '.join(set(fontes))}")
            print()

            logger.info(f"Pergunta {i} processada - {len(docs_relevantes)} documentos consultados")

        print("\n‚úÖ Demo RAG conclu√≠da com sucesso!")
        print("\nüí° OBSERVA√á√ÉO: O sistema consultou apenas documentos internos da empresa,")
        print("   garantindo respostas precisas e alinhadas com pol√≠ticas internas.")

        logger.info("Demo RAG finalizada com sucesso")

    except Exception as e:
        error_msg = f"Erro durante execu√ß√£o do RAG: {str(e)}"
        logger.error(error_msg)
        print(f"‚ùå {error_msg}")
        return False

    return True

def demonstrar_comparacao():
    """
    Demonstra a diferen√ßa entre chat normal e chat com RAG.
    """
    print("""
üîç COMPARA√á√ÉO: CHAT NORMAL vs CHAT COM RAG

CHAT NORMAL (Cen√°rio 1):
‚ùå Respostas gen√©ricas sobre recursos humanos
‚ùå Informa√ß√µes podem estar desatualizadas
‚ùå N√£o conhece pol√≠ticas espec√≠ficas da empresa
‚ùå Pode "alucinar" informa√ß√µes incorretas

CHAT COM RAG (Cen√°rio 2):
‚úÖ Respostas baseadas em documentos reais da empresa
‚úÖ Informa√ß√µes sempre atualizadas conforme documenta√ß√£o
‚úÖ Conhece pol√≠ticas, processos e procedimentos espec√≠ficos
‚úÖ Cita fontes das informa√ß√µes
‚úÖ Admite quando n√£o possui informa√ß√£o
""")

def configurar_ambiente():
    """
    Instru√ß√µes para configura√ß√£o do ambiente.
    """
    print("""
üìã CONFIGURA√á√ÉO DO AMBIENTE - RAG
==================================

Vari√°veis de ambiente necess√°rias:

1. LLM Provider:
   export LLM_PROVIDER=ollama
   export MODEL_NAME=llama3:latest

2. Vector Store (escolha uma):
   # ChromaDB (local, sem configura√ß√£o adicional)
   export VECTOR_PROVIDER=chroma

   # AWS (requer credenciais)
   export VECTOR_PROVIDER=aws
   export AWS_ACCESS_KEY_ID=sua_chave
   export AWS_SECRET_ACCESS_KEY=sua_chave_secreta
   export AWS_REGION=us-east-1

3. Depend√™ncias:
   pip install chromadb  # Para ChromaDB local
   pip install langchain-chroma
""")

if __name__ == "__main__":
    configurar_ambiente()
    demonstrar_comparacao()

    resposta = input("\nDeseja executar o demo RAG? (s/n): ")
    if resposta.lower() in ['s', 'sim', 'y', 'yes']:
        demo_chat_com_rag()
    else:
        print("Demo cancelado.")