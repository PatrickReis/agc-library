"""
Cen√°rio 6: Evals B√°sico - Valida√ß√£o e Qualidade de Prompts
Demonstra como validar e melhorar a qualidade de sistemas de IA.
"""

from agentCore import get_llm, get_logger
from agentCore.evaluation.prompt_evaluator import PromptEvaluator
from agentCore.evaluation.metrics_collector import MetricsCollector
from langchain_core.messages import HumanMessage, SystemMessage
import json
import os
import time
from datetime import datetime
from typing import List, Dict, Any
from dotenv import load_dotenv

load_dotenv()

class EvalDataset:
    """Classe para gerenciar datasets de avalia√ß√£o."""

    def __init__(self):
        self.test_cases = []

    def add_test_case(self, input_text: str, expected_output: str, category: str, criteria: dict = None):
        """Adiciona um caso de teste ao dataset."""
        test_case = {
            "id": len(self.test_cases) + 1,
            "input": input_text,
            "expected": expected_output,
            "category": category,
            "criteria": criteria or {},
            "timestamp": datetime.now().isoformat()
        }
        self.test_cases.append(test_case)

    def get_by_category(self, category: str) -> List[Dict]:
        """Retorna casos de teste por categoria."""
        return [tc for tc in self.test_cases if tc["category"] == category]

def criar_dataset_atendimento():
    """
    Cria dataset para avaliar sistema de atendimento ao cliente.
    """
    dataset = EvalDataset()

    # Casos de teste para diferentes categorias
    dataset.add_test_case(
        input_text="Oi, gostaria de saber sobre os benef√≠cios da empresa",
        expected_output="benef√≠cios oferecidos: vale refei√ß√£o, vale alimenta√ß√£o, plano de sa√∫de, plano odontol√≥gico",
        category="beneficios_rh",
        criteria={"deve_mencionar": ["vale refei√ß√£o", "plano de sa√∫de"], "tom": "profissional"}
    )

    dataset.add_test_case(
        input_text="Como funciona a pol√≠tica de home office?",
        expected_output="m√°ximo 3 dias por semana, presen√ßa obrigat√≥ria segundas e sextas",
        category="politicas_rh",
        criteria={"deve_mencionar": ["3 dias", "segundas e sextas"], "precisao": "alta"}
    )

    dataset.add_test_case(
        input_text="Qual √© o processo de vendas da empresa?",
        expected_output="6 etapas: qualifica√ß√£o, descoberta, proposta, apresenta√ß√£o, negocia√ß√£o, kick-off",
        category="comercial",
        criteria={"deve_mencionar": ["6 etapas", "qualifica√ß√£o", "proposta"], "estrutura": "sequencial"}
    )

    dataset.add_test_case(
        input_text="Quais s√£o os n√≠veis de SLA do suporte?",
        expected_output="4 n√≠veis: P1 (1h resposta), P2 (4h), P3 (1 dia), P4 (2 dias)",
        category="suporte_tecnico",
        criteria={"deve_mencionar": ["P1", "1 hora", "4 n√≠veis"], "precisao": "alta"}
    )

    dataset.add_test_case(
        input_text="Como est√° o clima hoje?",
        expected_output="n√£o tenho informa√ß√µes sobre clima, sou especializado em d√∫vidas da empresa",
        category="fora_escopo",
        criteria={"deve_reconhecer": "fora do escopo", "tom": "educado"}
    )

    return dataset

def criar_metricas_avaliacao():
    """
    Define m√©tricas para avalia√ß√£o de qualidade.
    """
    return {
        "relevancia": {
            "description": "Qu√£o relevante √© a resposta para a pergunta",
            "weight": 0.3,
            "scale": "1-5"
        },
        "precisao": {
            "description": "Precis√£o factual das informa√ß√µes",
            "weight": 0.3,
            "scale": "1-5"
        },
        "completude": {
            "description": "Completude da resposta",
            "weight": 0.2,
            "scale": "1-5"
        },
        "tom_profissional": {
            "description": "Adequa√ß√£o do tom profissional",
            "weight": 0.1,
            "scale": "1-5"
        },
        "clareza": {
            "description": "Clareza e objetividade",
            "weight": 0.1,
            "scale": "1-5"
        }
    }

def avaliar_resposta_automatica(resposta: str, caso_teste: Dict, metricas: Dict) -> Dict:
    """
    Avalia uma resposta automaticamente baseado em crit√©rios.
    """
    scores = {}

    # Relev√¢ncia - verifica se resposta est√° relacionada √† pergunta
    input_lower = caso_teste["input"].lower()
    resposta_lower = resposta.lower()

    if caso_teste["category"] == "fora_escopo":
        # Para perguntas fora do escopo, deve reconhecer limita√ß√£o
        if any(term in resposta_lower for term in ["n√£o tenho", "n√£o posso", "especializado em", "fora do"]):
            scores["relevancia"] = 5
        else:
            scores["relevancia"] = 1
    else:
        # Para perguntas no escopo, deve ter palavras-chave relacionadas
        palavras_chave = {
            "beneficios_rh": ["benef√≠cio", "vale", "plano", "aux√≠lio"],
            "politicas_rh": ["pol√≠tica", "home office", "dias", "presen√ßa"],
            "comercial": ["processo", "vendas", "etapa", "cliente"],
            "suporte_tecnico": ["sla", "suporte", "resposta", "resolu√ß√£o"]
        }

        palavras_categoria = palavras_chave.get(caso_teste["category"], [])
        matches = sum(1 for palavra in palavras_categoria if palavra in resposta_lower)
        scores["relevancia"] = min(5, max(1, matches + 1))

    # Precis√£o - verifica crit√©rios espec√≠ficos
    criterios = caso_teste.get("criteria", {})
    deve_mencionar = criterios.get("deve_mencionar", [])

    if deve_mencionar:
        mentions = sum(1 for item in deve_mencionar if item.lower() in resposta_lower)
        scores["precisao"] = min(5, max(1, (mentions / len(deve_mencionar)) * 5))
    else:
        scores["precisao"] = 3  # Neutro se n√£o h√° crit√©rios espec√≠ficos

    # Completude - baseado no tamanho e estrutura da resposta
    if len(resposta) < 50:
        scores["completude"] = 2
    elif len(resposta) < 150:
        scores["completude"] = 4
    else:
        scores["completude"] = 5

    # Tom profissional - verifica se evita linguagem inadequada
    linguagem_inadequada = ["cara", "mano", "tipo assim", "n√©"]
    if any(term in resposta_lower for term in linguagem_inadequada):
        scores["tom_profissional"] = 2
    else:
        scores["tom_profissional"] = 4

    # Clareza - verifica estrutura e organiza√ß√£o
    if any(char in resposta for char in [":", "-", "‚Ä¢"]) or "." in resposta:
        scores["clareza"] = 4
    else:
        scores["clareza"] = 3

    return scores

def calcular_score_geral(scores: Dict, metricas: Dict) -> float:
    """
    Calcula score geral ponderado.
    """
    score_total = 0
    peso_total = 0

    for metrica, peso_info in metricas.items():
        if metrica in scores:
            peso = peso_info["weight"]
            score_total += scores[metrica] * peso
            peso_total += peso

    return (score_total / peso_total) if peso_total > 0 else 0

def demo_evals_basico():
    """
    Demonstra sistema b√°sico de avalia√ß√£o de prompts.
    """
    logger = get_logger("evals_basico")

    try:
        print("üß™ SISTEMA DE AVALIA√á√ÉO B√ÅSICO (EVALS)")
        print("="*50)

        # Configurar LLM
        provider = os.getenv("LLM_PROVIDER", "ollama")
        model_name = os.getenv("MODEL_NAME", "llama3:latest")
        llm = get_llm(provider_name=provider)

        # Criar dataset de teste
        print("\nüìä Criando dataset de avalia√ß√£o...")
        dataset = criar_dataset_atendimento()
        metricas = criar_metricas_avaliacao()

        print(f"‚úÖ Dataset criado com {len(dataset.test_cases)} casos de teste")
        print(f"‚úÖ {len(metricas)} m√©tricas definidas")

        # Sistema de prompt a ser testado
        system_prompt = """Voc√™ √© um assistente especializado em responder d√∫vidas sobre nossa empresa.
        Responda de forma clara, objetiva e profissional.
        Use apenas informa√ß√µes que voc√™ tem certeza.
        Se n√£o souber algo, diga claramente que n√£o possui essa informa√ß√£o."""

        # Executar avalia√ß√µes
        print("\n" + "="*60)
        print("üîÑ EXECUTANDO AVALIA√á√ïES")
        print("="*60)

        resultados = []
        tempo_inicio = time.time()

        for i, caso in enumerate(dataset.test_cases, 1):
            print(f"\nüìù Teste {i}/{len(dataset.test_cases)}: {caso['category']}")
            print(f"Pergunta: {caso['input']}")

            # Gerar resposta
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=caso['input'])
            ]

            resposta = llm.invoke(messages)
            resposta_texto = resposta.content

            print(f"Resposta: {resposta_texto}")

            # Avaliar resposta
            scores = avaliar_resposta_automatica(resposta_texto, caso, metricas)
            score_geral = calcular_score_geral(scores, metricas)

            # Armazenar resultado
            resultado = {
                "test_id": caso["id"],
                "input": caso["input"],
                "expected": caso["expected"],
                "actual": resposta_texto,
                "category": caso["category"],
                "scores": scores,
                "score_geral": score_geral,
                "status": "PASS" if score_geral >= 3.0 else "FAIL"
            }
            resultados.append(resultado)

            # Mostrar scores
            print(f"Scores: {scores}")
            print(f"Score Geral: {score_geral:.2f}/5.0 - {resultado['status']}")

        tempo_total = time.time() - tempo_inicio

        # Gerar relat√≥rio final
        print("\n" + "="*60)
        print("üìä RELAT√ìRIO DE AVALIA√á√ÉO")
        print("="*60)

        # Estat√≠sticas gerais
        scores_gerais = [r["score_geral"] for r in resultados]
        score_medio = sum(scores_gerais) / len(scores_gerais)
        tests_passed = len([r for r in resultados if r["status"] == "PASS"])
        pass_rate = (tests_passed / len(resultados)) * 100

        print(f"\nüìà ESTAT√çSTICAS GERAIS:")
        print(f"Score M√©dio: {score_medio:.2f}/5.0")
        print(f"Taxa de Sucesso: {pass_rate:.1f}% ({tests_passed}/{len(resultados)})")
        print(f"Tempo Total: {tempo_total:.2f}s")
        print(f"Tempo por Teste: {tempo_total/len(resultados):.2f}s")

        # An√°lise por categoria
        print(f"\nüìä DESEMPENHO POR CATEGORIA:")
        categorias = {}
        for resultado in resultados:
            cat = resultado["category"]
            if cat not in categorias:
                categorias[cat] = []
            categorias[cat].append(resultado["score_geral"])

        for categoria, scores in categorias.items():
            score_cat = sum(scores) / len(scores)
            print(f"{categoria}: {score_cat:.2f}/5.0 ({len(scores)} testes)")

        # An√°lise por m√©trica
        print(f"\nüéØ DESEMPENHO POR M√âTRICA:")
        for metrica in metricas.keys():
            scores_metrica = []
            for resultado in resultados:
                if metrica in resultado["scores"]:
                    scores_metrica.append(resultado["scores"][metrica])

            if scores_metrica:
                media_metrica = sum(scores_metrica) / len(scores_metrica)
                print(f"{metrica}: {media_metrica:.2f}/5.0")

        # Casos problem√°ticos
        print(f"\n‚ùå CASOS QUE FALHARAM:")
        falhas = [r for r in resultados if r["status"] == "FAIL"]
        if falhas:
            for falha in falhas:
                print(f"- {falha['category']}: Score {falha['score_geral']:.2f}")
                print(f"  Pergunta: {falha['input']}")
                print(f"  Problema: Score abaixo de 3.0")
        else:
            print("üéâ Nenhum caso falhou!")

        # Recomenda√ß√µes
        print(f"\nüí° RECOMENDA√á√ïES:")
        if score_medio < 3.0:
            print("‚ùå Sistema precisa de melhorias significativas")
            print("  - Revisar prompt system")
            print("  - Adicionar mais contexto/exemplos")
            print("  - Considerar fine-tuning do modelo")
        elif score_medio < 4.0:
            print("‚ö†Ô∏è Sistema funcional mas pode melhorar")
            print("  - Otimizar prompts para categorias com menor score")
            print("  - Adicionar valida√ß√µes espec√≠ficas")
        else:
            print("‚úÖ Sistema com boa qualidade")
            print("  - Continuar monitoramento")
            print("  - Considerar testes A/B para otimiza√ß√µes")

        print("\n‚úÖ Avalia√ß√£o b√°sica conclu√≠da!")
        print(f"\nüí° PR√ìXIMO PASSO: Ver Cen√°rio 7 para Evals avan√ßados com")
        print("   compara√ß√£o de modelos e m√©tricas mais sofisticadas.")

        logger.info(f"Evals b√°sico conclu√≠do - Score m√©dio: {score_medio:.2f}")
        return True

    except Exception as e:
        error_msg = f"Erro durante execu√ß√£o dos Evals: {str(e)}"
        logger.error(error_msg)
        print(f"‚ùå {error_msg}")
        return False

def explicar_conceitos_evals():
    """
    Explica conceitos fundamentais de Evals.
    """
    print("""
üìö O QUE S√ÉO EVALS?
===================

EVALUATIONS (Evals) s√£o sistemas para:
‚úÖ Medir qualidade de sistemas de IA
‚úÖ Comparar diferentes prompts/modelos
‚úÖ Detectar regress√µes de performance
‚úÖ Validar melhorias antes do deploy

COMPONENTES PRINCIPAIS:
1. üìä Dataset de teste (casos conhecidos)
2. üéØ M√©tricas de avalia√ß√£o (como medir qualidade)
3. ü§ñ Sistema automatizado de scoring
4. üìà Relat√≥rios e an√°lises

BENEF√çCIOS:
- Qualidade consistente em produ√ß√£o
- Detec√ß√£o precoce de problemas
- Otimiza√ß√£o baseada em dados
- Confian√ßa para fazer mudan√ßas
""")

def mostrar_tipos_metricas():
    """
    Mostra diferentes tipos de m√©tricas de avalia√ß√£o.
    """
    print("""
üéØ TIPOS DE M√âTRICAS DE AVALIA√á√ÉO
=================================

1. M√âTRICAS DE QUALIDADE:
   - Relev√¢ncia: resposta relacionada √† pergunta
   - Precis√£o: fatos corretos
   - Completude: informa√ß√£o suficiente
   - Clareza: f√°cil de entender

2. M√âTRICAS DE COMPORTAMENTO:
   - Tom adequado (profissional, amig√°vel)
   - Seguimento de instru√ß√µes
   - Reconhecimento de limites
   - Consist√™ncia de respostas

3. M√âTRICAS T√âCNICAS:
   - Tempo de resposta
   - Uso de tokens/recursos
   - Taxa de erro
   - Disponibilidade

4. M√âTRICAS DE NEG√ìCIO:
   - Satisfa√ß√£o do usu√°rio
   - Redu√ß√£o de tickets de suporte
   - Convers√£o/engagement
   - ROI do sistema
""")

def configurar_ambiente():
    """
    Configura√ß√£o para Evals.
    """
    print("""
üìã CONFIGURA√á√ÉO - EVALS B√ÅSICO
===============================

Depend√™ncias:
pip install pytest  # Para estrutura de testes
pip install pandas  # Para an√°lise de dados

Configura√ß√£o:
export LLM_PROVIDER=ollama
export MODEL_NAME=llama3:latest

IMPORTANTE:
- Use temperatura baixa (0.1) para consist√™ncia
- Crie datasets representativos do uso real
- Defina crit√©rios claros de sucesso/falha
- Execute Evals regularmente (CI/CD)
""")

if __name__ == "__main__":
    configurar_ambiente()
    explicar_conceitos_evals()
    mostrar_tipos_metricas()

    resposta = input("\nDeseja executar o demo de Evals B√°sico? (s/n): ")
    if resposta.lower() in ['s', 'sim', 'y', 'yes']:
        demo_evals_basico()
    else:
        print("Demo cancelado.")