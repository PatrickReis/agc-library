"""
Cen√°rio 8: Compara√ß√£o Abrangente de Modelos de IA
Demonstra metodologia cient√≠fica para escolher o melhor modelo para cada caso de uso.
"""

from agentCore import get_llm, get_logger
from agentCore.evaluation.model_comparison import ModelComparator
from langchain_core.messages import HumanMessage, SystemMessage
import json
import os
import time
import statistics
import matplotlib.pyplot as plt
import pandas as pd
from datetime import datetime
from typing import List, Dict, Any, Tuple
from dataclasses import dataclass, asdict
from dotenv import load_dotenv

load_dotenv()

@dataclass
class ModelSpec:
    """Especifica√ß√£o detalhada de um modelo."""
    name: str
    provider: str
    model_id: str
    context_window: int
    max_tokens: int
    cost_per_1k_tokens: float
    latency_expected: float
    strengths: List[str]
    limitations: List[str]
    use_cases: List[str]

@dataclass
class BenchmarkResult:
    """Resultado de benchmark para um modelo."""
    model_name: str
    category: str
    test_name: str
    score: float
    execution_time: float
    tokens_used: int
    cost: float
    error_rate: float
    consistency: float

class ModelBenchmark:
    """Sistema abrangente de benchmark de modelos."""

    def __init__(self):
        self.models = {}
        self.benchmarks = []
        self.results = []

    def register_model(self, spec: ModelSpec):
        """Registra um modelo para compara√ß√£o."""
        self.models[spec.name] = spec

    def add_benchmark(self, name: str, category: str, test_cases: List[Dict], evaluation_criteria: Dict):
        """Adiciona um benchmark espec√≠fico."""
        benchmark = {
            "name": name,
            "category": category,
            "test_cases": test_cases,
            "criteria": evaluation_criteria
        }
        self.benchmarks.append(benchmark)

    def evaluate_reasoning_capability(self, response: str, expected_steps: List[str]) -> float:
        """Avalia capacidade de racioc√≠nio l√≥gico."""
        response_lower = response.lower()
        score = 0.0

        # Verifica se seguiu etapas l√≥gicas
        for step in expected_steps:
            if step.lower() in response_lower:
                score += 0.3

        # Verifica estrutura de racioc√≠nio
        reasoning_indicators = ["primeiro", "segundo", "ent√£o", "portanto", "porque", "logo"]
        reasoning_count = sum(1 for indicator in reasoning_indicators if indicator in response_lower)
        score += min(0.4, reasoning_count * 0.1)

        return min(1.0, score)

    def evaluate_factual_accuracy(self, response: str, facts: List[str]) -> float:
        """Avalia precis√£o factual."""
        response_lower = response.lower()
        correct_facts = 0

        for fact in facts:
            if fact.lower() in response_lower:
                correct_facts += 1

        return correct_facts / len(facts) if facts else 0.0

    def evaluate_creativity(self, response: str) -> float:
        """Avalia criatividade e originalidade."""
        # M√©tricas simples de criatividade
        unique_words = len(set(response.lower().split()))
        total_words = len(response.split())

        if total_words == 0:
            return 0.0

        vocabulary_diversity = unique_words / total_words

        # Verifica uso de linguagem elaborada
        sophisticated_words = ["innovative", "comprehensive", "strategic", "optimize", "enhance"]
        sophistication = sum(1 for word in sophisticated_words if word in response.lower())

        creativity_score = (vocabulary_diversity * 0.6) + (min(1.0, sophistication * 0.1) * 0.4)
        return min(1.0, creativity_score)

    def measure_consistency(self, model_name: str, test_case: Dict, runs: int = 3) -> Tuple[List[str], float]:
        """Mede consist√™ncia de respostas m√∫ltiplas."""
        spec = self.models[model_name]
        responses = []

        try:
            llm = get_llm(provider_name=spec.provider)

            messages = [
                SystemMessage(content="Responda de forma clara e consistente."),
                HumanMessage(content=test_case["input"])
            ]

            for _ in range(runs):
                response = llm.invoke(messages)
                responses.append(response.content)

            # Calcular similaridade entre respostas
            similarities = []
            for i in range(len(responses)):
                for j in range(i + 1, len(responses)):
                    # Similaridade simples baseada em palavras comuns
                    words1 = set(responses[i].lower().split())
                    words2 = set(responses[j].lower().split())
                    if len(words1) > 0 and len(words2) > 0:
                        similarity = len(words1 & words2) / len(words1 | words2)
                        similarities.append(similarity)

            consistency = statistics.mean(similarities) if similarities else 0.0
            return responses, consistency

        except Exception as e:
            print(f"Erro ao medir consist√™ncia para {model_name}: {str(e)}")
            return [], 0.0

    def run_comprehensive_benchmark(self, logger) -> List[BenchmarkResult]:
        """Executa benchmark abrangente de todos os modelos."""
        results = []

        print(f"\nüöÄ Iniciando benchmark de {len(self.models)} modelos")
        print(f"   Executando {len(self.benchmarks)} categorias de teste")

        for benchmark in self.benchmarks:
            category = benchmark["category"]
            test_cases = benchmark["test_cases"]
            criteria = benchmark["criteria"]

            print(f"\nüìä Categoria: {category}")

            for model_name, spec in self.models.items():
                print(f"  ü§ñ Testando {model_name}...")

                category_results = []

                try:
                    llm = get_llm(provider_name=spec.provider)

                    for test_case in test_cases:
                        start_time = time.time()

                        # Executar teste
                        messages = [
                            SystemMessage(content=test_case.get("system_prompt", "Responda de forma clara e precisa.")),
                            HumanMessage(content=test_case["input"])
                        ]

                        response = llm.invoke(messages)
                        execution_time = time.time() - start_time

                        # Avaliar resposta baseado nos crit√©rios
                        scores = {}

                        if "reasoning_steps" in criteria:
                            scores["reasoning"] = self.evaluate_reasoning_capability(
                                response.content, criteria["reasoning_steps"]
                            )

                        if "factual_accuracy" in criteria:
                            scores["accuracy"] = self.evaluate_factual_accuracy(
                                response.content, criteria["factual_accuracy"]
                            )

                        if "creativity" in criteria and criteria["creativity"]:
                            scores["creativity"] = self.evaluate_creativity(response.content)

                        # Medir consist√™ncia
                        _, consistency = self.measure_consistency(model_name, test_case, runs=2)

                        # Calcular score geral
                        overall_score = statistics.mean(scores.values()) if scores else 0.5

                        # Estimar custo
                        tokens_used = len(response.content) // 4  # Aproxima√ß√£o
                        cost = (tokens_used / 1000) * spec.cost_per_1k_tokens

                        # Criar resultado
                        result = BenchmarkResult(
                            model_name=model_name,
                            category=category,
                            test_name=test_case["name"],
                            score=overall_score,
                            execution_time=execution_time,
                            tokens_used=tokens_used,
                            cost=cost,
                            error_rate=0.0,  # Simplificado para demo
                            consistency=consistency
                        )

                        category_results.append(result)
                        results.append(result)

                    # Resumo da categoria para o modelo
                    avg_score = statistics.mean([r.score for r in category_results])
                    avg_time = statistics.mean([r.execution_time for r in category_results])
                    total_cost = sum([r.cost for r in category_results])

                    print(f"    Score: {avg_score:.3f}, Tempo: {avg_time:.2f}s, Custo: ${total_cost:.4f}")

                except Exception as e:
                    print(f"    ‚ùå Erro: {str(e)}")
                    logger.error(f"Erro no benchmark {category} para {model_name}: {str(e)}")

        return results

def criar_modelos_comparacao() -> List[ModelSpec]:
    """Cria especifica√ß√µes de modelos para compara√ß√£o."""
    return [
        ModelSpec(
            name="Llama-3.3-Latest",
            provider="ollama",
            model_id="llama3:latest",
            context_window=8192,
            max_tokens=200,
            cost_per_1k_tokens=0.0001,  # Custo local muito baixo
            latency_expected=2.0,
            strengths=["Efici√™ncia", "Privacidade", "Custo baixo"],
            limitations=["Capacidade limitada", "Conhecimento at√© cutoff"],
            use_cases=["Tarefas simples", "Alto volume", "Dados sens√≠veis"]
        ),
        ModelSpec(
            name="GptOss",
            provider="ollama",
            model_id="gpt-oss:latest",
            context_window=16384,
            max_tokens=200,
            cost_per_1k_tokens=0.00015,  # Custo local muito baixo
            latency_expected=1.5,
            strengths=["Velocidade", "Custo-benef√≠cio", "API est√°vel"],
            limitations=["Conhecimento at√© 2022", "Capacidades limitadas"],
            use_cases=["Uso geral", "Chatbots", "Automa√ß√£o simples"]
        ),
         ModelSpec(
            name="mistral",
            provider="ollama",
            model_id="mistral:latest",
            context_window=8192,
            max_tokens=200,
            cost_per_1k_tokens=0.00008, 
            latency_expected=1.5,
            strengths=["Velocidade", "Custo-benef√≠cio", "API est√°vel"],
            limitations=["Conhecimento at√© 2022", "Capacidades limitadas"],
            use_cases=["Uso geral", "Chatbots", "Automa√ß√£o simples"]
        ),
        # ModelSpec(
        #     name="GPT-3.5-Turbo",
        #     provider="openai",
        #     model_id="gpt-3.5-turbo",
        #     context_window=16384,
        #     max_tokens=200,
        #     cost_per_1k_tokens=0.0015,
        #     latency_expected=1.5,
        #     strengths=["Velocidade", "Custo-benef√≠cio", "API est√°vel"],
        #     limitations=["Conhecimento at√© 2022", "Capacidades limitadas"],
        #     use_cases=["Uso geral", "Chatbots", "Automa√ß√£o simples"]
        # ),
        # ModelSpec(
        #     name="GPT-4",
        #     provider="openai",
        #     model_id="gpt-4",
        #     context_window=8192,
        #     max_tokens=200,
        #     cost_per_1k_tokens=0.03,
        #     latency_expected=3.0,
        #     strengths=["Qualidade superior", "Racioc√≠nio complexo", "Criatividade"],
        #     limitations=["Custo alto", "Lat√™ncia maior"],
        #     use_cases=["An√°lises complexas", "Cria√ß√£o de conte√∫do", "Consultoria"]
        # )
    ]

def criar_benchmarks_abrangentes() -> List[Dict]:
    """Cria benchmarks para diferentes capacidades."""
    return [
        {
            "name": "Racioc√≠nio L√≥gico",
            "category": "reasoning",
            "test_cases": [
                {
                    "name": "analise_financeira",
                    "input": "Uma empresa teve receita de R$ 1M no Q1, R$ 1.2M no Q2 e R$ 1.5M no Q3. Analise a tend√™ncia e projete Q4.",
                    "system_prompt": "Analise os dados numericamente e explique seu racioc√≠nio passo a passo."
                },
                {
                    "name": "problema_logico",
                    "input": "Se todo A √© B, e todo B √© C, e Jo√£o √© A, o que podemos concluir sobre Jo√£o?",
                    "system_prompt": "Use l√≥gica formal para resolver este problema."
                }
            ],
            "criteria": {
                "reasoning_steps": ["crescimento", "tend√™ncia", "percentual", "proje√ß√£o"],
                "factual_accuracy": ["25%", "crescimento", "trimestre"]
            }
        },
        {
            "name": "Conhecimento Factual",
            "category": "knowledge",
            "test_cases": [
                {
                    "name": "geografia_brasil",
                    "input": "Quais s√£o as 5 maiores cidades do Brasil por popula√ß√£o?",
                    "system_prompt": "Forne√ßa informa√ß√µes factuais precisas."
                },
                {
                    "name": "tecnologia_atual",
                    "input": "Explique o que √© intelig√™ncia artificial e suas principais aplica√ß√µes empresariais.",
                    "system_prompt": "Seja preciso e atual nas informa√ß√µes."
                }
            ],
            "criteria": {
                "factual_accuracy": ["S√£o Paulo", "Rio de Janeiro", "Bras√≠lia", "Salvador", "Fortaleza"]
            }
        },
        {
            "name": "Criatividade",
            "category": "creativity",
            "test_cases": [
                {
                    "name": "estrategia_marketing",
                    "input": "Crie uma estrat√©gia criativa de marketing para lan√ßar um novo aplicativo de produtividade.",
                    "system_prompt": "Seja criativo e inovador, mas mantenha viabilidade pr√°tica."
                },
                {
                    "name": "solucao_problema",
                    "input": "Como uma empresa pode reduzir custos operacionais sem demitir funcion√°rios?",
                    "system_prompt": "Pense fora da caixa, mas seja pr√°tico."
                }
            ],
            "criteria": {
                "creativity": True
            }
        },
        {
            "name": "Efici√™ncia Empresarial",
            "category": "business",
            "test_cases": [
                {
                    "name": "analise_swot",
                    "input": "Fa√ßa uma an√°lise SWOT para uma startup de tecnologia que acabou de receber investimento.",
                    "system_prompt": "Use framework SWOT estruturado e seja conciso."
                },
                {
                    "name": "kpis_vendas",
                    "input": "Quais KPIs uma equipe de vendas B2B deveria acompanhar mensalmente?",
                    "system_prompt": "Foque em m√©tricas pr√°ticas e acion√°veis."
                }
            ],
            "criteria": {
                "reasoning_steps": ["for√ßas", "fraquezas", "oportunidades", "amea√ßas"],
                "factual_accuracy": ["SWOT", "startup", "investimento"]
            }
        }
    ]

def gerar_relatorio_comparativo_completo(results: List[BenchmarkResult], models: Dict[str, ModelSpec]):
    """Gera relat√≥rio executivo completo de compara√ß√£o."""
    print("\n" + "="*80)
    print("üìä RELAT√ìRIO EXECUTIVO DE COMPARA√á√ÉO DE MODELOS")
    print("="*80)

    # An√°lise geral por modelo
    print("\nüèÜ RANKING GERAL DOS MODELOS")
    print("-"*50)

    model_summary = {}
    for model_name in models.keys():
        model_results = [r for r in results if r.model_name == model_name]
        if model_results:
            avg_score = statistics.mean([r.score for r in model_results])
            avg_time = statistics.mean([r.execution_time for r in model_results])
            total_cost = sum([r.cost for r in model_results])
            avg_consistency = statistics.mean([r.consistency for r in model_results])

            model_summary[model_name] = {
                "score": avg_score,
                "time": avg_time,
                "cost": total_cost,
                "consistency": avg_consistency
            }

    # Ranking por score geral
    ranked_models = sorted(model_summary.items(), key=lambda x: x[1]["score"], reverse=True)

    for i, (model_name, summary) in enumerate(ranked_models, 1):
        print(f"\n{i}. ü§ñ {model_name}")
        print(f"   Score Geral: {summary['score']:.3f}")
        print(f"   Tempo M√©dio: {summary['time']:.2f}s")
        print(f"   Custo Total: ${summary['cost']:.4f}")
        print(f"   Consist√™ncia: {summary['consistency']:.3f}")

        # Mostrar caracter√≠sticas do modelo
        spec = models[model_name]
        print(f"   Pontos Fortes: {', '.join(spec.strengths)}")
        print(f"   Casos de Uso: {', '.join(spec.use_cases[:2])}")

    # An√°lise por categoria
    print("\nüìä PERFORMANCE POR CATEGORIA")
    print("-"*50)

    categories = {}
    for result in results:
        if result.category not in categories:
            categories[result.category] = {}
        if result.model_name not in categories[result.category]:
            categories[result.category][result.model_name] = []
        categories[result.category][result.model_name].append(result.score)

    for category, cat_models in categories.items():
        print(f"\nüìÅ {category.upper()}:")
        cat_ranking = []
        for model_name, scores in cat_models.items():
            avg_score = statistics.mean(scores)
            cat_ranking.append((model_name, avg_score))

        cat_ranking.sort(key=lambda x: x[1], reverse=True)

        for i, (model_name, score) in enumerate(cat_ranking, 1):
            medal = "ü•á" if i == 1 else "ü•à" if i == 2 else "ü•â" if i == 3 else "  "
            print(f"   {medal} {model_name}: {score:.3f}")

    # An√°lise de custo-benef√≠cio
    print("\nüí∞ AN√ÅLISE DE CUSTO-BENEF√çCIO")
    print("-"*50)

    for model_name, summary in model_summary.items():
        efficiency = summary["score"] / (summary["cost"] + 0.001)  # Evitar divis√£o por zero
        print(f"{model_name}: Score/Custo = {efficiency:.1f}")

    # Recomenda√ß√µes estrat√©gicas
    print("\nüéØ RECOMENDA√á√ïES ESTRAT√âGICAS")
    print("-"*50)

    best_overall = ranked_models[0][0]
    best_value = max(model_summary.items(), key=lambda x: x[1]["score"] / (x[1]["cost"] + 0.001))[0]
    fastest = min(model_summary.items(), key=lambda x: x[1]["time"])[0]
    most_consistent = max(model_summary.items(), key=lambda x: x[1]["consistency"])[0]

    print(f"\nüèÜ MELHOR GERAL: {best_overall}")
    print(f"   Use para: Casos cr√≠ticos, an√°lises complexas")

    print(f"\nüíé MELHOR CUSTO-BENEF√çCIO: {best_value}")
    print(f"   Use para: Volume alto, uso geral")

    print(f"\n‚ö° MAIS R√ÅPIDO: {fastest}")
    print(f"   Use para: Aplica√ß√µes time-sensitive")

    print(f"\nüéØ MAIS CONSISTENTE: {most_consistent}")
    print(f"   Use para: Processos cr√≠ticos, compliance")

    # Matrix de decis√£o
    print(f"\nüìã MATRIX DE DECIS√ÉO POR CASO DE USO")
    print("-"*50)

    use_cases = {
        "Customer Support (Alto Volume)": best_value,
        "An√°lises Estrat√©gicas": best_overall,
        "Automa√ß√£o Simples": fastest,
        "Processos Cr√≠ticos": most_consistent
    }

    for use_case, recommended_model in use_cases.items():
        print(f"‚úÖ {use_case}: {recommended_model}")

def demo_comparacao_modelos():
    """
    Demonstra compara√ß√£o abrangente de modelos de IA.
    """
    logger = get_logger("comparacao_modelos")

    try:
        print("üî¨ SISTEMA DE COMPARA√á√ÉO DE MODELOS DE IA")
        print("="*45)

        # Configurar benchmark
        benchmark = ModelBenchmark()

        # Registrar modelos
        print("\nü§ñ Registrando modelos para compara√ß√£o...")
        model_specs = criar_modelos_comparacao()

        for spec in model_specs:
            benchmark.register_model(spec)
            print(f"  ‚úÖ {spec.name} ({spec.provider})")

        # Configurar benchmarks
        print("\nüìä Configurando benchmarks...")
        benchmarks_config = criar_benchmarks_abrangentes()

        for bench_config in benchmarks_config:
            benchmark.add_benchmark(
                bench_config["name"],
                bench_config["category"],
                bench_config["test_cases"],
                bench_config["criteria"]
            )
            print(f"  ‚úÖ {bench_config['name']} ({len(bench_config['test_cases'])} testes)")

        # Executar benchmark completo
        print(f"\nüöÄ Executando benchmark completo...")
        print("   (Isso pode levar alguns minutos...)")

        start_time = time.time()
        results = benchmark.run_comprehensive_benchmark(logger)
        total_time = time.time() - start_time

        print(f"\n‚è±Ô∏è Benchmark conclu√≠do em {total_time:.1f}s")
        print(f"üìä {len(results)} avalia√ß√µes executadas")

        # Gerar relat√≥rio
        gerar_relatorio_comparativo_completo(results, benchmark.models)

        print(f"\n‚úÖ Compara√ß√£o de modelos conclu√≠da!")
        print(f"\nüí° Use estas m√©tricas para escolher o modelo ideal")
        print("   para cada caso de uso espec√≠fico da sua empresa.")

        logger.info(f"Compara√ß√£o de modelos conclu√≠da - {len(results)} avalia√ß√µes")
        return True

    except Exception as e:
        error_msg = f"Erro durante compara√ß√£o de modelos: {str(e)}"
        logger.error(error_msg)
        print(f"‚ùå {error_msg}")
        return False

def explicar_metodologia():
    """
    Explica a metodologia de compara√ß√£o.
    """
    print("""
üî¨ METODOLOGIA DE COMPARA√á√ÉO
============================

DIMENS√ïES AVALIADAS:
1. üß† Capacidade de Racioc√≠nio
   - L√≥gica sequencial
   - An√°lise quantitativa
   - Conclus√µes fundamentadas

2. üìö Conhecimento Factual
   - Precis√£o de informa√ß√µes
   - Cobertura de dom√≠nios
   - Atualidade dos dados

3. üé® Criatividade
   - Originalidade de ideias
   - Diversidade vocabular
   - Solu√ß√µes inovadoras

4. ‚ö° Performance Operacional
   - Velocidade de resposta
   - Consist√™ncia
   - Efici√™ncia de tokens

5. üí∞ Custo-Benef√≠cio
   - Custo por intera√ß√£o
   - Valor gerado
   - ROI efetivo

M√âTRICAS CALCULADAS:
- Score Absoluto (0-1)
- Ranking Relativo
- Consist√™ncia (varia√ß√£o)
- Efici√™ncia (score/custo)
- Especializa√ß√£o por categoria
""")

def mostrar_criterios_selecao():
    """
    Mostra crit√©rios para sele√ß√£o de modelos.
    """
    print("""
üéØ CRIT√âRIOS DE SELE√á√ÉO POR CONTEXTO
====================================

ALTO VOLUME / BAIXO CUSTO:
‚úÖ Priorizar: Custo por token
‚úÖ Aceitar: Qualidade moderada
‚úÖ Exemplo: Customer support L1

ALTA QUALIDADE / CUSTO SECUND√ÅRIO:
‚úÖ Priorizar: Score de qualidade
‚úÖ Aceitar: Custo mais alto
‚úÖ Exemplo: An√°lises estrat√©gicas

BAIXA LAT√äNCIA / TEMPO REAL:
‚úÖ Priorizar: Velocidade de resposta
‚úÖ Aceitar: Menor qualidade
‚úÖ Exemplo: Chatbots interativos

M√ÅXIMA CONSIST√äNCIA:
‚úÖ Priorizar: Baixa varia√ß√£o
‚úÖ Aceitar: Performance moderada
‚úÖ Exemplo: Processos regulat√≥rios

MODELO H√çBRIDO:
‚úÖ Usar diferentes modelos por contexto
‚úÖ Routing inteligente baseado em input
‚úÖ Otimiza√ß√£o cont√≠nua autom√°tica
""")

def configurar_ambiente():
    """
    Configura√ß√£o para compara√ß√£o de modelos.
    """
    print("""
üìã CONFIGURA√á√ÉO - COMPARA√á√ÉO DE MODELOS
=======================================

Depend√™ncias:
pip install matplotlib  # Para gr√°ficos
pip install pandas      # Para an√°lise
pip install numpy       # Para c√°lculos

Provedores necess√°rios:
- Pelo menos 2 modelos diferentes
- Credenciais configuradas
- Limites de API adequados

IMPORTANTE:
- Testes consomem tokens/cr√©ditos
- Execute em hor√°rios de baixo uso
- Salve resultados para an√°lise posterior
- Consider usar modelos locais para economia
""")

if __name__ == "__main__":
    configurar_ambiente()
    explicar_metodologia()
    mostrar_criterios_selecao()

    resposta = input("\nDeseja executar a compara√ß√£o de modelos? (s/n): ")
    if resposta.lower() in ['s', 'sim', 'y', 'yes']:
        demo_comparacao_modelos()
    else:
        print("Demo cancelado.")