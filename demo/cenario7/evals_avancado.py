"""
CenÃ¡rio 7: Evals AvanÃ§ado - ComparaÃ§Ã£o de Modelos e A/B Testing
Demonstra sistema sofisticado de avaliaÃ§Ã£o e otimizaÃ§Ã£o de IA.
"""

from agentCore import get_llm, get_logger
from agentCore.evaluation.model_comparison import ModelComparator
from agentCore.evaluation.metrics_collector import MetricsCollector
from langchain_core.messages import HumanMessage, SystemMessage
import json
import os
import time
import statistics
import random
from datetime import datetime, timedelta
from typing import List, Dict, Any, Tuple
from dataclasses import dataclass
from dotenv import load_dotenv

load_dotenv()

@dataclass
class ModelConfig:
    """ConfiguraÃ§Ã£o de modelo para comparaÃ§Ã£o."""
    name: str
    provider: str
    model: str
    temperature: float
    max_tokens: int
    description: str

@dataclass
class EvalResult:
    """Resultado de uma avaliaÃ§Ã£o."""
    model_name: str
    test_case_id: str
    input_text: str
    output_text: str
    scores: Dict[str, float]
    overall_score: float
    execution_time: float
    token_usage: int
    cost_estimate: float

class AdvancedEvaluator:
    """Sistema avanÃ§ado de avaliaÃ§Ã£o com mÃºltiplas mÃ©tricas."""

    def __init__(self):
        self.models = []
        self.test_cases = []
        self.results = []

    def add_model(self, config: ModelConfig):
        """Adiciona modelo para comparaÃ§Ã£o."""
        self.models.append(config)

    def add_test_case(self, id: str, input_text: str, category: str, expected_criteria: Dict):
        """Adiciona caso de teste."""
        test_case = {
            "id": id,
            "input": input_text,
            "category": category,
            "criteria": expected_criteria
        }
        self.test_cases.append(test_case)

    def evaluate_semantic_similarity(self, response: str, expected_keywords: List[str]) -> float:
        """Avalia similaridade semÃ¢ntica usando keywords."""
        response_lower = response.lower()
        matches = 0
        for keyword in expected_keywords:
            if keyword.lower() in response_lower:
                matches += 1
        return (matches / len(expected_keywords)) if expected_keywords else 0.0

    def evaluate_response_structure(self, response: str) -> float:
        """Avalia estrutura e organizaÃ§Ã£o da resposta."""
        score = 0.0

        # Verifica se tem estrutura organizada
        if any(char in response for char in [":", "-", "â€¢", "1.", "2."]):
            score += 0.3

        # Verifica se tem conclusÃ£o clara
        conclusion_words = ["portanto", "assim", "em resumo", "conclusÃ£o"]
        if any(word in response.lower() for word in conclusion_words):
            score += 0.2

        # Verifica tamanho adequado
        if 50 <= len(response) <= 500:
            score += 0.3
        elif len(response) > 500:
            score += 0.1

        # Verifica se evita repetiÃ§Ã£o
        words = response.lower().split()
        unique_words = set(words)
        if len(unique_words) / len(words) > 0.7:  # 70% palavras Ãºnicas
            score += 0.2

        return min(1.0, score)

    def evaluate_hallucination_detection(self, response: str, category: str) -> float:
        """Detecta possÃ­veis alucinaÃ§Ãµes baseado na categoria."""
        # Palavras que indicam incerteza (bom para evitar alucinaÃ§Ãµes)
        uncertainty_indicators = ["possivelmente", "talvez", "pode ser", "nÃ£o tenho certeza", "acredito que"]

        # Palavras que indicam alta confianÃ§a (risco de alucinaÃ§Ã£o se incorreto)
        high_confidence = ["certamente", "definitivamente", "sem dÃºvida", "100%", "sempre"]

        response_lower = response.lower()

        # Se categoria Ã© "fora_escopo", deve mostrar incerteza
        if category == "fora_escopo":
            if any(ind in response_lower for ind in uncertainty_indicators):
                return 1.0
            elif any(conf in response_lower for conf in high_confidence):
                return 0.1
            else:
                return 0.5

        # Para outras categorias, balance Ã© importante
        uncertainty_count = sum(1 for ind in uncertainty_indicators if ind in response_lower)
        confidence_count = sum(1 for conf in high_confidence if conf in response_lower)

        if uncertainty_count > 0 and confidence_count == 0:
            return 0.8  # Bom: mostra humildade
        elif uncertainty_count == 0 and confidence_count > 2:
            return 0.3  # Ruim: muito confiante, risco alucinaÃ§Ã£o
        else:
            return 0.6  # Neutro

    def calculate_comprehensive_score(self, response: str, test_case: Dict) -> Dict[str, float]:
        """Calcula score abrangente com mÃºltiplas mÃ©tricas."""
        scores = {}

        # MÃ©trica 1: RelevÃ¢ncia (similarity com keywords esperadas)
        expected_keywords = test_case["criteria"].get("keywords", [])
        scores["relevancia"] = self.evaluate_semantic_similarity(response, expected_keywords)

        # MÃ©trica 2: Estrutura e organizaÃ§Ã£o
        scores["estrutura"] = self.evaluate_response_structure(response)

        # MÃ©trica 3: DetecÃ§Ã£o de alucinaÃ§Ã£o
        scores["anti_alucinacao"] = self.evaluate_hallucination_detection(response, test_case["category"])

        # MÃ©trica 4: Completude baseada em critÃ©rios obrigatÃ³rios
        required_elements = test_case["criteria"].get("required_elements", [])
        if required_elements:
            response_lower = response.lower()
            found_elements = sum(1 for elem in required_elements if elem.lower() in response_lower)
            scores["completude"] = found_elements / len(required_elements)
        else:
            scores["completude"] = 0.7  # Neutro se nÃ£o hÃ¡ elementos obrigatÃ³rios

        # MÃ©trica 5: Tom e profissionalismo
        professional_tone = self.evaluate_professional_tone(response)
        scores["profissionalismo"] = professional_tone

        return scores

    def evaluate_professional_tone(self, response: str) -> float:
        """Avalia tom profissional da resposta."""
        response_lower = response.lower()

        # Palavras que prejudicam profissionalismo
        unprofessional = ["cara", "mano", "tipo assim", "nÃ©", "tÃ¡ ligado", "beleza"]
        unprofessional_count = sum(1 for word in unprofessional if word in response_lower)

        # Palavras que indicam profissionalismo
        professional = ["prezado", "senhor", "senhora", "cordialmente", "atenciosamente"]
        professional_count = sum(1 for word in professional if word in response_lower)

        if unprofessional_count > 0:
            return max(0.1, 0.8 - (unprofessional_count * 0.2))
        elif professional_count > 0:
            return min(1.0, 0.8 + (professional_count * 0.1))
        else:
            return 0.7  # Neutro

    def run_comprehensive_evaluation(self, logger) -> List[EvalResult]:
        """Executa avaliaÃ§Ã£o abrangente de todos os modelos."""
        all_results = []

        print(f"\nğŸ”„ Iniciando avaliaÃ§Ã£o de {len(self.models)} modelos com {len(self.test_cases)} casos")

        for model_config in self.models:
            print(f"\nğŸ“Š Avaliando modelo: {model_config.name}")

            try:
                # Configurar modelo
                llm = get_llm(
                    provider=model_config.provider,
                    model=model_config.model,
                    temperature=model_config.temperature,
                    max_tokens=model_config.max_tokens
                )

                model_results = []

                for test_case in self.test_cases:
                    start_time = time.time()

                    # Gerar resposta
                    messages = [
                        SystemMessage(content="""VocÃª Ã© um assistente empresarial especializado.
                        Responda de forma clara, precisa e profissional.
                        Se nÃ£o souber algo, admita claramente."""),
                        HumanMessage(content=test_case["input"])
                    ]

                    response = llm.invoke(messages)
                    execution_time = time.time() - start_time

                    # Calcular scores
                    scores = self.calculate_comprehensive_score(response.content, test_case)

                    # Score geral ponderado
                    weights = {
                        "relevancia": 0.25,
                        "estrutura": 0.20,
                        "anti_alucinacao": 0.20,
                        "completude": 0.25,
                        "profissionalismo": 0.10
                    }

                    overall_score = sum(scores[metric] * weights[metric] for metric in weights)

                    # Estimar custo (simulado)
                    estimated_tokens = len(response.content) // 4  # AproximaÃ§Ã£o
                    cost_estimate = estimated_tokens * 0.0001  # $0.0001 por token (exemplo)

                    # Criar resultado
                    result = EvalResult(
                        model_name=model_config.name,
                        test_case_id=test_case["id"],
                        input_text=test_case["input"],
                        output_text=response.content,
                        scores=scores,
                        overall_score=overall_score,
                        execution_time=execution_time,
                        token_usage=estimated_tokens,
                        cost_estimate=cost_estimate
                    )

                    model_results.append(result)
                    all_results.append(result)

                # Resumo do modelo
                avg_score = statistics.mean([r.overall_score for r in model_results])
                avg_time = statistics.mean([r.execution_time for r in model_results])
                total_cost = sum([r.cost_estimate for r in model_results])

                print(f"  âœ… Score mÃ©dio: {avg_score:.3f}")
                print(f"  â±ï¸ Tempo mÃ©dio: {avg_time:.2f}s")
                print(f"  ğŸ’° Custo estimado: ${total_cost:.4f}")

            except Exception as e:
                print(f"  âŒ Erro ao avaliar {model_config.name}: {str(e)}")
                logger.error(f"Erro na avaliaÃ§Ã£o do modelo {model_config.name}: {str(e)}")

        return all_results

def criar_configuracoes_modelos() -> List[ModelConfig]:
    """Cria diferentes configuraÃ§Ãµes de modelos para comparaÃ§Ã£o."""
    return [
        ModelConfig(
            name="Ollama_Conservative",
            provider="ollama",
            model="llama3:latest",
            temperature=0.1,
            max_tokens=150,
            description="ConfiguraÃ§Ã£o conservadora para mÃ¡xima consistÃªncia"
        ),
        ModelConfig(
            name="Ollama_Balanced",
            provider="ollama",
            model="llama3:latest",
            temperature=0.3,
            max_tokens=200,
            description="ConfiguraÃ§Ã£o balanceada para uso geral"
        ),
        ModelConfig(
            name="Ollama_Creative",
            provider="ollama",
            model="llama3:latest",
            temperature=0.7,
            max_tokens=250,
            description="ConfiguraÃ§Ã£o mais criativa para respostas elaboradas"
        )
    ]

def criar_dataset_avancado():
    """Cria dataset avanÃ§ado para avaliaÃ§Ã£o abrangente."""
    test_cases = [
        {
            "id": "rh_001",
            "input": "Quais sÃ£o os benefÃ­cios oferecidos pela empresa?",
            "category": "recursos_humanos",
            "criteria": {
                "keywords": ["vale refeiÃ§Ã£o", "plano de saÃºde", "vale alimentaÃ§Ã£o"],
                "required_elements": ["vale", "plano", "benefÃ­cios"],
                "max_length": 300
            }
        },
        {
            "id": "rh_002",
            "input": "Como funciona a polÃ­tica de fÃ©rias?",
            "category": "recursos_humanos",
            "criteria": {
                "keywords": ["30 dias", "fÃ©rias", "perÃ­odo aquisitivo"],
                "required_elements": ["dias", "fÃ©rias"],
                "should_avoid": ["nÃ£o sei", "nÃ£o tenho informaÃ§Ã£o"]
            }
        },
        {
            "id": "com_001",
            "input": "Qual Ã© o processo de vendas da empresa?",
            "category": "comercial",
            "criteria": {
                "keywords": ["qualificaÃ§Ã£o", "proposta", "apresentaÃ§Ã£o", "etapas"],
                "required_elements": ["processo", "etapas"],
                "should_include_structure": True
            }
        },
        {
            "id": "tec_001",
            "input": "Quais sÃ£o os nÃ­veis de SLA do suporte tÃ©cnico?",
            "category": "suporte_tecnico",
            "criteria": {
                "keywords": ["P1", "P2", "P3", "P4", "tempo", "resposta"],
                "required_elements": ["P1", "resposta", "resoluÃ§Ã£o"],
                "precision_required": True
            }
        },
        {
            "id": "edge_001",
            "input": "Qual Ã© a temperatura em Marte hoje?",
            "category": "fora_escopo",
            "criteria": {
                "keywords": ["nÃ£o sei", "nÃ£o tenho", "fora do escopo", "especializado"],
                "required_elements": ["nÃ£o"],
                "should_show_limitation": True
            }
        },
        {
            "id": "edge_002",
            "input": "Me conte uma piada sobre programaÃ§Ã£o",
            "category": "fora_escopo",
            "criteria": {
                "keywords": ["nÃ£o posso", "especializado em", "empresa"],
                "should_redirect": True
            }
        }
    ]

    return test_cases

def gerar_relatorio_comparativo(results: List[EvalResult], logger):
    """Gera relatÃ³rio comparativo abrangente."""
    print("\n" + "="*80)
    print("ğŸ“Š RELATÃ“RIO COMPARATIVO AVANÃ‡ADO")
    print("="*80)

    # Agrupar por modelo
    models = {}
    for result in results:
        if result.model_name not in models:
            models[result.model_name] = []
        models[result.model_name].append(result)

    # AnÃ¡lise geral por modelo
    print("\nğŸ“ˆ PERFORMANCE GERAL POR MODELO:")
    print("-"*50)

    for model_name, model_results in models.items():
        if not model_results:
            continue

        scores = [r.overall_score for r in model_results]
        times = [r.execution_time for r in model_results]
        costs = [r.cost_estimate for r in model_results]

        avg_score = statistics.mean(scores)
        std_score = statistics.stdev(scores) if len(scores) > 1 else 0
        avg_time = statistics.mean(times)
        total_cost = sum(costs)

        print(f"\nğŸ¤– {model_name}:")
        print(f"  Score MÃ©dio: {avg_score:.3f} Â± {std_score:.3f}")
        print(f"  Tempo MÃ©dio: {avg_time:.2f}s")
        print(f"  Custo Total: ${total_cost:.4f}")
        print(f"  ConsistÃªncia: {'Alta' if std_score < 0.1 else 'MÃ©dia' if std_score < 0.2 else 'Baixa'}")

    # AnÃ¡lise por categoria
    print("\nğŸ“Š PERFORMANCE POR CATEGORIA:")
    print("-"*50)

    categories = {}
    for result in results:
        # Encontrar categoria do test case
        test_case = next((tc for tc in criar_dataset_avancado() if tc["id"] == result.test_case_id), None)
        if test_case:
            cat = test_case["category"]
            if cat not in categories:
                categories[cat] = {}
            if result.model_name not in categories[cat]:
                categories[cat][result.model_name] = []
            categories[cat][result.model_name].append(result.overall_score)

    for category, cat_models in categories.items():
        print(f"\nğŸ“ {category.upper()}:")
        for model_name, scores in cat_models.items():
            avg_score = statistics.mean(scores)
            print(f"  {model_name}: {avg_score:.3f}")

    # AnÃ¡lise por mÃ©trica
    print("\nğŸ¯ ANÃLISE DETALHADA POR MÃ‰TRICA:")
    print("-"*50)

    metrics = ["relevancia", "estrutura", "anti_alucinacao", "completude", "profissionalismo"]

    for metric in metrics:
        print(f"\nğŸ“Š {metric.upper()}:")
        metric_scores = {}

        for result in results:
            if result.model_name not in metric_scores:
                metric_scores[result.model_name] = []
            if metric in result.scores:
                metric_scores[result.model_name].append(result.scores[metric])

        for model_name, scores in metric_scores.items():
            if scores:
                avg_score = statistics.mean(scores)
                print(f"  {model_name}: {avg_score:.3f}")

    # RecomendaÃ§Ã£o final
    print("\nğŸ¯ RECOMENDAÃ‡ÃƒO ESTRATÃ‰GICA:")
    print("-"*50)

    # Encontrar melhor modelo geral
    model_performance = {}
    for model_name, model_results in models.items():
        if model_results:
            scores = [r.overall_score for r in model_results]
            times = [r.execution_time for r in model_results]
            costs = [r.cost_estimate for r in model_results]

            avg_score = statistics.mean(scores)
            avg_time = statistics.mean(times)
            total_cost = sum(costs)

            # Score composto (considera qualidade, velocidade e custo)
            composite_score = (avg_score * 0.6) + ((1/avg_time) * 0.2) + ((1/(total_cost+0.001)) * 0.2)
            model_performance[model_name] = {
                "quality": avg_score,
                "speed": avg_time,
                "cost": total_cost,
                "composite": composite_score
            }

    best_overall = max(model_performance.items(), key=lambda x: x[1]["composite"])
    best_quality = max(model_performance.items(), key=lambda x: x[1]["quality"])
    fastest = min(model_performance.items(), key=lambda x: x[1]["speed"])
    cheapest = min(model_performance.items(), key=lambda x: x[1]["cost"])

    print(f"ğŸ† MELHOR GERAL: {best_overall[0]}")
    print(f"   (Score composto: {best_overall[1]['composite']:.3f})")

    print(f"\nğŸ–ï¸ MELHOR QUALIDADE: {best_quality[0]}")
    print(f"   (Score: {best_quality[1]['quality']:.3f})")

    print(f"\nâš¡ MAIS RÃPIDO: {fastest[0]}")
    print(f"   (Tempo: {fastest[1]['speed']:.2f}s)")

    print(f"\nğŸ’° MAIS ECONÃ”MICO: {cheapest[0]}")
    print(f"   (Custo: ${cheapest[1]['cost']:.4f})")

    print(f"\nğŸ’¡ RECOMENDAÃ‡ÃƒO DE USO:")
    if best_overall[1]["quality"] > 0.8:
        print("âœ… Sistema pronto para produÃ§Ã£o")
        print(f"   Usar {best_overall[0]} para uso geral")
        print(f"   Usar {best_quality[0]} para casos crÃ­ticos")
    elif best_overall[1]["quality"] > 0.6:
        print("âš ï¸ Sistema precisa de ajustes antes da produÃ§Ã£o")
        print("   Focar em melhorar prompts e datasets")
    else:
        print("âŒ Sistema nÃ£o estÃ¡ pronto para produÃ§Ã£o")
        print("   RevisÃ£o completa de prompts e modelos necessÃ¡ria")

def demo_evals_avancado():
    """
    Demonstra sistema avanÃ§ado de avaliaÃ§Ã£o e comparaÃ§Ã£o.
    """
    logger = get_logger("evals_avancado")

    try:
        print("ğŸ§ª SISTEMA DE AVALIAÃ‡ÃƒO AVANÃ‡ADO")
        print("="*40)

        # Criar avaliador
        evaluator = AdvancedEvaluator()

        # Configurar modelos para comparaÃ§Ã£o
        print("\nğŸ¤– Configurando modelos para comparaÃ§Ã£o...")
        model_configs = criar_configuracoes_modelos()

        for config in model_configs:
            evaluator.add_model(config)
            print(f"  âœ… {config.name} ({config.description})")

        # Carregar dataset avanÃ§ado
        print("\nğŸ“Š Carregando dataset avanÃ§ado...")
        test_cases = criar_dataset_avancado()

        for test_case in test_cases:
            evaluator.add_test_case(
                test_case["id"],
                test_case["input"],
                test_case["category"],
                test_case["criteria"]
            )

        print(f"  âœ… {len(test_cases)} casos de teste carregados")

        # Executar avaliaÃ§Ã£o abrangente
        print(f"\nğŸš€ Executando avaliaÃ§Ã£o comparativa...")
        start_time = time.time()

        results = evaluator.run_comprehensive_evaluation(logger)

        total_time = time.time() - start_time

        print(f"\nâ±ï¸ AvaliaÃ§Ã£o concluÃ­da em {total_time:.2f}s")
        print(f"ğŸ“Š {len(results)} avaliaÃ§Ãµes executadas")

        # Gerar relatÃ³rio comparativo
        gerar_relatorio_comparativo(results, logger)

        print(f"\nâœ… Evals AvanÃ§ado concluÃ­do!")
        print(f"\nğŸ’¡ PRÃ“XIMO PASSO: Use essas mÃ©tricas para escolher")
        print("   a configuraÃ§Ã£o ideal para seu caso de uso especÃ­fico.")

        logger.info(f"Evals avanÃ§ado concluÃ­do - {len(results)} avaliaÃ§Ãµes executadas")
        return True

    except Exception as e:
        error_msg = f"Erro durante execuÃ§Ã£o dos Evals AvanÃ§ados: {str(e)}"
        logger.error(error_msg)
        print(f"âŒ {error_msg}")
        return False

def explicar_diferencas_evals():
    """
    Explica diferenÃ§as entre Evals bÃ¡sico e avanÃ§ado.
    """
    print("""
ğŸ” COMPARAÃ‡ÃƒO: EVALS BÃSICO vs AVANÃ‡ADO
=======================================

EVALS BÃSICO (CenÃ¡rio 6):
âœ… ValidaÃ§Ã£o simples de qualidade
âœ… MÃ©tricas bÃ¡sicas (relevÃ¢ncia, precisÃ£o)
âœ… Dataset pequeno e focado
âœ… RelatÃ³rios simples

EVALS AVANÃ‡ADO (CenÃ¡rio 7):
ğŸš€ ComparaÃ§Ã£o mÃºltiplos modelos/configuraÃ§Ãµes
ğŸš€ MÃ©tricas sofisticadas (anti-alucinaÃ§Ã£o, estrutura)
ğŸš€ A/B testing automatizado
ğŸš€ AnÃ¡lise estatÃ­stica robusta
ğŸš€ RecomendaÃ§Ãµes estratÃ©gicas
ğŸš€ ConsideraÃ§Ã£o de custo-benefÃ­cio

QUANDO USAR CADA UM:
- BÃ¡sico: ValidaÃ§Ã£o inicial, sistemas simples
- AvanÃ§ado: OtimizaÃ§Ã£o, sistemas crÃ­ticos, produÃ§Ã£o
""")

def mostrar_metricas_avancadas():
    """
    Explica mÃ©tricas avanÃ§adas utilizadas.
    """
    print("""
ğŸ¯ MÃ‰TRICAS AVANÃ‡ADAS EXPLICADAS
================================

1. ANTI-ALUCINAÃ‡ÃƒO:
   - Detecta confianÃ§a excessiva
   - Valida admissÃ£o de limitaÃ§Ãµes
   - Previne informaÃ§Ãµes incorretas

2. ESTRUTURA E ORGANIZAÃ‡ÃƒO:
   - Lista estruturada vs texto corrido
   - ConclusÃµes claras
   - Tamanho adequado da resposta

3. ANÃLISE ESTATÃSTICA:
   - MÃ©dia Â± desvio padrÃ£o
   - ConsistÃªncia entre execuÃ§Ãµes
   - Intervalos de confianÃ§a

4. COST-BENEFIT ANALYSIS:
   - Tokens utilizados
   - Tempo de execuÃ§Ã£o
   - Custo estimado por interaÃ§Ã£o

5. COMPOSITE SCORING:
   - Combina qualidade + velocidade + custo
   - PonderaÃ§Ã£o customizÃ¡vel
   - Ranking multi-critÃ©rio
""")

def configurar_ambiente():
    """
    ConfiguraÃ§Ã£o para Evals AvanÃ§ado.
    """
    print("""
ğŸ“‹ CONFIGURAÃ‡ÃƒO - EVALS AVANÃ‡ADO
================================

DependÃªncias adicionais:
pip install scipy  # Para anÃ¡lise estatÃ­stica
pip install matplotlib  # Para grÃ¡ficos (opcional)

Recursos necessÃ¡rios:
- MÃºltiplos modelos/provedores configurados
- Dataset abrangente e representativo
- Tempo para execuÃ§Ã£o completa (~10-30min)

IMPORTANTE:
- Execute em ambiente isolado para evitar interferÃªncias
- Use seeds fixos para reproducibilidade
- Monitore recursos durante execuÃ§Ã£o
- Salve resultados para anÃ¡lise posterior
""")

if __name__ == "__main__":
    configurar_ambiente()
    explicar_diferencas_evals()
    mostrar_metricas_avancadas()

    resposta = input("\nDeseja executar o demo de Evals AvanÃ§ado? (s/n): ")
    if resposta.lower() in ['s', 'sim', 'y', 'yes']:
        demo_evals_avancado()
    else:
        print("Demo cancelado.")