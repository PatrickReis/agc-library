"""
Cen√°rio 7: Evals Avan√ßado - Compara√ß√£o de Modelos e A/B Testing
Demonstra sistema sofisticado de avalia√ß√£o e otimiza√ß√£o de IA.
"""

from agentCore import get_llm, get_logger
from agentCore.evaluation.model_comparison import ModelComparator
from agentCore.evaluation.metrics_collector import MetricsCollector
from langchain_core.messages import HumanMessage, SystemMessage
import json
import os
import time
import statistics
import random
from datetime import datetime, timedelta
from typing import List, Dict, Any, Tuple
from dataclasses import dataclass
from dotenv import load_dotenv

load_dotenv()

@dataclass
class ModelConfig:
    """Configura√ß√£o de modelo para compara√ß√£o."""
    name: str
    provider: str
    model: str
    temperature: float
    max_tokens: int
    description: str

@dataclass
class EvalResult:
    """Resultado de uma avalia√ß√£o."""
    model_name: str
    test_case_id: str
    input_text: str
    output_text: str
    scores: Dict[str, float]
    overall_score: float
    execution_time: float
    token_usage: int
    cost_estimate: float

class AdvancedEvaluator:
    """Sistema avan√ßado de avalia√ß√£o com m√∫ltiplas m√©tricas."""

    def __init__(self):
        self.models = []
        self.test_cases = []
        self.results = []

    def add_model(self, config: ModelConfig):
        """Adiciona modelo para compara√ß√£o."""
        self.models.append(config)

    def add_test_case(self, id: str, input_text: str, category: str, expected_criteria: Dict):
        """Adiciona caso de teste."""
        test_case = {
            "id": id,
            "input": input_text,
            "category": category,
            "criteria": expected_criteria
        }
        self.test_cases.append(test_case)

    def evaluate_semantic_similarity(self, response: str, expected_keywords: List[str]) -> float:
        """Avalia similaridade sem√¢ntica usando keywords."""
        response_lower = response.lower()
        matches = 0
        for keyword in expected_keywords:
            if keyword.lower() in response_lower:
                matches += 1
        return (matches / len(expected_keywords)) if expected_keywords else 0.0

    def evaluate_response_structure(self, response: str) -> float:
        """Avalia estrutura e organiza√ß√£o da resposta."""
        score = 0.0

        # Verifica se tem estrutura organizada
        if any(char in response for char in [":", "-", "‚Ä¢", "1.", "2."]):
            score += 0.3

        # Verifica se tem conclus√£o clara
        conclusion_words = ["portanto", "assim", "em resumo", "conclus√£o"]
        if any(word in response.lower() for word in conclusion_words):
            score += 0.2

        # Verifica tamanho adequado
        if 50 <= len(response) <= 500:
            score += 0.3
        elif len(response) > 500:
            score += 0.1

        # Verifica se evita repeti√ß√£o
        words = response.lower().split()
        unique_words = set(words)
        if len(unique_words) / len(words) > 0.7:  # 70% palavras √∫nicas
            score += 0.2

        return min(1.0, score)

    def evaluate_hallucination_detection(self, response: str, category: str) -> float:
        """Detecta poss√≠veis alucina√ß√µes baseado na categoria."""
        # Palavras que indicam incerteza (bom para evitar alucina√ß√µes)
        uncertainty_indicators = ["possivelmente", "talvez", "pode ser", "n√£o tenho certeza", "acredito que"]

        # Palavras que indicam alta confian√ßa (risco de alucina√ß√£o se incorreto)
        high_confidence = ["certamente", "definitivamente", "sem d√∫vida", "100%", "sempre"]

        response_lower = response.lower()

        # Se categoria √© "fora_escopo", deve mostrar incerteza
        if category == "fora_escopo":
            if any(ind in response_lower for ind in uncertainty_indicators):
                return 1.0
            elif any(conf in response_lower for conf in high_confidence):
                return 0.1
            else:
                return 0.5

        # Para outras categorias, balance √© importante
        uncertainty_count = sum(1 for ind in uncertainty_indicators if ind in response_lower)
        confidence_count = sum(1 for conf in high_confidence if conf in response_lower)

        if uncertainty_count > 0 and confidence_count == 0:
            return 0.8  # Bom: mostra humildade
        elif uncertainty_count == 0 and confidence_count > 2:
            return 0.3  # Ruim: muito confiante, risco alucina√ß√£o
        else:
            return 0.6  # Neutro

    def calculate_comprehensive_score(self, response: str, test_case: Dict) -> Dict[str, float]:
        """Calcula score abrangente com m√∫ltiplas m√©tricas."""
        scores = {}

        # M√©trica 1: Relev√¢ncia (similarity com keywords esperadas)
        expected_keywords = test_case["criteria"].get("keywords", [])
        scores["relevancia"] = self.evaluate_semantic_similarity(response, expected_keywords)

        # M√©trica 2: Estrutura e organiza√ß√£o
        scores["estrutura"] = self.evaluate_response_structure(response)

        # M√©trica 3: Detec√ß√£o de alucina√ß√£o
        scores["anti_alucinacao"] = self.evaluate_hallucination_detection(response, test_case["category"])

        # M√©trica 4: Completude baseada em crit√©rios obrigat√≥rios
        required_elements = test_case["criteria"].get("required_elements", [])
        if required_elements:
            response_lower = response.lower()
            found_elements = sum(1 for elem in required_elements if elem.lower() in response_lower)
            scores["completude"] = found_elements / len(required_elements)
        else:
            scores["completude"] = 0.7  # Neutro se n√£o h√° elementos obrigat√≥rios

        # M√©trica 5: Tom e profissionalismo
        professional_tone = self.evaluate_professional_tone(response)
        scores["profissionalismo"] = professional_tone

        return scores

    def evaluate_professional_tone(self, response: str) -> float:
        """Avalia tom profissional da resposta."""
        response_lower = response.lower()

        # Palavras que prejudicam profissionalismo
        unprofessional = ["cara", "mano", "tipo assim", "n√©", "t√° ligado", "beleza"]
        unprofessional_count = sum(1 for word in unprofessional if word in response_lower)

        # Palavras que indicam profissionalismo
        professional = ["prezado", "senhor", "senhora", "cordialmente", "atenciosamente"]
        professional_count = sum(1 for word in professional if word in response_lower)

        if unprofessional_count > 0:
            return max(0.1, 0.8 - (unprofessional_count * 0.2))
        elif professional_count > 0:
            return min(1.0, 0.8 + (professional_count * 0.1))
        else:
            return 0.7  # Neutro

    def run_comprehensive_evaluation(self, logger) -> List[EvalResult]:
        """Executa avalia√ß√£o abrangente de todos os modelos."""
        all_results = []

        print(f"\nüîÑ Iniciando avalia√ß√£o de {len(self.models)} modelos com {len(self.test_cases)} casos")

        for model_config in self.models:
            print(f"\nüìä Avaliando modelo: {model_config.name}")

            try:
                # Configurar modelo
                llm = get_llm(
                    provider=model_config.provider,
                    model=model_config.model,
                    temperature=model_config.temperature,
                    max_tokens=model_config.max_tokens
                )

                model_results = []

                for test_case in self.test_cases:
                    start_time = time.time()

                    # Gerar resposta
                    messages = [
                        SystemMessage(content="""Voc√™ √© um assistente empresarial especializado.
                        Responda de forma clara, precisa e profissional.
                        Se n√£o souber algo, admita claramente."""),
                        HumanMessage(content=test_case["input"])
                    ]

                    response = llm.invoke(messages)
                    execution_time = time.time() - start_time

                    # Calcular scores
                    scores = self.calculate_comprehensive_score(response.content, test_case)

                    # Score geral ponderado
                    weights = {
                        "relevancia": 0.25,
                        "estrutura": 0.20,
                        "anti_alucinacao": 0.20,
                        "completude": 0.25,
                        "profissionalismo": 0.10
                    }

                    overall_score = sum(scores[metric] * weights[metric] for metric in weights)

                    # Estimar custo (simulado)
                    estimated_tokens = len(response.content) // 4  # Aproxima√ß√£o
                    cost_estimate = estimated_tokens * 0.0001  # $0.0001 por token (exemplo)

                    # Criar resultado
                    result = EvalResult(
                        model_name=model_config.name,
                        test_case_id=test_case["id"],
                        input_text=test_case["input"],
                        output_text=response.content,
                        scores=scores,
                        overall_score=overall_score,
                        execution_time=execution_time,
                        token_usage=estimated_tokens,
                        cost_estimate=cost_estimate
                    )

                    model_results.append(result)
                    all_results.append(result)

                # Resumo do modelo
                avg_score = statistics.mean([r.overall_score for r in model_results])
                avg_time = statistics.mean([r.execution_time for r in model_results])
                total_cost = sum([r.cost_estimate for r in model_results])

                print(f"  ‚úÖ Score m√©dio: {avg_score:.3f}")
                print(f"  ‚è±Ô∏è Tempo m√©dio: {avg_time:.2f}s")
                print(f"  üí∞ Custo estimado: ${total_cost:.4f}")

            except Exception as e:
                print(f"  ‚ùå Erro ao avaliar {model_config.name}: {str(e)}")
                logger.error(f"Erro na avalia√ß√£o do modelo {model_config.name}: {str(e)}")

        return all_results

def criar_configuracoes_modelos() -> List[ModelConfig]:
    """Cria diferentes configura√ß√µes de modelos para compara√ß√£o."""
    return [
        ModelConfig(
            name="Ollama_Conservative",
            provider="ollama",
            model="llama3:latest",
            temperature=0.1,
            max_tokens=150,
            description="Configura√ß√£o conservadora para m√°xima consist√™ncia"
        ),
        ModelConfig(
            name="Ollama_Balanced",
            provider="ollama",
            model="llama3:latest",
            temperature=0.3,
            max_tokens=200,
            description="Configura√ß√£o balanceada para uso geral"
        ),
        ModelConfig(
            name="Ollama_Creative",
            provider="ollama",
            model="llama3:latest",
            temperature=0.7,
            max_tokens=250,
            description="Configura√ß√£o mais criativa para respostas elaboradas"
        )
    ]

def criar_dataset_avancado():
    """Cria dataset avan√ßado para avalia√ß√£o abrangente."""
    test_cases = [
        {
            "id": "rh_001",
            "input": "Quais s√£o os benef√≠cios oferecidos pela empresa?",
            "category": "recursos_humanos",
            "criteria": {
                "keywords": ["vale refei√ß√£o", "plano de sa√∫de", "vale alimenta√ß√£o"],
                "required_elements": ["vale", "plano", "benef√≠cios"],
                "max_length": 300
            }
        },
        {
            "id": "rh_002",
            "input": "Como funciona a pol√≠tica de f√©rias?",
            "category": "recursos_humanos",
            "criteria": {
                "keywords": ["30 dias", "f√©rias", "per√≠odo aquisitivo"],
                "required_elements": ["dias", "f√©rias"],
                "should_avoid": ["n√£o sei", "n√£o tenho informa√ß√£o"]
            }
        },
        {
            "id": "com_001",
            "input": "Qual √© o processo de vendas da empresa?",
            "category": "comercial",
            "criteria": {
                "keywords": ["qualifica√ß√£o", "proposta", "apresenta√ß√£o", "etapas"],
                "required_elements": ["processo", "etapas"],
                "should_include_structure": True
            }
        },
        {
            "id": "tec_001",
            "input": "Quais s√£o os n√≠veis de SLA do suporte t√©cnico?",
            "category": "suporte_tecnico",
            "criteria": {
                "keywords": ["P1", "P2", "P3", "P4", "tempo", "resposta"],
                "required_elements": ["P1", "resposta", "resolu√ß√£o"],
                "precision_required": True
            }
        },
        {
            "id": "edge_001",
            "input": "Qual √© a temperatura em Marte hoje?",
            "category": "fora_escopo",
            "criteria": {
                "keywords": ["n√£o sei", "n√£o tenho", "fora do escopo", "especializado"],
                "required_elements": ["n√£o"],
                "should_show_limitation": True
            }
        },
        {
            "id": "edge_002",
            "input": "Me conte uma piada sobre programa√ß√£o",
            "category": "fora_escopo",
            "criteria": {
                "keywords": ["n√£o posso", "especializado em", "empresa"],
                "should_redirect": True
            }
        }
    ]

    return test_cases

def gerar_relatorio_comparativo(results: List[EvalResult], logger):
    """Gera relat√≥rio comparativo abrangente."""
    print("\n" + "="*80)
    print("üìä RELAT√ìRIO COMPARATIVO AVAN√áADO")
    print("="*80)

    # Agrupar por modelo
    models = {}
    for result in results:
        if result.model_name not in models:
            models[result.model_name] = []
        models[result.model_name].append(result)

    # An√°lise geral por modelo
    print("\nüìà PERFORMANCE GERAL POR MODELO:")
    print("-"*50)

    for model_name, model_results in models.items():
        if not model_results:
            continue

        scores = [r.overall_score for r in model_results]
        times = [r.execution_time for r in model_results]
        costs = [r.cost_estimate for r in model_results]

        avg_score = statistics.mean(scores)
        std_score = statistics.stdev(scores) if len(scores) > 1 else 0
        avg_time = statistics.mean(times)
        total_cost = sum(costs)

        print(f"\nü§ñ {model_name}:")
        print(f"  Score M√©dio: {avg_score:.3f} ¬± {std_score:.3f}")
        print(f"  Tempo M√©dio: {avg_time:.2f}s")
        print(f"  Custo Total: ${total_cost:.4f}")
        print(f"  Consist√™ncia: {'Alta' if std_score < 0.1 else 'M√©dia' if std_score < 0.2 else 'Baixa'}")

    # An√°lise por categoria
    print("\nüìä PERFORMANCE POR CATEGORIA:")
    print("-"*50)

    categories = {}
    for result in results:
        # Encontrar categoria do test case
        test_case = next((tc for tc in criar_dataset_avancado() if tc["id"] == result.test_case_id), None)
        if test_case:
            cat = test_case["category"]
            if cat not in categories:
                categories[cat] = {}
            if result.model_name not in categories[cat]:
                categories[cat][result.model_name] = []
            categories[cat][result.model_name].append(result.overall_score)

    for category, cat_models in categories.items():
        print(f"\nüìÅ {category.upper()}:")
        for model_name, scores in cat_models.items():
            avg_score = statistics.mean(scores)
            print(f"  {model_name}: {avg_score:.3f}")

    # An√°lise por m√©trica
    print("\nüéØ AN√ÅLISE DETALHADA POR M√âTRICA:")
    print("-"*50)

    metrics = ["relevancia", "estrutura", "anti_alucinacao", "completude", "profissionalismo"]

    for metric in metrics:
        print(f"\nüìä {metric.upper()}:")
        metric_scores = {}

        for result in results:
            if result.model_name not in metric_scores:
                metric_scores[result.model_name] = []
            if metric in result.scores:
                metric_scores[result.model_name].append(result.scores[metric])

        for model_name, scores in metric_scores.items():
            if scores:
                avg_score = statistics.mean(scores)
                print(f"  {model_name}: {avg_score:.3f}")

    # Recomenda√ß√£o final
    print("\nüéØ RECOMENDA√á√ÉO ESTRAT√âGICA:")
    print("-"*50)

    # Encontrar melhor modelo geral
    model_performance = {}
    for model_name, model_results in models.items():
        if model_results:
            scores = [r.overall_score for r in model_results]
            times = [r.execution_time for r in model_results]
            costs = [r.cost_estimate for r in model_results]

            avg_score = statistics.mean(scores)
            avg_time = statistics.mean(times)
            total_cost = sum(costs)

            # Score composto (considera qualidade, velocidade e custo)
            composite_score = (avg_score * 0.6) + ((1/avg_time) * 0.2) + ((1/(total_cost+0.001)) * 0.2)
            model_performance[model_name] = {
                "quality": avg_score,
                "speed": avg_time,
                "cost": total_cost,
                "composite": composite_score
            }

    best_overall = max(model_performance.items(), key=lambda x: x[1]["composite"])
    best_quality = max(model_performance.items(), key=lambda x: x[1]["quality"])
    fastest = min(model_performance.items(), key=lambda x: x[1]["speed"])
    cheapest = min(model_performance.items(), key=lambda x: x[1]["cost"])

    print(f"üèÜ MELHOR GERAL: {best_overall[0]}")
    print(f"   (Score composto: {best_overall[1]['composite']:.3f})")

    print(f"\nüéñÔ∏è MELHOR QUALIDADE: {best_quality[0]}")
    print(f"   (Score: {best_quality[1]['quality']:.3f})")

    print(f"\n‚ö° MAIS R√ÅPIDO: {fastest[0]}")
    print(f"   (Tempo: {fastest[1]['speed']:.2f}s)")

    print(f"\nüí∞ MAIS ECON√îMICO: {cheapest[0]}")
    print(f"   (Custo: ${cheapest[1]['cost']:.4f})")

    print(f"\nüí° RECOMENDA√á√ÉO DE USO:")
    if best_overall[1]["quality"] > 0.8:
        print("‚úÖ Sistema pronto para produ√ß√£o")
        print(f"   Usar {best_overall[0]} para uso geral")
        print(f"   Usar {best_quality[0]} para casos cr√≠ticos")
    elif best_overall[1]["quality"] > 0.6:
        print("‚ö†Ô∏è Sistema precisa de ajustes antes da produ√ß√£o")
        print("   Focar em melhorar prompts e datasets")
    else:
        print("‚ùå Sistema n√£o est√° pronto para produ√ß√£o")
        print("   Revis√£o completa de prompts e modelos necess√°ria")

def demo_evals_avancado():
    """
    Demonstra sistema avan√ßado de avalia√ß√£o e compara√ß√£o.
    """
    logger = get_logger("evals_avancado")

    try:
        print("üß™ SISTEMA DE AVALIA√á√ÉO AVAN√áADO")
        print("="*40)

        # Criar avaliador
        evaluator = AdvancedEvaluator()

        # Configurar modelos para compara√ß√£o
        print("\nü§ñ Configurando modelos para compara√ß√£o...")
        model_configs = criar_configuracoes_modelos()

        for config in model_configs:
            evaluator.add_model(config)
            print(f"  ‚úÖ {config.name} ({config.description})")

        # Carregar dataset avan√ßado
        print("\nüìä Carregando dataset avan√ßado...")
        test_cases = criar_dataset_avancado()

        for test_case in test_cases:
            evaluator.add_test_case(
                test_case["id"],
                test_case["input"],
                test_case["category"],
                test_case["criteria"]
            )

        print(f"  ‚úÖ {len(test_cases)} casos de teste carregados")

        # Executar avalia√ß√£o abrangente
        print(f"\nüöÄ Executando avalia√ß√£o comparativa...")
        start_time = time.time()

        results = evaluator.run_comprehensive_evaluation(logger)

        total_time = time.time() - start_time

        print(f"\n‚è±Ô∏è Avalia√ß√£o conclu√≠da em {total_time:.2f}s")
        print(f"üìä {len(results)} avalia√ß√µes executadas")

        # Gerar relat√≥rio comparativo
        gerar_relatorio_comparativo(results, logger)

        print(f"\n‚úÖ Evals Avan√ßado conclu√≠do!")
        print(f"\nüí° PR√ìXIMO PASSO: Use essas m√©tricas para escolher")
        print("   a configura√ß√£o ideal para seu caso de uso espec√≠fico.")

        logger.info(f"Evals avan√ßado conclu√≠do - {len(results)} avalia√ß√µes executadas")
        return True

    except Exception as e:
        error_msg = f"Erro durante execu√ß√£o dos Evals Avan√ßados: {str(e)}"
        logger.error(error_msg)
        print(f"‚ùå {error_msg}")
        return False

def explicar_diferencas_evals():
    """
    Explica diferen√ßas entre Evals b√°sico e avan√ßado.
    """
    print("""
üîç COMPARA√á√ÉO: EVALS B√ÅSICO vs AVAN√áADO
=======================================

EVALS B√ÅSICO (Cen√°rio 6):
‚úÖ Valida√ß√£o simples de qualidade
‚úÖ M√©tricas b√°sicas (relev√¢ncia, precis√£o)
‚úÖ Dataset pequeno e focado
‚úÖ Relat√≥rios simples

EVALS AVAN√áADO (Cen√°rio 7):
üöÄ Compara√ß√£o m√∫ltiplos modelos/configura√ß√µes
üöÄ M√©tricas sofisticadas (anti-alucina√ß√£o, estrutura)
üöÄ A/B testing automatizado
üöÄ An√°lise estat√≠stica robusta
üöÄ Recomenda√ß√µes estrat√©gicas
üöÄ Considera√ß√£o de custo-benef√≠cio

QUANDO USAR CADA UM:
- B√°sico: Valida√ß√£o inicial, sistemas simples
- Avan√ßado: Otimiza√ß√£o, sistemas cr√≠ticos, produ√ß√£o
""")

def mostrar_metricas_avancadas():
    """
    Explica m√©tricas avan√ßadas utilizadas.
    """
    print("""
üéØ M√âTRICAS AVAN√áADAS EXPLICADAS
================================

1. ANTI-ALUCINA√á√ÉO:
   - Detecta confian√ßa excessiva
   - Valida admiss√£o de limita√ß√µes
   - Previne informa√ß√µes incorretas

2. ESTRUTURA E ORGANIZA√á√ÉO:
   - Lista estruturada vs texto corrido
   - Conclus√µes claras
   - Tamanho adequado da resposta

3. AN√ÅLISE ESTAT√çSTICA:
   - M√©dia ¬± desvio padr√£o
   - Consist√™ncia entre execu√ß√µes
   - Intervalos de confian√ßa

4. COST-BENEFIT ANALYSIS:
   - Tokens utilizados
   - Tempo de execu√ß√£o
   - Custo estimado por intera√ß√£o

5. COMPOSITE SCORING:
   - Combina qualidade + velocidade + custo
   - Pondera√ß√£o customiz√°vel
   - Ranking multi-crit√©rio
""")

def configurar_ambiente():
    """
    Configura√ß√£o para Evals Avan√ßado.
    """
    print("""
üìã CONFIGURA√á√ÉO - EVALS AVAN√áADO
================================

Depend√™ncias adicionais:
pip install scipy  # Para an√°lise estat√≠stica
pip install matplotlib  # Para gr√°ficos (opcional)

Recursos necess√°rios:
- M√∫ltiplos modelos/provedores configurados
- Dataset abrangente e representativo
- Tempo para execu√ß√£o completa (~10-30min)

IMPORTANTE:
- Execute em ambiente isolado para evitar interfer√™ncias
- Use seeds fixos para reproducibilidade
- Monitore recursos durante execu√ß√£o
- Salve resultados para an√°lise posterior
""")

if __name__ == "__main__":
    configurar_ambiente()
    explicar_diferencas_evals()
    mostrar_metricas_avancadas()

    resposta = input("\nDeseja executar o demo de Evals Avan√ßado? (s/n): ")
    if resposta.lower() in ['s', 'sim', 'y', 'yes']:
        demo_evals_avancado()
    else:
        print("Demo cancelado.")