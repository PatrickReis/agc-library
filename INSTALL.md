# Guia de Instala√ß√£o - AgentCore

Este guia fornece instru√ß√µes detalhadas para instalar e configurar a biblioteca AgentCore.

## üìã Requisitos do Sistema

- Python 3.8 ou superior
- pip (gerenciador de pacotes Python)
- Acesso √† internet para download das depend√™ncias

### Requisitos Opcionais por Provedor

#### AWS Bedrock (Recomendado)
- Conta AWS ativa
- Credenciais AWS configuradas
- Acesso ao servi√ßo Bedrock na sua regi√£o

#### OpenAI
- Chave da API OpenAI
- Cr√©ditos na conta OpenAI

#### Ollama (Desenvolvimento Local)
- Ollama instalado localmente
- Modelos baixados (ex: llama3)

#### Google Gemini
- Chave da API Google Gemini
- Acesso ao Google AI Studio

## üöÄ Instala√ß√£o R√°pida

### Op√ß√£o 1: Instala√ß√£o B√°sica
```bash
pip install agentcore
```

### Op√ß√£o 2: Instala√ß√£o com Provedores Espec√≠ficos

#### Para uso com AWS Bedrock (Produ√ß√£o)
```bash
pip install agentcore[aws]
```

#### Para uso com OpenAI
```bash
pip install agentcore[openai]
```

#### Para desenvolvimento local com Ollama
```bash
pip install agentcore[ollama]
```

#### Para uso com Google Gemini
```bash
pip install agentcore[google]
```

#### Instala√ß√£o completa (todos os provedores)
```bash
pip install agentcore[aws,openai,ollama,google]
```

### Op√ß√£o 3: Para Desenvolvimento
```bash
pip install agentcore[dev]
```

## ‚öôÔ∏è Configura√ß√£o

### 1. Configura√ß√£o AWS Bedrock (Recomendado)

#### M√©todo 1: AWS CLI (Recomendado)
```bash
# Instalar AWS CLI se n√£o tiver
pip install awscli

# Configurar credenciais
aws configure
```

#### M√©todo 2: Vari√°veis de Ambiente
```bash
export AWS_ACCESS_KEY_ID=seu_access_key
export AWS_SECRET_ACCESS_KEY=sua_secret_key
export AWS_REGION=us-east-1
```

#### M√©todo 3: IAM Roles (Para EC2, Lambda, etc.)
Se executando em servi√ßos AWS, configure uma role IAM com permiss√µes Bedrock.

#### Configura√ß√µes Espec√≠ficas do Bedrock
```bash
export BEDROCK_MODEL=anthropic.claude-3-sonnet-20240229-v1:0
export BEDROCK_EMBEDDINGS_MODEL=amazon.titan-embed-text-v1
```

### 2. Configura√ß√£o OpenAI
```bash
export OPENAI_API_KEY=sua_chave_openai
export OPENAI_MODEL=gpt-4  # opcional, padr√£o: gpt-3.5-turbo
```

### 3. Configura√ß√£o Ollama
```bash
# Instalar Ollama (se n√£o tiver)
curl -fsSL https://ollama.ai/install.sh | sh

# Baixar modelo
ollama pull llama3

# Iniciar Ollama
ollama serve

# Configurar AgentCore
export OLLAMA_BASE_URL=http://localhost:11434
export OLLAMA_MODEL=llama3:latest
```

### 4. Configura√ß√£o Google Gemini
```bash
export GEMINI_API_KEY=sua_chave_gemini
export GEMINI_MODEL=gemini-1.5-flash  # opcional
```

### 5. Arquivo .env (Recomendado)

Crie um arquivo `.env` na raiz do seu projeto:

```bash
# Provedor principal
MAIN_PROVIDER=bedrock

# AWS Bedrock (recomendado para produ√ß√£o)
AWS_REGION=us-east-1
AWS_ACCESS_KEY_ID=seu_access_key
AWS_SECRET_ACCESS_KEY=sua_secret_key
BEDROCK_MODEL=anthropic.claude-3-sonnet-20240229-v1:0
BEDROCK_EMBEDDINGS_MODEL=amazon.titan-embed-text-v1

# OpenAI (opcional)
OPENAI_API_KEY=sua_chave_openai
OPENAI_MODEL=gpt-4

# Ollama (opcional)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3:latest

# Google Gemini (opcional)
GEMINI_API_KEY=sua_chave_gemini
GEMINI_MODEL=gemini-1.5-flash
```

## ‚úÖ Verifica√ß√£o da Instala√ß√£o

### 1. Teste via Python
```python
# Testar importa√ß√£o
from agentCore.utils import api2tool
from agentCore.providers import get_llm, get_provider_info

# Verificar provedor configurado
info = get_provider_info()
print(f"Provedor ativo: {info['provider']}")

# Testar LLM
llm = get_llm()
print("‚úÖ LLM configurado com sucesso!")
```

### 2. Teste via CLI
```bash
# Verificar vers√£o
agentcore version

# Verificar informa√ß√µes do provedor
agentcore info

# Testar convers√£o de OpenAPI
agentcore validate https://petstore.swagger.io/v2/swagger.json
```

### 3. Executar Exemplo
```bash
# Baixar exemplo (se instalado via git)
python example.py

# Ou criar um teste simples
python -c "
from agentCore.utils import api2tool
tools = api2tool('https://petstore.swagger.io/v2/swagger.json', output_format='info')
print(f'API encontrada: {tools[\"title\"]} - {tools[\"tool_count\"]} ferramentas')
"
```

## üîß Instala√ß√£o para Desenvolvimento

### 1. Clone do Reposit√≥rio
```bash
git clone <url-do-repositorio>
cd agentcore
```

### 2. Instala√ß√£o em Modo Desenvolvimento
```bash
# Criar ambiente virtual
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows

# Instalar em modo desenvolvimento
pip install -e .[dev]
```

### 3. Configurar Ferramentas de Desenvolvimento
```bash
# Instalar pre-commit hooks
pre-commit install

# Executar testes
pytest

# Executar testes com cobertura
pytest --cov=agentCore

# Formatar c√≥digo
black agentCore/
isort agentCore/
```

## üê≥ Instala√ß√£o com Docker

### 1. Dockerfile de Exemplo
```dockerfile
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install -r requirements.txt

# Instalar AgentCore
RUN pip install agentcore[aws]

COPY . .

CMD ["python", "app.py"]
```

### 2. Docker Compose
```yaml
version: '3.8'

services:
  agentcore-app:
    build: .
    environment:
      - AWS_REGION=us-east-1
      - BEDROCK_MODEL=anthropic.claude-3-sonnet-20240229-v1:0
      - MAIN_PROVIDER=bedrock
    volumes:
      - ~/.aws:/root/.aws:ro  # Compartilhar credenciais AWS
```

## üö® Solu√ß√£o de Problemas

### Erro: "langchain-aws n√£o est√° instalado"
```bash
pip install langchain-aws boto3
# ou
pip install agentcore[aws]
```

### Erro: "OPENAI_API_KEY n√£o configurada"
```bash
export OPENAI_API_KEY=sua_chave
# ou adicione ao arquivo .env
```

### Erro: "Erro ao conectar com provedor LLM"
#### Para Bedrock:
- Verifique credenciais AWS
- Confirme acesso ao Bedrock na sua regi√£o
- Verifique se o modelo est√° dispon√≠vel

#### Para Ollama:
```bash
# Verificar se Ollama est√° rodando
ollama list

# Iniciar Ollama
ollama serve
```

### Erro de Importa√ß√£o
```bash
# Verificar instala√ß√£o
pip list | grep agentcore

# Reinstalar se necess√°rio
pip uninstall agentcore
pip install agentcore
```

### Problemas de Depend√™ncias
```bash
# Atualizar pip
pip install --upgrade pip

# Limpar cache do pip
pip cache purge

# Reinstalar depend√™ncias
pip install --force-reinstall agentcore
```
# üîß Instala√ß√£o Local
Op√ß√£o 1: Instala√ß√£o em modo desenvolvimento (Recomendado)
# No diret√≥rio do projeto agc-lib
pip install -e .
O -e (editable) permite que mudan√ßas no c√≥digo sejam refletidas imediatamente sem reinstalar.
Op√ß√£o 2: Instala√ß√£o local direta
# No diret√≥rio do projeto
pip install .
Op√ß√£o 3: Build e instala√ß√£o
# Construir distribui√ß√£o
python -m build

# Instalar do arquivo gerado
pip install dist/agentcore-1.0.0-py3-none-any.whl
üì¶ Verificar Instala√ß√£o
Ap√≥s instalar, teste:
import agentCore
from agentCore.providers import get_llm
from agentCore.utils import api2tool

print("AgentCore instalado com sucesso!")
üõ†Ô∏è Para Desenvolvimento
Se voc√™ vai desenvolver no projeto:
# Instalar com depend√™ncias de desenvolvimento
pip install -e .[dev]

# Instalar todas as depend√™ncias opcionais
pip install -e .[aws,openai,ollama,google,dev]
üêç Ambiente Virtual (Recomendado)
# Criar ambiente virtual
python -m venv venv

# Ativar (Linux/Mac)
source venv/bin/activate

# Ativar (Windows)
venv\Scripts\activate

# Instalar projeto
pip install -e .
‚úÖ Verifica√ß√£o Final
# Testar CLI
agentcore --help

# Executar demo
python e2e_demo.py

# Executar testes
pytest tests/ -v


## üìö Pr√≥ximos Passos

Ap√≥s a instala√ß√£o bem-sucedida:

1. **Leia o README.md** para exemplos de uso
2. **Execute o example.py** para ver a biblioteca em a√ß√£o
3. **Configure seu provedor LLM preferido**
4. **Teste com suas pr√≥prias APIs OpenAPI**
5. **Explore a documenta√ß√£o completa**

## üìû Suporte

Se encontrar problemas durante a instala√ß√£o:

- Verifique os [Issues no GitHub](https://github.com/your-org/agent-core/issues)
- Consulte a [Documenta√ß√£o](https://agent-core.readthedocs.io)

